{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Study DA documentation","text":""},{"location":"case_studies/index.html","title":"Coucou","text":"<p>We're going to pla</p>"},{"location":"case_studies/simple_collider.html","title":"Test","text":"coucou.py<pre><code>print('coucou')\n</code></pre> <p>Your directory structure must be identical to the provided templates one: <pre><code>\ud83d\udcc1 root/\n\u251c\u2500\u2574\ud83d\udcc1 children/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc1 job/\n\u2502   \u2514\u2500\u2500 \ud83d\udcc1 job/\n\u2514\u2500\u2500 \ud83d\udcc1 children/\n    \u251c\u2500\u2500 \ud83d\udcc1 job/\n    \u2514\u2500\u2500 \ud83d\udcc1 job/\n</code></pre></p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>study_da<ul> <li>generate<ul> <li>generate_scan</li> <li>master_classes<ul> <li>mad_collider</li> <li>particles_distribution</li> <li>scheme_utils</li> <li>utils</li> <li>xsuite_collider</li> <li>xsuite_leveling</li> <li>xsuite_tracking</li> </ul> </li> <li>parameter_space</li> <li>template_scripts<ul> <li>generation_1</li> <li>generation_1_dummy</li> <li>generation_2_dummy</li> <li>generation_2_level_by_nb</li> <li>generation_2_level_by_sep</li> <li>generation_3_dummy</li> </ul> </li> <li>version_specific_files<ul> <li>hllhc13<ul> <li>crab_fix</li> <li>optics_specific_tools</li> <li>orbit_correction</li> </ul> </li> <li>hllhc16<ul> <li>optics_specific_tools</li> <li>orbit_correction</li> </ul> </li> <li>runIII<ul> <li>optics_specific_tools</li> </ul> </li> <li>runIII_ions<ul> <li>optics_specific_tools</li> </ul> </li> </ul> </li> </ul> </li> <li>plot<ul> <li>build_title</li> <li>plot_study</li> <li>utils<ul> <li>maplotlib_utils</li> </ul> </li> </ul> </li> <li>postprocess<ul> <li>postprocess</li> </ul> </li> <li>study_da</li> <li>submit<ul> <li>ask_user_config</li> <li>cluster_submission<ul> <li>cluster_submission</li> <li>submission_statements</li> </ul> </li> <li>config_jobs</li> <li>dependency_graph</li> <li>generate_run</li> <li>submit_scan</li> </ul> </li> <li>utils<ul> <li>dic_utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/study_da/index.html","title":"study_da","text":""},{"location":"reference/study_da/index.html#study_da.GenerateScan","title":"<code>GenerateScan</code>","text":"<p>A class to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> <code>ryaml</code> <code>YAML</code> <p>The YAML parser.</p> <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary of common parameters across generations.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the generation scan with a configuration file or dictionary.</p> <code>render</code> <p>Renders the study file using a template.</p> <code>write</code> <p>Writes the study file to disk.</p> <code>generate_render_write</code> <p>Generates, renders, and writes the study file.</p> <code>get_dic_parametric_scans</code> <p>Retrieves dictionaries of parametric scan values.</p> <code>parse_parameter_space</code> <p>Parses the parameter space for a given parameter.</p> <code>browse_and_collect_parameter_space</code> <p>Browses and collects the parameter space for a given generation.</p> <code>postprocess_parameter_lists</code> <p>Postprocesses the parameter lists.</p> <code>create_scans</code> <p>Creates study files for parametric scans.</p> <code>complete_tree</code> <p>Completes the tree structure of the study dictionary.</p> <code>write_tree</code> <p>Writes the study tree structure to a YAML file.</p> <code>create_study_for_current_gen</code> <p>Creates study files for the current generation.</p> <code>create_study</code> <p>Creates study files for the entire study.</p> <code>eval_conditions</code> <p>Evaluates the conditions to filter out some parameter values.</p> <code>filter_for_concomitant_parameters</code> <p>Filters the conditions for concomitant parameters.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>class GenerateScan:\n    \"\"\"\n    A class to generate a study (along with the corresponding tree) from a parameter file,\n    and potentially a set of template files.\n\n    Attributes:\n        config (dict): The configuration dictionary.\n        ryaml (yaml.YAML): The YAML parser.\n        dic_common_parameters (dict): Dictionary of common parameters across generations.\n\n    Methods:\n        __init__(): Initializes the generation scan with a configuration file or dictionary.\n        render(): Renders the study file using a template.\n        write(): Writes the study file to disk.\n        generate_render_write(): Generates, renders, and writes the study file.\n        get_dic_parametric_scans(): Retrieves dictionaries of parametric scan values.\n        parse_parameter_space(): Parses the parameter space for a given parameter.\n        browse_and_collect_parameter_space(): Browses and collects the parameter space for a given\n            generation.\n        postprocess_parameter_lists(): Postprocesses the parameter lists.\n        create_scans(): Creates study files for parametric scans.\n        complete_tree(): Completes the tree structure of the study dictionary.\n        write_tree(): Writes the study tree structure to a YAML file.\n        create_study_for_current_gen(): Creates study files for the current generation.\n        create_study(): Creates study files for the entire study.\n        eval_conditions(): Evaluates the conditions to filter out some parameter values.\n        filter_for_concomitant_parameters(): Filters the conditions for concomitant parameters.\n    \"\"\"\n\n    def __init__(\n        self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n    ):  # sourcery skip: remove-redundant-if\n        \"\"\"\n        Initialize the generation scan with a configuration file or dictionary.\n\n        Args:\n            path_config (Optional[str]): Path to the configuration file for the scan.\n                Default is None.\n            dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n                Default is None.\n\n        Raises:\n            ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n        \"\"\"\n        # Load the study configuration from file or dictionary\n        if dic_scan is None and path_config is None:\n            raise ValueError(\n                \"Either a path to the configuration file or a dictionary must be provided.\"\n            )\n        elif dic_scan is not None and path_config is not None:\n            raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n        elif path_config is not None:\n            self.config, self.ryaml = load_dic_from_path(path_config)\n        elif dic_scan is not None:\n            self.config = dic_scan\n            self.ryaml = yaml.YAML()\n        else:\n            raise ValueError(\"An unexpected error occurred.\")\n\n        # Parameters common across all generations (e.g. for parallelization)\n        self.dic_common_parameters: dict[str, Any] = {}\n\n        # Path to the tree file\n        self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n\n    def render(\n        self,\n        str_parameters: str,\n        template_path: str,\n        dependencies: Optional[dict[str, str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Renders the study file using a template.\n\n        Args:\n            str_parameters (str): The string representation of parameters to declare/mutate.\n            template_path (str): The path to the template file.\n            dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n        Returns:\n            str: The rendered study file.\n        \"\"\"\n\n        # Handle mutable default argument\n        if dependencies is None:\n            dependencies = {}\n\n        # Generate generations from template\n        directory_path = os.path.dirname(template_path)\n        template_name = os.path.basename(template_path)\n        environment = Environment(loader=FileSystemLoader(directory_path))\n        template = environment.get_template(template_name)\n\n        return template.render(parameters=str_parameters, **dependencies)\n\n    def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n        \"\"\"\n        Writes the study file to disk.\n\n        Args:\n            study_str (str): The study file string.\n            file_path (str): The path to write the study file.\n            format_with_black (bool, optional): Whether to format the output file with black.\n                Defaults to True.\n        \"\"\"\n\n        # Format the string with black\n        if format_with_black:\n            study_str = format_str(study_str, mode=FileMode())\n\n        # Make folder if it doesn't exist\n        folder = os.path.dirname(file_path)\n        if folder != \"\":\n            os.makedirs(folder, exist_ok=True)\n\n        with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(study_str)\n\n    def generate_render_write(\n        self,\n        gen_name: str,\n        study_path: str,\n        template_path: str,\n        dic_mutated_parameters: dict[str, Any] = {},\n    ) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n        \"\"\"\n        Generates, renders, and writes the study file.\n\n        Args:\n            gen_name (str): The name of the generation.\n            study_path (str): The path to the study folder.\n            template_path (str): The path to the template folder.\n            dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n                Defaults to {}.\n\n        Returns:\n            tuple[str, list[str]]: The study file string and the list of study paths.\n        \"\"\"\n\n        directory_path_gen = f\"{study_path}\"\n        if not directory_path_gen.endswith(\"/\"):\n            directory_path_gen += \"/\"\n        file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n        logging.info(f'Now rendering generation \"{file_path_gen}\"')\n        # Generate the string of parameters\n        str_parameters = \"{\"\n        for key, value in dic_mutated_parameters.items():\n            if isinstance(value, str):\n                str_parameters += f\"'{key}' : '{value}', \"\n            else:\n                str_parameters += f\"'{key}' : {value}, \"\n        str_parameters += \"}\"\n\n        # Adapt the dict of dependencies to the current generation\n        dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n        # Always load configuration from above generation\n        depth_gen = 1\n        # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n        dic_dependencies = {\n            key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n        }\n\n        # Render and write the study file\n        study_str = self.render(\n            str_parameters,\n            template_path=template_path,\n            dependencies=dic_dependencies,\n        )\n\n        self.write(study_str, file_path_gen)\n        return [directory_path_gen]\n\n    def get_dic_parametric_scans(\n        self, generation: str\n    ) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n        \"\"\"\n        Retrieves dictionaries of parametric scan values.\n\n        Args:\n            generation: The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n                scan values, another dictionnary with better naming for the tree creation, and an\n                array of conditions to filter out some parameter values.\n        \"\"\"\n\n        if generation == \"base\":\n            raise ValueError(\"Generation 'base' should not have scans.\")\n\n        # Remember common parameters as they might be used across generations\n        if \"common_parameters\" in self.config[\"structure\"][generation]:\n            self.dic_common_parameters[generation] = {}\n            for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n                self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                    generation\n                ][\"common_parameters\"][parameter]\n\n        # Check that the generation has scans\n        if (\n            \"scans\" not in self.config[\"structure\"][generation]\n            or self.config[\"structure\"][generation][\"scans\"] is None\n        ):\n            dic_parameter_lists = {\"\": [generation]}\n            dic_parameter_lists_for_naming = {\"\": [generation]}\n            array_conditions = None\n            ll_concomitant_parameters = []\n        else:\n            # Browse and collect the parameter space for the generation\n            (\n                dic_parameter_lists,\n                dic_parameter_lists_for_naming,\n                dic_subvariables,\n                ll_concomitant_parameters,\n                l_conditions,\n            ) = self.browse_and_collect_parameter_space(generation)\n\n            # Get the dimension corresponding to each parameter\n            dic_dimension_indices = {\n                parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n            }\n\n            # Generate array of conditions to filter out some of the values later\n            # Is an array of True values if no conditions are present\n            array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n            # Filter for concomitant parameters\n            array_conditions = self.filter_for_concomitant_parameters(\n                array_conditions, ll_concomitant_parameters, dic_dimension_indices\n            )\n\n            # Postprocess the parameter lists and update the dictionaries\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n                dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n            )\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            array_conditions,\n        )\n\n    def parse_parameter_space(\n        self,\n        parameter: str,\n        dic_curr_parameter: dict[str, Any],\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Parses the parameter space for a given parameter.\n\n        Args:\n            parameter (str): The parameter name.\n            dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n            dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n        \"\"\"\n\n        if \"linspace\" in dic_curr_parameter:\n            parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"logspace\" in dic_curr_parameter:\n            parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"path_list\" in dic_curr_parameter:\n            l_values_path_list = dic_curr_parameter[\"path_list\"]\n            parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n            dic_parameter_lists_for_naming[parameter] = [\n                f\"{n:02d}\" for n, path in enumerate(parameter_list)\n            ]\n        elif \"list\" in dic_curr_parameter:\n            parameter_list = dic_curr_parameter[\"list\"]\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"expression\" in dic_curr_parameter:\n            parameter_list = np.round(\n                eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n                8,\n            )\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        else:\n            raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n        dic_parameter_lists[parameter] = np.array(parameter_list)\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def browse_and_collect_parameter_space(\n        self,\n        generation: str,\n    ) -&gt; tuple[\n        dict[str, Any],\n        dict[str, Any],\n        dict[str, Any],\n        list[list[str]],\n        list[str],\n    ]:\n        \"\"\"\n        Browses and collects the parameter space for a given generation.\n\n        Args:\n            generation (str): The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n                dictionaries of parameter lists.\n        \"\"\"\n\n        l_conditions = []\n        ll_concomitant_parameters = []\n        dic_subvariables = {}\n        dic_parameter_lists = {}\n        dic_parameter_lists_for_naming = {}\n        for parameter in self.config[\"structure\"][generation][\"scans\"]:\n            dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n            # Parse the parameter space\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n                parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n            )\n\n            # Store potential subvariables\n            if \"subvariables\" in dic_curr_parameter:\n                dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n            # Save the condition if it exists\n            if \"condition\" in dic_curr_parameter:\n                l_conditions.append(dic_curr_parameter[\"condition\"])\n\n            # Save the concomitant parameters if they exist\n            if \"concomitant\" in dic_curr_parameter:\n                if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                    dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n                for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                    # Assert that the parameters list have the same size\n                    assert len(dic_parameter_lists[parameter]) == len(\n                        dic_parameter_lists[concomitant_parameter]\n                    ), (\n                        f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                        \"same size.\"\n                    )\n                # Add to the list for filtering later\n                ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        )\n\n    def postprocess_parameter_lists(\n        self,\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n        dic_subvariables: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Post-processes parameter lists by ensuring values are not numpy types and handling nested\n        parameters.\n\n        Args:\n            dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n                for naming.\n            dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n                parameters.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n                parameter lists for naming.\n        \"\"\"\n        for parameter, parameter_list in dic_parameter_lists.items():\n            parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n            # Ensure that all values are not numpy types (to avoid serialization issues)\n            parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n            # Handle nested parameters\n            parameter_list_updated = (\n                convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n                if parameter in dic_subvariables\n                else parameter_list\n            )\n            # Update the dictionaries\n            dic_parameter_lists[parameter] = parameter_list_updated\n            dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def create_scans(\n        self,\n        generation: str,\n        generation_path: str,\n        template_path: str,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for parametric scans.\n\n        Args:\n            generation (str): The generation name.\n            generation_path (str): The path to the layer folder.\n            template_path (str): The path to the template folder.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        if dic_parameter_lists is None:\n            # Get dictionnary of parametric values being scanned\n            dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n                self.get_dic_parametric_scans(generation)\n            )\n        else:\n            if dic_parameter_lists_for_naming is None:\n                dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n            array_conditions = None\n\n        # Generate render write for cartesian product of all parameters\n        l_study_path = []\n        logging.info(\n            f\"Now generation cartesian product of all parameters for generation: {generation}\"\n        )\n        for l_values, l_values_for_naming, l_idx in zip(\n            itertools.product(*dic_parameter_lists.values()),\n            itertools.product(*dic_parameter_lists_for_naming.values()),\n            itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()]),\n        ):\n            # Check the idx to keep if conditions are present\n            if array_conditions is not None and not array_conditions[l_idx]:\n                continue\n\n            # Create the path for the study\n            dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n            dic_mutated_parameters_for_naming = dict(\n                zip(dic_parameter_lists.keys(), l_values_for_naming)\n            )\n            suffix_path = \"_\".join(\n                [\n                    f\"{parameter}_{value}\"\n                    for parameter, value in dic_mutated_parameters_for_naming.items()\n                ]\n            )\n\n            # Remove '_' at the beginning of the suffix path if needed (e.g. for generation)\n            suffix_path = suffix_path.removeprefix(\"_\")\n            # Create final path\n            path = generation_path + suffix_path + \"/\"\n\n            # Add common parameters\n            if generation in self.dic_common_parameters:\n                dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n            # Remove \"\" from mutated parameters, if it's in the dictionary\n            # as it's only used when no scan is done\n            if \"\" in dic_mutated_parameters:\n                dic_mutated_parameters.pop(\"\")\n\n            # Generate the study for current generation\n            self.generate_render_write(\n                generation,\n                path,\n                template_path,\n                dic_mutated_parameters=dic_mutated_parameters,\n            )\n\n            # Append the list of study paths to build the tree later on\n            l_study_path.append(path)\n\n        if not l_study_path:\n            logging.warning(\n                f\"No study paths were created for generation {generation}.\"\n                \"Please check the conditions.\"\n            )\n\n        return l_study_path\n\n    def complete_tree(\n        self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n    ) -&gt; dict:\n        \"\"\"\n        Completes the tree structure of the study dictionary.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n            l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n            gen (str): The generation name.\n\n        Returns:\n            dict: The updated dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(f\"Completing the tree structure for generation: {gen}\")\n        for path_next in l_study_path_next_gen:\n            nested_set(\n                dictionary_tree,\n                path_next.split(\"/\")[1:-1] + [gen],\n                {\"file\": f\"{path_next}{gen}.py\"},\n            )\n\n        return dictionary_tree\n\n    def write_tree(self, dictionary_tree: dict):\n        \"\"\"\n        Writes the study tree structure to a YAML file.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(\"Writing the tree structure to a YAML file.\")\n        ryaml = yaml.YAML()\n        with open(self.path_tree, \"w\") as yaml_file:\n            ryaml.indent(sequence=4, offset=2)\n            ryaml.dump(dictionary_tree, yaml_file)\n\n    def create_study_for_current_gen(\n        self,\n        generation: str,\n        study_path: str,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for the current generation.\n\n        Args:\n            generation (str): The name of the current generation.\n            study_path (str): The path to the study folder.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        executable_path = self.config[\"structure\"][generation][\"executable\"]\n        path_local_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/template_scripts/\"\n\n        # Check if the executable path corresponds to a file\n        if not os.path.isfile(executable_path):\n            # Check if the executable path corresponds to a file in the template folder\n            executable_path_template = f\"{path_local_template}{executable_path}\"\n            if not os.path.isfile(executable_path_template):\n                raise FileNotFoundError(\n                    f\"Executable file {executable_path} not found locally nor in the study-da \"\n                    \"template folder.\"\n                )\n            else:\n                executable_path = executable_path_template\n\n        # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n        if dic_parameter_lists is not None:\n            # Recursively convert all numpy types to standard types\n            clean_dic(dic_parameter_lists)\n            logging.info(\"An external dictionary of parameters was provided.\")\n        else:\n            logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n        return self.create_scans(\n            generation,\n            study_path,\n            executable_path,\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n        )\n\n    def create_study(\n        self,\n        tree_file: bool = True,\n        force_overwrite: bool = False,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        l_study_path = [self.config[\"name\"] + \"/\"]\n        dictionary_tree = {}\n        \"\"\"\n        Creates study files for the entire study.\n\n        Args:\n            tree_file (bool, optional): Whether to write the study tree structure to a YAML file. \n                Defaults to True.\n            force_overwrite (bool, optional): Whether to overwrite existing study files. \n                Defaults to False.\n\n        Returns:\n            list[str]: The list of study file strings.\n        \"\"\"\n        # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n        if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n            raise ValueError(\n                \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n            )\n\n        # Remove existing study if force_overwrite\n        if os.path.exists(self.config[\"name\"]):\n            if not force_overwrite:\n                logging.info(\n                    f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                    \"overwrite. Continuing without overwriting.\"\n                )\n                return\n            shutil.rmtree(self.config[\"name\"])\n\n        # Browse through the generations\n        l_generations = list(self.config[\"structure\"].keys())\n        for idx, generation in enumerate(l_generations):\n            l_study_path_all_next_generation = []\n            logging.info(f\"Taking care of generation: {generation}\")\n            for study_path in l_study_path:\n                if dic_parameter_all_gen is None or generation not in dic_parameter_all_gen:\n                    dic_parameter_current_gen = None\n                    dic_parameter_naming_current_gen = None\n                else:\n                    dic_parameter_current_gen = dic_parameter_all_gen[generation]\n                    if (\n                        dic_parameter_all_gen_naming is not None\n                        and generation in dic_parameter_all_gen_naming\n                    ):\n                        dic_parameter_naming_current_gen = dic_parameter_all_gen_naming[generation]\n                    else:\n                        dic_parameter_naming_current_gen = None\n\n                # Get list of paths for the children of the current study\n                l_study_path_next_generation = self.create_study_for_current_gen(\n                    generation,\n                    study_path,\n                    dic_parameter_current_gen,\n                    dic_parameter_naming_current_gen,\n                )\n                # Update tree\n                dictionary_tree = self.complete_tree(\n                    dictionary_tree, l_study_path_next_generation, generation\n                )\n                # Complete list of paths for the children of all studies (of the current generation)\n                l_study_path_all_next_generation.extend(l_study_path_next_generation)\n\n            # Update study path for next later\n            l_study_path = l_study_path_all_next_generation\n\n        # Add dependencies to the study\n        if \"dependencies\" in self.config:\n            for dependency, path in self.config[\"dependencies\"].items():\n                shutil.copy2(path, self.config[\"name\"])\n\n        if tree_file:\n            self.write_tree(dictionary_tree)\n\n    @staticmethod\n    def eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the conditions to filter out some parameter values.\n\n        Args:\n            l_condition (list[str]): The list of conditions.\n            dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n        Returns:\n            np.ndarray: The array of conditions.\n        \"\"\"\n        # Initialize the array of parameters as a meshgrid of all parameters\n        l_parameters = list(dic_parameter_lists.values())\n        meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n        # Associate the parameters to their names\n        dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n        # Evaluate the conditions and take the intersection of all conditions\n        array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n        for condition in l_condition:\n            array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n        return array_conditions\n\n    @staticmethod\n    def filter_for_concomitant_parameters(\n        array_conditions: np.ndarray,\n        ll_concomitant_parameters: list[list[str]],\n        dic_dimension_indices: dict[str, int],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Filters the conditions for concomitant parameters.\n\n        Args:\n            array_conditions (np.ndarray): The array of conditions.\n            ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n            dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n        Returns:\n            np.ndarray: The filtered array of conditions.\n        \"\"\"\n\n        # Return the array of conditions if no concomitant parameters\n        if not ll_concomitant_parameters:\n            return array_conditions\n\n        # Get the indices of the concomitant parameters\n        ll_idx_concomitant_parameters = [\n            [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n            for concomitant_parameters in ll_concomitant_parameters\n        ]\n\n        # Browse all the values of array_conditions\n        for idx, _ in np.ndenumerate(array_conditions):\n            # Check if the value is on the diagonal of the concomitant parameters\n            for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n                if any(\n                    idx[i] != idx[j]\n                    for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n                ):\n                    array_conditions[idx] = False\n                    break\n\n        return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.__init__","title":"<code>__init__(path_config=None, dic_scan=None)</code>","text":"<p>Initialize the generation scan with a configuration file or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path_config</code> <code>Optional[str]</code> <p>Path to the configuration file for the scan. Default is None.</p> <code>None</code> <code>dic_scan</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary containing the scan configuration. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of <code>path_config</code> and <code>dic_scan</code> are provided.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def __init__(\n    self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n):  # sourcery skip: remove-redundant-if\n    \"\"\"\n    Initialize the generation scan with a configuration file or dictionary.\n\n    Args:\n        path_config (Optional[str]): Path to the configuration file for the scan.\n            Default is None.\n        dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n            Default is None.\n\n    Raises:\n        ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n    \"\"\"\n    # Load the study configuration from file or dictionary\n    if dic_scan is None and path_config is None:\n        raise ValueError(\n            \"Either a path to the configuration file or a dictionary must be provided.\"\n        )\n    elif dic_scan is not None and path_config is not None:\n        raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n    elif path_config is not None:\n        self.config, self.ryaml = load_dic_from_path(path_config)\n    elif dic_scan is not None:\n        self.config = dic_scan\n        self.ryaml = yaml.YAML()\n    else:\n        raise ValueError(\"An unexpected error occurred.\")\n\n    # Parameters common across all generations (e.g. for parallelization)\n    self.dic_common_parameters: dict[str, Any] = {}\n\n    # Path to the tree file\n    self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.browse_and_collect_parameter_space","title":"<code>browse_and_collect_parameter_space(generation)</code>","text":"<p>Browses and collects the parameter space for a given generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]], list[str]]</code> <p>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def browse_and_collect_parameter_space(\n    self,\n    generation: str,\n) -&gt; tuple[\n    dict[str, Any],\n    dict[str, Any],\n    dict[str, Any],\n    list[list[str]],\n    list[str],\n]:\n    \"\"\"\n    Browses and collects the parameter space for a given generation.\n\n    Args:\n        generation (str): The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n            dictionaries of parameter lists.\n    \"\"\"\n\n    l_conditions = []\n    ll_concomitant_parameters = []\n    dic_subvariables = {}\n    dic_parameter_lists = {}\n    dic_parameter_lists_for_naming = {}\n    for parameter in self.config[\"structure\"][generation][\"scans\"]:\n        dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n        # Parse the parameter space\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n            parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n        )\n\n        # Store potential subvariables\n        if \"subvariables\" in dic_curr_parameter:\n            dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n        # Save the condition if it exists\n        if \"condition\" in dic_curr_parameter:\n            l_conditions.append(dic_curr_parameter[\"condition\"])\n\n        # Save the concomitant parameters if they exist\n        if \"concomitant\" in dic_curr_parameter:\n            if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n            for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                # Assert that the parameters list have the same size\n                assert len(dic_parameter_lists[parameter]) == len(\n                    dic_parameter_lists[concomitant_parameter]\n                ), (\n                    f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                    \"same size.\"\n                )\n            # Add to the list for filtering later\n            ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        dic_subvariables,\n        ll_concomitant_parameters,\n        l_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.complete_tree","title":"<code>complete_tree(dictionary_tree, l_study_path_next_gen, gen)</code>","text":"<p>Completes the tree structure of the study dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required <code>l_study_path_next_gen</code> <code>list[str]</code> <p>The list of study paths for the next gen.</p> required <code>gen</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary representing the study tree structure.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def complete_tree(\n    self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n) -&gt; dict:\n    \"\"\"\n    Completes the tree structure of the study dictionary.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n        l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n        gen (str): The generation name.\n\n    Returns:\n        dict: The updated dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(f\"Completing the tree structure for generation: {gen}\")\n    for path_next in l_study_path_next_gen:\n        nested_set(\n            dictionary_tree,\n            path_next.split(\"/\")[1:-1] + [gen],\n            {\"file\": f\"{path_next}{gen}.py\"},\n        )\n\n    return dictionary_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.create_scans","title":"<code>create_scans(generation, generation_path, template_path, dic_parameter_lists=None, dic_parameter_lists_for_naming=None)</code>","text":"<p>Creates study files for parametric scans.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <code>generation_path</code> <code>str</code> <p>The path to the layer folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_scans(\n    self,\n    generation: str,\n    generation_path: str,\n    template_path: str,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for parametric scans.\n\n    Args:\n        generation (str): The generation name.\n        generation_path (str): The path to the layer folder.\n        template_path (str): The path to the template folder.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    if dic_parameter_lists is None:\n        # Get dictionnary of parametric values being scanned\n        dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n            self.get_dic_parametric_scans(generation)\n        )\n    else:\n        if dic_parameter_lists_for_naming is None:\n            dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n        array_conditions = None\n\n    # Generate render write for cartesian product of all parameters\n    l_study_path = []\n    logging.info(\n        f\"Now generation cartesian product of all parameters for generation: {generation}\"\n    )\n    for l_values, l_values_for_naming, l_idx in zip(\n        itertools.product(*dic_parameter_lists.values()),\n        itertools.product(*dic_parameter_lists_for_naming.values()),\n        itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()]),\n    ):\n        # Check the idx to keep if conditions are present\n        if array_conditions is not None and not array_conditions[l_idx]:\n            continue\n\n        # Create the path for the study\n        dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n        dic_mutated_parameters_for_naming = dict(\n            zip(dic_parameter_lists.keys(), l_values_for_naming)\n        )\n        suffix_path = \"_\".join(\n            [\n                f\"{parameter}_{value}\"\n                for parameter, value in dic_mutated_parameters_for_naming.items()\n            ]\n        )\n\n        # Remove '_' at the beginning of the suffix path if needed (e.g. for generation)\n        suffix_path = suffix_path.removeprefix(\"_\")\n        # Create final path\n        path = generation_path + suffix_path + \"/\"\n\n        # Add common parameters\n        if generation in self.dic_common_parameters:\n            dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n        # Remove \"\" from mutated parameters, if it's in the dictionary\n        # as it's only used when no scan is done\n        if \"\" in dic_mutated_parameters:\n            dic_mutated_parameters.pop(\"\")\n\n        # Generate the study for current generation\n        self.generate_render_write(\n            generation,\n            path,\n            template_path,\n            dic_mutated_parameters=dic_mutated_parameters,\n        )\n\n        # Append the list of study paths to build the tree later on\n        l_study_path.append(path)\n\n    if not l_study_path:\n        logging.warning(\n            f\"No study paths were created for generation {generation}.\"\n            \"Please check the conditions.\"\n        )\n\n    return l_study_path\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.create_study_for_current_gen","title":"<code>create_study_for_current_gen(generation, study_path, dic_parameter_lists=None, dic_parameter_lists_for_naming=None)</code>","text":"<p>Creates study files for the current generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The name of the current generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the study folder.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study_for_current_gen(\n    self,\n    generation: str,\n    study_path: str,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for the current generation.\n\n    Args:\n        generation (str): The name of the current generation.\n        study_path (str): The path to the study folder.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    executable_path = self.config[\"structure\"][generation][\"executable\"]\n    path_local_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/template_scripts/\"\n\n    # Check if the executable path corresponds to a file\n    if not os.path.isfile(executable_path):\n        # Check if the executable path corresponds to a file in the template folder\n        executable_path_template = f\"{path_local_template}{executable_path}\"\n        if not os.path.isfile(executable_path_template):\n            raise FileNotFoundError(\n                f\"Executable file {executable_path} not found locally nor in the study-da \"\n                \"template folder.\"\n            )\n        else:\n            executable_path = executable_path_template\n\n    # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n    if dic_parameter_lists is not None:\n        # Recursively convert all numpy types to standard types\n        clean_dic(dic_parameter_lists)\n        logging.info(\"An external dictionary of parameters was provided.\")\n    else:\n        logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n    return self.create_scans(\n        generation,\n        study_path,\n        executable_path,\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.eval_conditions","title":"<code>eval_conditions(l_condition, dic_parameter_lists)</code>  <code>staticmethod</code>","text":"<p>Evaluates the conditions to filter out some parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>l_condition</code> <code>list[str]</code> <p>The list of conditions.</p> required <code>dic_parameter_lists</code> <code>dict[str</code> <p>Any]): The dictionary of parameter lists.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the conditions to filter out some parameter values.\n\n    Args:\n        l_condition (list[str]): The list of conditions.\n        dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n    Returns:\n        np.ndarray: The array of conditions.\n    \"\"\"\n    # Initialize the array of parameters as a meshgrid of all parameters\n    l_parameters = list(dic_parameter_lists.values())\n    meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n    # Associate the parameters to their names\n    dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n    # Evaluate the conditions and take the intersection of all conditions\n    array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n    for condition in l_condition:\n        array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.filter_for_concomitant_parameters","title":"<code>filter_for_concomitant_parameters(array_conditions, ll_concomitant_parameters, dic_dimension_indices)</code>  <code>staticmethod</code>","text":"<p>Filters the conditions for concomitant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array_conditions</code> <code>ndarray</code> <p>The array of conditions.</p> required <code>ll_concomitant_parameters</code> <code>list[list[str]]</code> <p>The list of concomitant parameters.</p> required <code>dic_dimension_indices</code> <code>dict[str, int]</code> <p>The dictionary of dimension indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The filtered array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef filter_for_concomitant_parameters(\n    array_conditions: np.ndarray,\n    ll_concomitant_parameters: list[list[str]],\n    dic_dimension_indices: dict[str, int],\n) -&gt; np.ndarray:\n    \"\"\"\n    Filters the conditions for concomitant parameters.\n\n    Args:\n        array_conditions (np.ndarray): The array of conditions.\n        ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n        dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n    Returns:\n        np.ndarray: The filtered array of conditions.\n    \"\"\"\n\n    # Return the array of conditions if no concomitant parameters\n    if not ll_concomitant_parameters:\n        return array_conditions\n\n    # Get the indices of the concomitant parameters\n    ll_idx_concomitant_parameters = [\n        [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n        for concomitant_parameters in ll_concomitant_parameters\n    ]\n\n    # Browse all the values of array_conditions\n    for idx, _ in np.ndenumerate(array_conditions):\n        # Check if the value is on the diagonal of the concomitant parameters\n        for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n            if any(\n                idx[i] != idx[j]\n                for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n            ):\n                array_conditions[idx] = False\n                break\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.generate_render_write","title":"<code>generate_render_write(gen_name, study_path, template_path, dic_mutated_parameters={})</code>","text":"<p>Generates, renders, and writes the study file.</p> <p>Parameters:</p> Name Type Description Default <code>gen_name</code> <code>str</code> <p>The name of the generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the study folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>dic_mutated_parameters</code> <code>dict[str, Any]</code> <p>The dictionary of mutated parameters. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, list[str]]: The study file string and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def generate_render_write(\n    self,\n    gen_name: str,\n    study_path: str,\n    template_path: str,\n    dic_mutated_parameters: dict[str, Any] = {},\n) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n    \"\"\"\n    Generates, renders, and writes the study file.\n\n    Args:\n        gen_name (str): The name of the generation.\n        study_path (str): The path to the study folder.\n        template_path (str): The path to the template folder.\n        dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n            Defaults to {}.\n\n    Returns:\n        tuple[str, list[str]]: The study file string and the list of study paths.\n    \"\"\"\n\n    directory_path_gen = f\"{study_path}\"\n    if not directory_path_gen.endswith(\"/\"):\n        directory_path_gen += \"/\"\n    file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n    logging.info(f'Now rendering generation \"{file_path_gen}\"')\n    # Generate the string of parameters\n    str_parameters = \"{\"\n    for key, value in dic_mutated_parameters.items():\n        if isinstance(value, str):\n            str_parameters += f\"'{key}' : '{value}', \"\n        else:\n            str_parameters += f\"'{key}' : {value}, \"\n    str_parameters += \"}\"\n\n    # Adapt the dict of dependencies to the current generation\n    dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n    # Always load configuration from above generation\n    depth_gen = 1\n    # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n    dic_dependencies = {\n        key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n    }\n\n    # Render and write the study file\n    study_str = self.render(\n        str_parameters,\n        template_path=template_path,\n        dependencies=dic_dependencies,\n    )\n\n    self.write(study_str, file_path_gen)\n    return [directory_path_gen]\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.get_dic_parametric_scans","title":"<code>get_dic_parametric_scans(generation)</code>","text":"<p>Retrieves dictionaries of parametric scan values.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], ndarray | None]</code> <p>tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric scan values, another dictionnary with better naming for the tree creation, and an array of conditions to filter out some parameter values.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def get_dic_parametric_scans(\n    self, generation: str\n) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n    \"\"\"\n    Retrieves dictionaries of parametric scan values.\n\n    Args:\n        generation: The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n            scan values, another dictionnary with better naming for the tree creation, and an\n            array of conditions to filter out some parameter values.\n    \"\"\"\n\n    if generation == \"base\":\n        raise ValueError(\"Generation 'base' should not have scans.\")\n\n    # Remember common parameters as they might be used across generations\n    if \"common_parameters\" in self.config[\"structure\"][generation]:\n        self.dic_common_parameters[generation] = {}\n        for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n            self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                generation\n            ][\"common_parameters\"][parameter]\n\n    # Check that the generation has scans\n    if (\n        \"scans\" not in self.config[\"structure\"][generation]\n        or self.config[\"structure\"][generation][\"scans\"] is None\n    ):\n        dic_parameter_lists = {\"\": [generation]}\n        dic_parameter_lists_for_naming = {\"\": [generation]}\n        array_conditions = None\n        ll_concomitant_parameters = []\n    else:\n        # Browse and collect the parameter space for the generation\n        (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        ) = self.browse_and_collect_parameter_space(generation)\n\n        # Get the dimension corresponding to each parameter\n        dic_dimension_indices = {\n            parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n        }\n\n        # Generate array of conditions to filter out some of the values later\n        # Is an array of True values if no conditions are present\n        array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n        # Filter for concomitant parameters\n        array_conditions = self.filter_for_concomitant_parameters(\n            array_conditions, ll_concomitant_parameters, dic_dimension_indices\n        )\n\n        # Postprocess the parameter lists and update the dictionaries\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n            dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n        )\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        array_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.parse_parameter_space","title":"<code>parse_parameter_space(parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming)</code>","text":"<p>Parses the parameter space for a given parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>str</code> <p>The parameter name.</p> required <code>dic_curr_parameter</code> <code>dict[str, Any]</code> <p>The dictionary of current parameter values.</p> required <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists for naming.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def parse_parameter_space(\n    self,\n    parameter: str,\n    dic_curr_parameter: dict[str, Any],\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Parses the parameter space for a given parameter.\n\n    Args:\n        parameter (str): The parameter name.\n        dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n        dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n    \"\"\"\n\n    if \"linspace\" in dic_curr_parameter:\n        parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"logspace\" in dic_curr_parameter:\n        parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"path_list\" in dic_curr_parameter:\n        l_values_path_list = dic_curr_parameter[\"path_list\"]\n        parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n        dic_parameter_lists_for_naming[parameter] = [\n            f\"{n:02d}\" for n, path in enumerate(parameter_list)\n        ]\n    elif \"list\" in dic_curr_parameter:\n        parameter_list = dic_curr_parameter[\"list\"]\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"expression\" in dic_curr_parameter:\n        parameter_list = np.round(\n            eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n            8,\n        )\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    else:\n        raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n    dic_parameter_lists[parameter] = np.array(parameter_list)\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.postprocess_parameter_lists","title":"<code>postprocess_parameter_lists(dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables)</code>","text":"<p>Post-processes parameter lists by ensuring values are not numpy types and handling nested parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists for naming.</p> required <code>dic_subvariables</code> <code>dict[str, Any]</code> <p>Dictionary containing subvariables for nested parameters.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and parameter lists for naming.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def postprocess_parameter_lists(\n    self,\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n    dic_subvariables: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Post-processes parameter lists by ensuring values are not numpy types and handling nested\n    parameters.\n\n    Args:\n        dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n            for naming.\n        dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n            parameters.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n            parameter lists for naming.\n    \"\"\"\n    for parameter, parameter_list in dic_parameter_lists.items():\n        parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n        # Ensure that all values are not numpy types (to avoid serialization issues)\n        parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n        # Handle nested parameters\n        parameter_list_updated = (\n            convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n            if parameter in dic_subvariables\n            else parameter_list\n        )\n        # Update the dictionaries\n        dic_parameter_lists[parameter] = parameter_list_updated\n        dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.render","title":"<code>render(str_parameters, template_path, dependencies=None)</code>","text":"<p>Renders the study file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>str_parameters</code> <code>str</code> <p>The string representation of parameters to declare/mutate.</p> required <code>template_path</code> <code>str</code> <p>The path to the template file.</p> required <code>dependencies</code> <code>dict[str, str]</code> <p>The dictionary of dependencies. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rendered study file.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def render(\n    self,\n    str_parameters: str,\n    template_path: str,\n    dependencies: Optional[dict[str, str]] = None,\n) -&gt; str:\n    \"\"\"\n    Renders the study file using a template.\n\n    Args:\n        str_parameters (str): The string representation of parameters to declare/mutate.\n        template_path (str): The path to the template file.\n        dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n    Returns:\n        str: The rendered study file.\n    \"\"\"\n\n    # Handle mutable default argument\n    if dependencies is None:\n        dependencies = {}\n\n    # Generate generations from template\n    directory_path = os.path.dirname(template_path)\n    template_name = os.path.basename(template_path)\n    environment = Environment(loader=FileSystemLoader(directory_path))\n    template = environment.get_template(template_name)\n\n    return template.render(parameters=str_parameters, **dependencies)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.write","title":"<code>write(study_str, file_path, format_with_black=True)</code>","text":"<p>Writes the study file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>study_str</code> <code>str</code> <p>The study file string.</p> required <code>file_path</code> <code>str</code> <p>The path to write the study file.</p> required <code>format_with_black</code> <code>bool</code> <p>Whether to format the output file with black. Defaults to True.</p> <code>True</code> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n    \"\"\"\n    Writes the study file to disk.\n\n    Args:\n        study_str (str): The study file string.\n        file_path (str): The path to write the study file.\n        format_with_black (bool, optional): Whether to format the output file with black.\n            Defaults to True.\n    \"\"\"\n\n    # Format the string with black\n    if format_with_black:\n        study_str = format_str(study_str, mode=FileMode())\n\n    # Make folder if it doesn't exist\n    folder = os.path.dirname(file_path)\n    if folder != \"\":\n        os.makedirs(folder, exist_ok=True)\n\n    with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(study_str)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.GenerateScan.write_tree","title":"<code>write_tree(dictionary_tree)</code>","text":"<p>Writes the study tree structure to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write_tree(self, dictionary_tree: dict):\n    \"\"\"\n    Writes the study tree structure to a YAML file.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(\"Writing the tree structure to a YAML file.\")\n    ryaml = yaml.YAML()\n    with open(self.path_tree, \"w\") as yaml_file:\n        ryaml.indent(sequence=4, offset=2)\n        ryaml.dump(dictionary_tree, yaml_file)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan","title":"<code>SubmitScan</code>","text":"Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>class SubmitScan:\n    def __init__(\n        self,\n        path_tree: str,\n        path_python_environment: str,\n        path_python_environment_container: str = \"\",\n        path_container_image: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SubmitScan class.\n\n        Args:\n            path_tree (str): The path to the tree structure.\n            path_python_environment (str): The path to the Python environment.\n            path_python_environment_container (str, optional): The path to the Python environment\n                in the container. Defaults to \"\".\n            path_container_image (Optional[str], optional): The path to the container image.\n                Defaults to None.\n        \"\"\"\n        # Path to study files\n        self.path_tree = path_tree\n\n        # Absolute path to the tree\n        self.abs_path_tree = os.path.abspath(path_tree)\n\n        # Name of the study folder\n        self.study_name = os.path.dirname(path_tree)\n\n        # Absolute path to the study folder (get from the path_tree)\n        self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n        # Path to the python environment, activate with `source path_python_environment`\n        # Turn to absolute path if it is not already\n        if not os.path.isabs(path_python_environment):\n            self.path_python_environment = os.path.abspath(path_python_environment)\n        else:\n            self.path_python_environment = path_python_environment\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment.endswith(\"/bin/activate\"):\n            self.path_python_environment += \"/bin/activate\"\n\n        # Container image (Docker or Singularity, if any)\n        # Turn to absolute path if it is not already\n        if path_container_image is None:\n            self.path_container_image = None\n        elif not os.path.isabs(path_container_image):\n            self.path_container_image = os.path.abspath(path_container_image)\n        else:\n            self.path_container_image = path_container_image\n\n        # Python environment for the container\n        self.path_python_environment_container = path_python_environment_container\n\n        # Ensure that the container image is set if the python environment is set\n        if self.path_container_image and not self.path_python_environment_container:\n            raise ValueError(\n                \"The path to the python environment in the container must be set if the container\"\n                \"image is set.\"\n            )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n            self.path_python_environment_container += \"/bin/activate\"\n\n        # Lock file to avoid concurrent access (softlock as several platforms are used)\n        self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n\n    # dic_tree as a property so that it is reloaded every time it is accessed\n    @property\n    def dic_tree(self) -&gt; dict:\n        \"\"\"\n        Loads the dictionary tree from the path.\n\n        Returns:\n            dict: The loaded dictionary tree.\n        \"\"\"\n        logging.info(f\"Loading tree from {self.path_tree}\")\n        return load_dic_from_path(self.path_tree)[0]\n\n    # Setter for the dic_tree property\n    @dic_tree.setter\n    def dic_tree(self, value: dict) -&gt; None:\n        \"\"\"\n        Writes the dictionary tree to the path.\n\n        Args:\n            value (dict): The dictionary tree to write.\n        \"\"\"\n        logging.info(f\"Writing tree to {self.path_tree}\")\n        write_dic_to_path(value, self.path_tree)\n\n    def configure_jobs(\n        self,\n        force_configure: bool = False,\n        dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n        Args:\n            force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n            dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n                the configuration of the jobs. Defaults to None.\n        \"\"\"\n        # Lock since we are modifying the tree\n        logging.info(\"Acquiring lock to configure jobs\")\n        with self.lock:\n            # Get the tree\n            dic_tree = self.dic_tree\n\n            # Ensure jobs have not been configured already\n            if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n                logging.warning(\"Jobs have already been configured. Skipping.\")\n                return\n\n            # Configure the jobs (add generation and job keys, set status to \"To finish\")\n            dic_tree = ConfigJobs(dic_tree).find_and_configure_jobs(dic_config_jobs)\n\n            # Add the python environment, container image and absolute path of the study to the tree\n            dic_tree[\"python_environment\"] = self.path_python_environment\n            dic_tree[\"container_image\"] = self.path_container_image\n            dic_tree[\"absolute_path\"] = self.abs_path\n            dic_tree[\"status\"] = \"to_finish\"\n            dic_tree[\"configured\"] = True\n\n            # Explicitly set the dic_tree property to force rewrite\n            self.dic_tree = dic_tree\n\n        logging.info(\"Jobs have been configured. Lock released.\")\n\n    def get_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Retrieves all jobs from the configuration, without modifying the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs.\n        \"\"\"\n        # Get a copy of the tree as it's safer\n        with self.lock:\n            dic_tree = self.dic_tree\n        return ConfigJobs(dic_tree).find_all_jobs()\n\n    def generate_run_files(\n        self,\n        dic_tree: dict[str, Any],\n        l_jobs: list[str],\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; dict:\n        \"\"\"\n        Generates run files for the specified jobs.\n\n        Args:\n            dic_tree (dict): The dictionary tree structure.\n            l_jobs (list[str]): List of jobs to submit.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to {}.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n            name_config (str, optional): The name of the configuration file for the study.\n\n        Returns:\n            dict: The updated dictionary tree structure.\n        \"\"\"\n\n        logging.info(\"Generating run files for the jobs to submit\")\n        # Generate the run files for the jobs to submit\n        dic_all_jobs = self.get_all_jobs()\n        for job in l_jobs:\n            l_keys = dic_all_jobs[job][\"l_keys\"]\n            job_name = os.path.basename(job)\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            generation_number = dic_all_jobs[job][\"gen\"]\n            submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n            singularity = \"docker\" in submission_type\n            path_python_environment = (\n                self.path_python_environment_container\n                if singularity\n                else self.path_python_environment\n            )\n\n            # Ensure that the run file does not already exist\n            if \"path_run\" in nested_get(dic_tree, l_keys):\n                path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                    continue\n\n            # Build l_dependencies and add to the kwargs\n            l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n            # Get arguments of current generation\n            dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n            # Mutate the keys\n            dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n            # Build kwargs for the run file\n            kwargs_htc = {\n                \"l_dependencies\": l_dependencies,\n                \"name_config\": name_config,\n            } | dic_args\n\n            run_str = generate_run_file(\n                absolute_job_folder,\n                job_name,\n                path_python_environment,\n                htc=\"htc\" in submission_type,\n                additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n                **kwargs_htc,\n            )\n            # Write the run file\n            path_run_job = f\"{absolute_job_folder}/run.sh\"\n            with open(path_run_job, \"w\") as f:\n                f.write(run_str)\n\n            # Record the path to the run file in the tree\n            nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n        return dic_tree\n\n    def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"\n        Checks the status of all jobs and updates their status in the job dictionary.\n\n        This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n        the job's folder, and updates the job's status accordingly. If at least one job is not\n        finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n        failed, the overall status is set to \"finished\".\n\n        Returns:\n            tuple[dict[str, Any], str]: A tuple containing:\n            - A dictionary with all jobs and their updated statuses.\n            - A string representing the final status (\"to_finish\" or \"finished\").\n        \"\"\"\n        dic_all_jobs = self.get_all_jobs()\n        at_least_one_job_to_finish = False\n        final_status = \"to_finish\"\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n            for job in dic_all_jobs:\n                relative_job_folder = os.path.dirname(job)\n                absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n                # Check if the file .finished exists\n                if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n                # Check if the job failed otherwise (not to resubmit it again)\n                elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n                else:\n                    at_least_one_job_to_finish = True\n\n            if not at_least_one_job_to_finish:\n                dic_tree[\"status\"] = final_status = \"finished\"\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n\n        return dic_all_jobs, final_status\n\n    def submit(\n        self,\n        one_generation_at_a_time: bool = False,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n    ) -&gt; str:\n        \"\"\"\n        Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n        can trigger a throttling mechanism in AFS.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n\n        Returns:\n            str: The final status of the jobs.\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n        if dic_copy_back_per_gen is None:\n            dic_copy_back_per_gen = {}\n\n        # Update the status of all jobs before submitting\n        dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n        if final_status == \"finished\":\n            logging.info(\"All jobs are finished. No need to submit.\")\n            return final_status\n\n        logging.info(\"Acquiring lock to submit jobs\")\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # Submit the jobs\n            self._submit(\n                dic_tree,\n                dic_all_jobs,\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n        logging.info(\"Jobs have been submitted. Lock released.\")\n        return final_status\n\n    def _submit(\n        self,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        one_generation_at_a_time: bool,\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; None:\n        \"\"\"\n        Submits the jobs to the cluster.\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            one_generation_at_a_time (bool): Whether to submit one full generation at a time.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation.\n\n            The following arguments are only used for HTC jobs submission:\n\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation.\n            name_config (str, optional): The name of the configuration file for the study.\n        \"\"\"\n        # Collect dict of list of unfinished jobs for every tree branch and every gen\n        dic_to_submit_by_gen = {}\n        dic_summary_by_gen = {}\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            dic_to_submit_by_gen, dic_summary_by_gen = self._check_job_submit_status(\n                job,\n                dic_tree,\n                dic_all_jobs,\n                dic_to_submit_by_gen,\n                dic_summary_by_gen,\n                dependency_graph,\n            )\n\n        # Only keep the topmost generation if one_generation_at_a_time is True\n        if one_generation_at_a_time:\n            logging.info(\n                \"Cropping list of jobs to submit to ensure only one generation is submitted at \"\n                \"a time.\"\n            )\n            max_gen = max(dic_to_submit_by_gen.keys())\n            dic_to_submit_by_gen = {max_gen: dic_to_submit_by_gen[max_gen]}\n\n        # Convert dic_to_submit_by_gen to contain all requested information\n        l_jobs_to_submit = [job for dic_gen in dic_to_submit_by_gen.values() for job in dic_gen]\n\n        # Generate run files for the jobs to submit\n        # ! Run files are generated at submit and not at configuration as the configuration\n        # ! files are created at the end of each generation\n        dic_tree = self.generate_run_files(\n            dic_tree,\n            l_jobs_to_submit,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n\n        # Create the ClusterSubmission object\n        path_submission_file = f\"{self.abs_path}/{self.study_name}/submission/submission_file.sub\"\n        cluster_submission = ClusterSubmission(\n            self.study_name,\n            l_jobs_to_submit,\n            dic_all_jobs,\n            dic_tree,\n            path_submission_file,\n            self.abs_path,\n        )\n\n        # Write and submit the submission files\n        logging.info(\"Writing and submitting submission files\")\n        dic_submission_files = cluster_submission.write_sub_files(dic_summary_by_gen)\n\n        # Log the state of the jobs\n        self.log_jobs_state(dic_summary_by_gen)\n        for submission_type, (\n            list_of_jobs,\n            l_submission_filenames,\n        ) in dic_submission_files.items():\n            cluster_submission.submit(list_of_jobs, l_submission_filenames, submission_type)\n\n    @staticmethod\n    def log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n        \"\"\"\n        Logs the state of jobs for each generation.\n\n        Args:\n            dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n                and the values are dictionaries summarizing job states.\n                Each summary dictionary should contain the following keys:\n                - 'to_submit_later': int, number of jobs left to submit later\n                - 'running_or_queuing': int, number of jobs running or queuing\n                - 'submitted_now': int, number of jobs submitted now\n                - 'finished': int, number of jobs finished\n                - 'failed': int, number of jobs failed\n                - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n        Returns:\n            None\n        \"\"\"\n        logging.info(\"State of the jobs:\")\n        for gen, dic_summary in dic_summary_by_gen.items():\n            logging.info(\"********************************\")\n            logging.info(f\"Generation {gen}\")\n            logging.info(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n            logging.info(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n            logging.info(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n            logging.info(f\"Jobs finished: {dic_summary['finished']}\")\n            logging.info(f\"Jobs failed: {dic_summary['failed']}\")\n            logging.info(\n                f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\"\n            )\n            logging.info(\"********************************\")\n\n    @staticmethod\n    def _check_job_submit_status(\n        job: str,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        dic_to_submit_by_gen: dict[int, list[str]],\n        dic_summary_by_gen: dict[int, dict[str, int]],\n        dependency_graph: DependencyGraph,\n    ) -&gt; tuple[dict[int, list[str]], dict[int, dict[str, int]]]:\n        \"\"\"\n        Checks the status and dependencies of a job and updates the submission and summary\n        dictionaries.\n\n        Args:\n            job (str): The job identifier.\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            dic_to_submit_by_gen (dict[int, list[str]]): A dictionary where keys are generation\n                numbers and values are lists of jobs to submit for each generation.\n            dic_summary_by_gen (dict[int, dict[str, int]]): A dictionary where keys are generation\n                numbers and values are dictionaries summarizing job states.\n            dependency_graph (DependencyGraph): An object to check job dependencies.\n\n        Returns:\n            tuple[dict[int, list[str]], dict[int, dict[str, int]]]: Updated dictionaries for jobs to\n                submit and job summaries.\n        \"\"\"\n        gen = dic_all_jobs[job][\"gen\"]\n        if gen not in dic_to_submit_by_gen:\n            dic_to_submit_by_gen[gen] = []\n            dic_summary_by_gen[gen] = {\n                \"finished\": 0,\n                \"failed\": 0,\n                \"dependency_failed\": 0,\n                \"running_or_queuing\": 0,\n                \"submitted_now\": 0,\n                \"to_submit_later\": 0,\n            }\n        logging.info(f\"Checking job {job} dependencies and status in tree\")\n        l_dep = dependency_graph.get_unfinished_dependency(job)\n        l_dep_failed = dependency_graph.get_failed_dependency(job)\n\n        # Job will be on hold as it has failed dependencies\n        if len(l_dep_failed) &gt; 0:\n            logging.warning(\n                f\"Job {job} has failed dependencies: {l_dep_failed}, it won't be submitted.\"\n            )\n            dic_summary_by_gen[gen][\"dependency_failed\"] += 1\n\n        # Jobs is waiting for dependencies to finish\n        elif len(l_dep) &gt; 0:\n            dic_summary_by_gen[gen][\"to_submit_later\"] += 1\n\n        # Job dependencies are ok\n        elif len(l_dep) == 0:\n            # But job has failed already\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"failed\":\n                dic_summary_by_gen[gen][\"failed\"] += 1\n\n            # Or job has finished already\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"finished\":\n                dic_summary_by_gen[gen][\"finished\"] += 1\n\n            # Else everything is ok, added to the submit dict\n            else:\n                logging.info(f\"Job {job} is added for submission.\")\n                dic_to_submit_by_gen[gen].append(job)\n                # We'll determine which jobs actually have to be submitted and which jobs\n                # are running at the end of the function, after querying the cluster or the local pc\n\n        return dic_to_submit_by_gen, dic_summary_by_gen\n\n    def keep_submit_until_done(\n        self,\n        one_generation_at_a_time: bool = False,\n        wait_time: float = 30,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n    ) -&gt; None:\n        \"\"\"\n        Keeps submitting jobs until all jobs are finished or failed.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            wait_time (float, optional): The wait time between submissions in minutes.\n                Defaults to 30.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n\n\n        Returns:\n            None\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n\n        if wait_time &lt; 1 / 20:\n            logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n            logging.warning(\"Setting wait time to 10 seconds.\")\n            wait_time = 10 / 60\n\n        # I don't need to lock the tree here since the status cheking is read only and\n        # the lock is acquired in the submit method for the submission\n        while (\n            self.submit(\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n            != \"finished\"\n        ):\n            # Wait for a certain amount of time before checking again\n            logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n            time.sleep(wait_time * 60)\n\n        logging.info(\"All jobs are finished.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.dic_tree","title":"<code>dic_tree: dict</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the dictionary tree from the path.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded dictionary tree.</p>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.__init__","title":"<code>__init__(path_tree, path_python_environment, path_python_environment_container='', path_container_image=None)</code>","text":"<p>Initializes the SubmitScan class.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree structure.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the Python environment.</p> required <code>path_python_environment_container</code> <code>str</code> <p>The path to the Python environment in the container. Defaults to \"\".</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def __init__(\n    self,\n    path_tree: str,\n    path_python_environment: str,\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SubmitScan class.\n\n    Args:\n        path_tree (str): The path to the tree structure.\n        path_python_environment (str): The path to the Python environment.\n        path_python_environment_container (str, optional): The path to the Python environment\n            in the container. Defaults to \"\".\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n    \"\"\"\n    # Path to study files\n    self.path_tree = path_tree\n\n    # Absolute path to the tree\n    self.abs_path_tree = os.path.abspath(path_tree)\n\n    # Name of the study folder\n    self.study_name = os.path.dirname(path_tree)\n\n    # Absolute path to the study folder (get from the path_tree)\n    self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n    # Path to the python environment, activate with `source path_python_environment`\n    # Turn to absolute path if it is not already\n    if not os.path.isabs(path_python_environment):\n        self.path_python_environment = os.path.abspath(path_python_environment)\n    else:\n        self.path_python_environment = path_python_environment\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment.endswith(\"/bin/activate\"):\n        self.path_python_environment += \"/bin/activate\"\n\n    # Container image (Docker or Singularity, if any)\n    # Turn to absolute path if it is not already\n    if path_container_image is None:\n        self.path_container_image = None\n    elif not os.path.isabs(path_container_image):\n        self.path_container_image = os.path.abspath(path_container_image)\n    else:\n        self.path_container_image = path_container_image\n\n    # Python environment for the container\n    self.path_python_environment_container = path_python_environment_container\n\n    # Ensure that the container image is set if the python environment is set\n    if self.path_container_image and not self.path_python_environment_container:\n        raise ValueError(\n            \"The path to the python environment in the container must be set if the container\"\n            \"image is set.\"\n        )\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n        self.path_python_environment_container += \"/bin/activate\"\n\n    # Lock file to avoid concurrent access (softlock as several platforms are used)\n    self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.check_and_update_all_jobs_status","title":"<code>check_and_update_all_jobs_status()</code>","text":"<p>Checks the status of all jobs and updates their status in the job dictionary.</p> <p>This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in the job's folder, and updates the job's status accordingly. If at least one job is not finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or failed, the overall status is set to \"finished\".</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>tuple[dict[str, Any], str]: A tuple containing:</p> <code>str</code> <ul> <li>A dictionary with all jobs and their updated statuses.</li> </ul> <code>tuple[dict[str, Any], str]</code> <ul> <li>A string representing the final status (\"to_finish\" or \"finished\").</li> </ul> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"\n    Checks the status of all jobs and updates their status in the job dictionary.\n\n    This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n    the job's folder, and updates the job's status accordingly. If at least one job is not\n    finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n    failed, the overall status is set to \"finished\".\n\n    Returns:\n        tuple[dict[str, Any], str]: A tuple containing:\n        - A dictionary with all jobs and their updated statuses.\n        - A string representing the final status (\"to_finish\" or \"finished\").\n    \"\"\"\n    dic_all_jobs = self.get_all_jobs()\n    at_least_one_job_to_finish = False\n    final_status = \"to_finish\"\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n        for job in dic_all_jobs:\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            # Check if the file .finished exists\n            if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n            # Check if the job failed otherwise (not to resubmit it again)\n            elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n            else:\n                at_least_one_job_to_finish = True\n\n        if not at_least_one_job_to_finish:\n            dic_tree[\"status\"] = final_status = \"finished\"\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n\n    return dic_all_jobs, final_status\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.configure_jobs","title":"<code>configure_jobs(force_configure=False, dic_config_jobs=None)</code>","text":"<p>Configures the jobs by modifying the tree structure and creating the run files for each job.</p> <p>Parameters:</p> Name Type Description Default <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def configure_jobs(\n    self,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n    Args:\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n    \"\"\"\n    # Lock since we are modifying the tree\n    logging.info(\"Acquiring lock to configure jobs\")\n    with self.lock:\n        # Get the tree\n        dic_tree = self.dic_tree\n\n        # Ensure jobs have not been configured already\n        if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n            logging.warning(\"Jobs have already been configured. Skipping.\")\n            return\n\n        # Configure the jobs (add generation and job keys, set status to \"To finish\")\n        dic_tree = ConfigJobs(dic_tree).find_and_configure_jobs(dic_config_jobs)\n\n        # Add the python environment, container image and absolute path of the study to the tree\n        dic_tree[\"python_environment\"] = self.path_python_environment\n        dic_tree[\"container_image\"] = self.path_container_image\n        dic_tree[\"absolute_path\"] = self.abs_path\n        dic_tree[\"status\"] = \"to_finish\"\n        dic_tree[\"configured\"] = True\n\n        # Explicitly set the dic_tree property to force rewrite\n        self.dic_tree = dic_tree\n\n    logging.info(\"Jobs have been configured. Lock released.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.generate_run_files","title":"<code>generate_run_files(dic_tree, l_jobs, dic_additional_commands_per_gen, dic_dependencies_per_gen, dic_copy_back_per_gen, name_config)</code>","text":"<p>Generates run files for the specified jobs.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary tree structure.</p> required <code>l_jobs</code> <code>list[str]</code> <p>List of jobs to submit.</p> required <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to {}.</p> required <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission.</p> required <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".</p> required <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def generate_run_files(\n    self,\n    dic_tree: dict[str, Any],\n    l_jobs: list[str],\n    dic_additional_commands_per_gen: dict[int, str],\n    dic_dependencies_per_gen: dict[int, list[str]],\n    dic_copy_back_per_gen: dict[int, dict[str, bool]],\n    name_config: str,\n) -&gt; dict:\n    \"\"\"\n    Generates run files for the specified jobs.\n\n    Args:\n        dic_tree (dict): The dictionary tree structure.\n        l_jobs (list[str]): List of jobs to submit.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to {}.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n        name_config (str, optional): The name of the configuration file for the study.\n\n    Returns:\n        dict: The updated dictionary tree structure.\n    \"\"\"\n\n    logging.info(\"Generating run files for the jobs to submit\")\n    # Generate the run files for the jobs to submit\n    dic_all_jobs = self.get_all_jobs()\n    for job in l_jobs:\n        l_keys = dic_all_jobs[job][\"l_keys\"]\n        job_name = os.path.basename(job)\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n        generation_number = dic_all_jobs[job][\"gen\"]\n        submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n        singularity = \"docker\" in submission_type\n        path_python_environment = (\n            self.path_python_environment_container\n            if singularity\n            else self.path_python_environment\n        )\n\n        # Ensure that the run file does not already exist\n        if \"path_run\" in nested_get(dic_tree, l_keys):\n            path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                continue\n\n        # Build l_dependencies and add to the kwargs\n        l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n        # Get arguments of current generation\n        dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n        # Mutate the keys\n        dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n        # Build kwargs for the run file\n        kwargs_htc = {\n            \"l_dependencies\": l_dependencies,\n            \"name_config\": name_config,\n        } | dic_args\n\n        run_str = generate_run_file(\n            absolute_job_folder,\n            job_name,\n            path_python_environment,\n            htc=\"htc\" in submission_type,\n            additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n            **kwargs_htc,\n        )\n        # Write the run file\n        path_run_job = f\"{absolute_job_folder}/run.sh\"\n        with open(path_run_job, \"w\") as f:\n            f.write(run_str)\n\n        # Record the path to the run file in the tree\n        nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Retrieves all jobs from the configuration, without modifying the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def get_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Retrieves all jobs from the configuration, without modifying the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs.\n    \"\"\"\n    # Get a copy of the tree as it's safer\n    with self.lock:\n        dic_tree = self.dic_tree\n    return ConfigJobs(dic_tree).find_all_jobs()\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.keep_submit_until_done","title":"<code>keep_submit_until_done(one_generation_at_a_time=False, wait_time=30, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Keeps submitting jobs until all jobs are finished or failed.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def keep_submit_until_done(\n    self,\n    one_generation_at_a_time: bool = False,\n    wait_time: float = 30,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; None:\n    \"\"\"\n    Keeps submitting jobs until all jobs are finished or failed.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes.\n            Defaults to 30.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n\n    Returns:\n        None\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n\n    if wait_time &lt; 1 / 20:\n        logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n        logging.warning(\"Setting wait time to 10 seconds.\")\n        wait_time = 10 / 60\n\n    # I don't need to lock the tree here since the status cheking is read only and\n    # the lock is acquired in the submit method for the submission\n    while (\n        self.submit(\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n        != \"finished\"\n    ):\n        # Wait for a certain amount of time before checking again\n        logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n        time.sleep(wait_time * 60)\n\n    logging.info(\"All jobs are finished.\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.log_jobs_state","title":"<code>log_jobs_state(dic_summary_by_gen)</code>  <code>staticmethod</code>","text":"<p>Logs the state of jobs for each generation.</p> <p>Parameters:</p> Name Type Description Default <code>dic_summary_by_gen</code> <code>dict</code> <p>A dictionary where the keys are generation numbers and the values are dictionaries summarizing job states. Each summary dictionary should contain the following keys: - 'to_submit_later': int, number of jobs left to submit later - 'running_or_queuing': int, number of jobs running or queuing - 'submitted_now': int, number of jobs submitted now - 'finished': int, number of jobs finished - 'failed': int, number of jobs failed - 'dependency_failed': int, number of jobs on hold due to failed dependencies</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>@staticmethod\ndef log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n    \"\"\"\n    Logs the state of jobs for each generation.\n\n    Args:\n        dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n            and the values are dictionaries summarizing job states.\n            Each summary dictionary should contain the following keys:\n            - 'to_submit_later': int, number of jobs left to submit later\n            - 'running_or_queuing': int, number of jobs running or queuing\n            - 'submitted_now': int, number of jobs submitted now\n            - 'finished': int, number of jobs finished\n            - 'failed': int, number of jobs failed\n            - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n    Returns:\n        None\n    \"\"\"\n    logging.info(\"State of the jobs:\")\n    for gen, dic_summary in dic_summary_by_gen.items():\n        logging.info(\"********************************\")\n        logging.info(f\"Generation {gen}\")\n        logging.info(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n        logging.info(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n        logging.info(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n        logging.info(f\"Jobs finished: {dic_summary['finished']}\")\n        logging.info(f\"Jobs failed: {dic_summary['failed']}\")\n        logging.info(\n            f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\"\n        )\n        logging.info(\"********************************\")\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.SubmitScan.submit","title":"<code>submit(one_generation_at_a_time=False, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final status of the jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def submit(\n    self,\n    one_generation_at_a_time: bool = False,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; str:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n    Returns:\n        str: The final status of the jobs.\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n    if dic_copy_back_per_gen is None:\n        dic_copy_back_per_gen = {}\n\n    # Update the status of all jobs before submitting\n    dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n    if final_status == \"finished\":\n        logging.info(\"All jobs are finished. No need to submit.\")\n        return final_status\n\n    logging.info(\"Acquiring lock to submit jobs\")\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # Submit the jobs\n        self._submit(\n            dic_tree,\n            dic_all_jobs,\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n    logging.info(\"Jobs have been submitted. Lock released.\")\n    return final_status\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output, l_group_by_parameters, only_keep_lost_particles, l_parameters_to_keep\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.create","title":"<code>create(path_config_scan='config_scan.yaml', force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None)</code>","text":"<p>Create a study based on the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path_config_scan</code> <code>str</code> <p>Path to the configuration file for the scan. Defaults to \"config_scan.yaml\".</p> <code>'config_scan.yaml'</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to force overwrite the study. Defaults to True.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the scan, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the naming of the scan subfolders, if not provided through the scan config. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The path to the tree file and the name of the main configuration file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create(\n    path_config_scan: str = \"config_scan.yaml\",\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Create a study based on the configuration file.\n\n    Args:\n        path_config_scan (str, optional): Path to the configuration file for the scan.\n            Defaults to \"config_scan.yaml\".\n        force_overwrite (bool, optional): Flag to force overwrite the study. Defaults to True.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the scan, if not provided through the scan config. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the naming of the scan subfolders, if not provided through the scan\n            config. Defaults to None.\n\n    Returns:\n        tuple[str, str]: The path to the tree file and the name of the main configuration file.\n    \"\"\"\n    logging.info(f\"Create study from configuration file: {path_config_scan}\")\n    study = GenerateScan(path_config=path_config_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n        dic_parameter_all_gen=dic_parameter_all_gen,\n        dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n    )\n\n    # Get variables of interest for the submission\n    path_tree = study.path_tree\n    name_main_configuration = study.config[\"dependencies\"][\"main_configuration\"]\n\n    return path_tree, name_main_configuration\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.create_single_job","title":"<code>create_single_job(name_main_configuration, name_executable_generation_1, name_executable_generation_2=None, name_executable_generation_3=None, name_study='single_job_study', force_overwrite=False)</code>","text":"<p>Create a single job study (not a parametric scan) with the specified configuration and executables. Limited to three generations.</p> <p>Parameters:</p> Name Type Description Default <code>name_main_configuration</code> <code>str</code> <p>The name of the main configuration file for the study.</p> required <code>name_executable_generation_1</code> <code>str</code> <p>The name of the executable for the first generation.</p> required <code>name_executable_generation_2</code> <code>Optional[str]</code> <p>The name of the executable for the second generation. Defaults to None.</p> <code>None</code> <code>name_executable_generation_3</code> <code>Optional[str]</code> <p>The name of the executable for the third generation. Defaults to None.</p> <code>None</code> <code>name_study</code> <code>str</code> <p>The name of the study. Defaults to \"single_job_study\".</p> <code>'single_job_study'</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to force overwrite existing files. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the tree file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create_single_job(\n    name_main_configuration: str,\n    name_executable_generation_1: str,\n    name_executable_generation_2: Optional[str] = None,\n    name_executable_generation_3: Optional[str] = None,\n    name_study: str = \"single_job_study\",\n    force_overwrite: bool = False,\n) -&gt; str:\n    \"\"\"\n    Create a single job study (not a parametric scan) with the specified configuration and\n    executables. Limited to three generations.\n\n    Args:\n        name_main_configuration (str): The name of the main configuration file for the study.\n        name_executable_generation_1 (str): The name of the executable for the first generation.\n        name_executable_generation_2 (Optional[str], optional): The name of the executable for the\n            second generation. Defaults to None.\n        name_executable_generation_3 (Optional[str], optional): The name of the executable for the\n            third generation. Defaults to None.\n        name_study (str, optional): The name of the study. Defaults to \"single_job_study\".\n        force_overwrite (bool, optional): Whether to force overwrite existing files.\n            Defaults to True.\n\n    Returns:\n        str: The path to the tree file.\n    \"\"\"\n    # Generate the scan dictionnary\n    dic_scan = {\n        \"name\": name_study,\n        \"dependencies\": {\"main_configuration\": name_main_configuration},\n        \"structure\": {\n            \"generation_1\": {\n                \"executable\": name_executable_generation_1,\n            },\n        },\n    }\n\n    if name_executable_generation_2 is not None:\n        dic_scan[\"structure\"][\"generation_2\"] = {\n            \"executable\": name_executable_generation_2,\n        }\n\n    if name_executable_generation_3 is not None:\n        dic_scan[\"structure\"][\"generation_3\"] = {\n            \"executable\": name_executable_generation_3,\n        }\n\n    # Create the study\n    logging.info(f\"Create single job study: {name_study}\")\n    study = GenerateScan(dic_scan=dic_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n    )\n\n    return study.path_tree\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, betx_value=np.nan, bety_value=np.nan, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>betx_value</code> <code>float</code> <p>The value of the horizontal beta function. Defaults to np.nan.</p> <code>nan</code> <code>bety_value</code> <code>float</code> <p>The value of the vertical beta function. Defaults to np.nan.</p> <code>nan</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing. Defaults to \"flathv\".</p> <code>None</code> <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    betx_value: float = np.nan,\n    bety_value: float = np.nan,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        betx_value (float, optional): The value of the horizontal beta function. Defaults to np.nan.\n        bety_value (float, optional): The value of the vertical beta function. Defaults to np.nan.\n        crossing_type (str, optional): The type of crossing. Defaults to \"flathv\".\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n    # Find out what is the crossing type\n    crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data)\n    energy_str = get_energy_str(dataframe_data)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(betx_value, bety_value)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n\n    return title\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', output_path='output.png', output_path_html='output.html', display_plot=True)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/index.html#study_da.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, symmetric_missing=True, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.pdf', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.</p> <code>True</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.pdf'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    symmetric_missing: bool = True,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.pdf\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values with. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            raise NotImplementedError(\"Interpolation of missing values is not implemented yet\")\n\n    # Mask the lower or upper triangle\n    if mask_lower_triangle or mask_upper_triangle:\n        data_array_masked = _mask(mask_lower_triangle, mask_upper_triangle, data_array, k_masking)\n    else:\n        data_array_masked = data_array\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        plt.savefig(output_path, bbox_inches=\"tight\")\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/study_da.html","title":"study_da","text":""},{"location":"reference/study_da/study_da.html#study_da.study_da.create","title":"<code>create(path_config_scan='config_scan.yaml', force_overwrite=False, dic_parameter_all_gen=None, dic_parameter_all_gen_naming=None)</code>","text":"<p>Create a study based on the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path_config_scan</code> <code>str</code> <p>Path to the configuration file for the scan. Defaults to \"config_scan.yaml\".</p> <code>'config_scan.yaml'</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to force overwrite the study. Defaults to True.</p> <code>False</code> <code>dic_parameter_all_gen</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the scan, if not provided through the scan config. Defaults to None.</p> <code>None</code> <code>dic_parameter_all_gen_naming</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>Dictionary of parameters for the naming of the scan subfolders, if not provided through the scan config. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The path to the tree file and the name of the main configuration file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create(\n    path_config_scan: str = \"config_scan.yaml\",\n    force_overwrite: bool = False,\n    dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n    dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Create a study based on the configuration file.\n\n    Args:\n        path_config_scan (str, optional): Path to the configuration file for the scan.\n            Defaults to \"config_scan.yaml\".\n        force_overwrite (bool, optional): Flag to force overwrite the study. Defaults to True.\n        dic_parameter_all_gen (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the scan, if not provided through the scan config. Defaults to None.\n        dic_parameter_all_gen_naming (Optional[dict[str, dict[str, Any]]], optional): Dictionary of\n            parameters for the naming of the scan subfolders, if not provided through the scan\n            config. Defaults to None.\n\n    Returns:\n        tuple[str, str]: The path to the tree file and the name of the main configuration file.\n    \"\"\"\n    logging.info(f\"Create study from configuration file: {path_config_scan}\")\n    study = GenerateScan(path_config=path_config_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n        dic_parameter_all_gen=dic_parameter_all_gen,\n        dic_parameter_all_gen_naming=dic_parameter_all_gen_naming,\n    )\n\n    # Get variables of interest for the submission\n    path_tree = study.path_tree\n    name_main_configuration = study.config[\"dependencies\"][\"main_configuration\"]\n\n    return path_tree, name_main_configuration\n</code></pre>"},{"location":"reference/study_da/study_da.html#study_da.study_da.create_single_job","title":"<code>create_single_job(name_main_configuration, name_executable_generation_1, name_executable_generation_2=None, name_executable_generation_3=None, name_study='single_job_study', force_overwrite=False)</code>","text":"<p>Create a single job study (not a parametric scan) with the specified configuration and executables. Limited to three generations.</p> <p>Parameters:</p> Name Type Description Default <code>name_main_configuration</code> <code>str</code> <p>The name of the main configuration file for the study.</p> required <code>name_executable_generation_1</code> <code>str</code> <p>The name of the executable for the first generation.</p> required <code>name_executable_generation_2</code> <code>Optional[str]</code> <p>The name of the executable for the second generation. Defaults to None.</p> <code>None</code> <code>name_executable_generation_3</code> <code>Optional[str]</code> <p>The name of the executable for the third generation. Defaults to None.</p> <code>None</code> <code>name_study</code> <code>str</code> <p>The name of the study. Defaults to \"single_job_study\".</p> <code>'single_job_study'</code> <code>force_overwrite</code> <code>bool</code> <p>Whether to force overwrite existing files. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the tree file.</p> Source code in <code>study_da/study_da.py</code> <pre><code>def create_single_job(\n    name_main_configuration: str,\n    name_executable_generation_1: str,\n    name_executable_generation_2: Optional[str] = None,\n    name_executable_generation_3: Optional[str] = None,\n    name_study: str = \"single_job_study\",\n    force_overwrite: bool = False,\n) -&gt; str:\n    \"\"\"\n    Create a single job study (not a parametric scan) with the specified configuration and\n    executables. Limited to three generations.\n\n    Args:\n        name_main_configuration (str): The name of the main configuration file for the study.\n        name_executable_generation_1 (str): The name of the executable for the first generation.\n        name_executable_generation_2 (Optional[str], optional): The name of the executable for the\n            second generation. Defaults to None.\n        name_executable_generation_3 (Optional[str], optional): The name of the executable for the\n            third generation. Defaults to None.\n        name_study (str, optional): The name of the study. Defaults to \"single_job_study\".\n        force_overwrite (bool, optional): Whether to force overwrite existing files.\n            Defaults to True.\n\n    Returns:\n        str: The path to the tree file.\n    \"\"\"\n    # Generate the scan dictionnary\n    dic_scan = {\n        \"name\": name_study,\n        \"dependencies\": {\"main_configuration\": name_main_configuration},\n        \"structure\": {\n            \"generation_1\": {\n                \"executable\": name_executable_generation_1,\n            },\n        },\n    }\n\n    if name_executable_generation_2 is not None:\n        dic_scan[\"structure\"][\"generation_2\"] = {\n            \"executable\": name_executable_generation_2,\n        }\n\n    if name_executable_generation_3 is not None:\n        dic_scan[\"structure\"][\"generation_3\"] = {\n            \"executable\": name_executable_generation_3,\n        }\n\n    # Create the study\n    logging.info(f\"Create single job study: {name_study}\")\n    study = GenerateScan(dic_scan=dic_scan)\n    study.create_study(\n        force_overwrite=force_overwrite,\n    )\n\n    return study.path_tree\n</code></pre>"},{"location":"reference/study_da/study_da.html#study_da.study_da.submit","title":"<code>submit(path_tree, path_python_environment, path_python_environment_container='', path_container_image=None, force_configure=False, dic_config_jobs=None, one_generation_at_a_time=False, keep_submit_until_done=False, wait_time=30, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the python environment.</p> required <code>path_python_environment_container</code> <code>str</code> <p>The path to the python environment in the container.</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>keep_submit_until_done</code> <code>bool</code> <p>Whether to keep submitting jobs until all jobs are finished or failed. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/study_da.py</code> <pre><code>def submit(\n    path_tree: str,\n    path_python_environment: str,\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    one_generation_at_a_time: bool = False,\n    keep_submit_until_done: bool = False,\n    wait_time: float = 30,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; None:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        path_tree (str): The path to the tree file.\n        path_python_environment (str): The path to the python environment.\n        path_python_environment_container (str): The path to the python environment in the\n            container.\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        keep_submit_until_done (bool, optional): Whether to keep submitting jobs until all jobs\n            are finished or failed. Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes. Defaults to 30.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n    Returns:\n        None\n    \"\"\"\n    # Instantiate the study (does not affect already existing study)\n    study_sub = SubmitScan(\n        path_tree=path_tree,\n        path_python_environment=path_python_environment,\n        path_python_environment_container=path_python_environment_container,\n        path_container_image=path_container_image,\n    )\n\n    # Configure the jobs (will only configure if not already done)\n    study_sub.configure_jobs(force_configure=force_configure, dic_config_jobs=dic_config_jobs)\n\n    # Submit the jobs (only submit the jobs that are not already submitted or finished)\n    if keep_submit_until_done:\n        study_sub.keep_submit_until_done(\n            wait_time=wait_time,\n            one_generation_at_a_time=one_generation_at_a_time,\n            dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n    else:\n        study_sub.submit(\n            one_generation_at_a_time=one_generation_at_a_time,\n            dic_additional_commands_per_gen=dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html","title":"generate","text":""},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider","title":"<code>MadCollider</code>","text":"<p>MadCollider class is responsible for setting up and managing the collider environment using MAD-X and xsuite.</p> <p>Attributes:</p> Name Type Description <code>sanity_checks</code> <code>bool</code> <p>Flag to enable or disable sanity checks.</p> <code>links</code> <code>str</code> <p>Path to the links configuration.</p> <code>beam_config</code> <code>dict</code> <p>Configuration for the beam.</p> <code>optics</code> <code>str</code> <p>Path to the optics file.</p> <code>enable_imperfections</code> <code>bool</code> <p>Flag to enable or disable imperfections.</p> <code>enable_knob_synthesis</code> <code>bool</code> <p>Flag to enable or disable knob synthesis.</p> <code>rename_coupling_knobs</code> <code>bool</code> <p>Flag to enable or disable renaming of coupling knobs.</p> <code>pars_for_imperfections</code> <code>dict</code> <p>Parameters for imperfections.</p> <code>ver_lhc_run</code> <code>float | None</code> <p>Version of LHC run.</p> <code>ver_hllhc_optics</code> <code>float | None</code> <p>Version of HL-LHC optics.</p> <code>ions</code> <code>bool</code> <p>Flag to indicate if ions are used.</p> <code>phasing</code> <code>dict</code> <p>Phasing configuration.</p> <code>path_collider_file_for_configuration_as_output</code> <code>str</code> <p>Path to save the collider.</p> <code>compress</code> <code>bool</code> <p>Flag to enable or disable compression of collider file.</p> <p>Methods:</p> Name Description <code>ost</code> <p>Property to get the appropriate optics specific tools.</p> <code>prepare_mad_collider</code> <p>Prepares the MAD-X collider environment.</p> <code>build_collider</code> <p>Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.</p> <code>activate_RF_and_twiss</code> <p>xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.</p> <code>check_xsuite_lattices</code> <p>xt.Line) -&gt; None: Checks the xsuite lattices.</p> <code>write_collider_to_disk</code> <p>xt.Multiline) -&gt; None: Writes the collider to disk and optionally compresses it.</p> <code>clean_temporary_files</code> <p>Cleans up temporary files created during the process.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>class MadCollider:\n    \"\"\"\n    MadCollider class is responsible for setting up and managing the collider environment using\n    MAD-X and xsuite.\n\n    Attributes:\n        sanity_checks (bool): Flag to enable or disable sanity checks.\n        links (str): Path to the links configuration.\n        beam_config (dict): Configuration for the beam.\n        optics (str): Path to the optics file.\n        enable_imperfections (bool): Flag to enable or disable imperfections.\n        enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n        rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling knobs.\n        pars_for_imperfections (dict): Parameters for imperfections.\n        ver_lhc_run (float | None): Version of LHC run.\n        ver_hllhc_optics (float | None): Version of HL-LHC optics.\n        ions (bool): Flag to indicate if ions are used.\n        phasing (dict): Phasing configuration.\n        path_collider_file_for_configuration_as_output (str): Path to save the collider.\n        compress (bool): Flag to enable or disable compression of collider file.\n\n    Methods:\n        ost: Property to get the appropriate optics specific tools.\n        prepare_mad_collider() -&gt; tuple[Madx, Madx]: Prepares the MAD-X collider environment.\n        build_collider(mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.\n        activate_RF_and_twiss(collider: xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.\n        check_xsuite_lattices(line: xt.Line) -&gt; None: Checks the xsuite lattices.\n        write_collider_to_disk(collider: xt.Multiline) -&gt; None: Writes the collider to disk and\n            optionally compresses it.\n        clean_temporary_files() -&gt; None: Cleans up temporary files created during the process.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initializes the MadCollider class with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the following keys:\n                - sanity_checks (bool): Flag to enable or disable sanity checks.\n                - links (str): Path to the links configuration.\n                - beam_config (dict): Configuration for the beam.\n                - optics_file (str): Path to the optics file.\n                - enable_imperfections (bool): Flag to enable or disable imperfections.\n                - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n                - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                    knobs.\n                - pars_for_imperfections (dict): Parameters for imperfections.\n                - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n                - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n                - ions (bool): Flag to indicate if ions are used.\n                - phasing (dict): Configuration for phasing.\n                - path_collider_file_for_configuration_as_output (str): Path to the collider.\n                - compress (bool): Flag to enable or disable compression.\n        \"\"\"\n        # Configuration variables\n        self.sanity_checks: bool = configuration[\"sanity_checks\"]\n        self.links: str = configuration[\"links\"]\n        self.beam_config: dict = configuration[\"beam_config\"]\n        self.optics: str = configuration[\"optics_file\"]\n        self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n        self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n        self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n        self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n        self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n        self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n        self.ions: bool = configuration[\"ions\"]\n        self.phasing: dict = configuration[\"phasing\"]\n\n        # Optics specific tools\n        self._ost = None\n\n        # Path to disk and compression\n        self.path_collider_file_for_configuration_as_output = configuration[\n            \"path_collider_file_for_configuration_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def ost(self) -&gt; Any:\n        \"\"\"\n        Determines and returns the appropriate optics-specific tools (OST) based on the\n        version of HLLHC optics or LHC run configuration.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics-specific tools are available for the given configuration.\n\n        Returns:\n            Any: The appropriate OST module based on the configuration.\n        \"\"\"\n        if self._ost is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._ost = ost_hllhc16\n                    case 1.3:\n                        self._ost = ost_hllhc13\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._ost = ost_runIII_ions if self.ions else ost_runIII\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._ost\n\n    def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n        # sourcery skip: extract-duplicate-method\n        \"\"\"\n        Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n        This method performs the following steps:\n        1. Creates the MAD-X environment using the provided links.\n        2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n        3. Builds the sequences for both beams using the provided beam configuration.\n        4. Applies the specified optics to the beam 1/2 sequence.\n        5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n            the MAD-X lattices.\n        6. Applies the specified optics to the beam 4 sequence.\n        7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n            the MAD-X lattices.\n\n        Returns:\n            tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n        \"\"\"\n        # Make mad environment\n        xm.make_mad_environment(links=self.links)\n\n        # Start mad\n        mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n        mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n        # Build sequences\n        self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n        self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n        # Apply optics (only for b1b2, b4 will be generated from b1b2)\n        self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n        if self.sanity_checks:\n            mad_b1b2.use(sequence=\"lhcb1\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n            mad_b1b2.use(sequence=\"lhcb2\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n\n        # Apply optics (only for b4, just for check)\n        self.ost.apply_optics(mad_b4, optics_file=self.optics)\n        if self.sanity_checks:\n            mad_b4.use(sequence=\"lhcb2\")\n            mad_b4.twiss()\n            # ! Investigate why this is failing for run III\n            try:\n                self.ost.check_madx_lattices(mad_b4)\n            except AssertionError:\n                logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n        return mad_b1b2, mad_b4\n\n    def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n        \"\"\"\n        Build an xsuite collider using provided MAD-X sequences and configuration.\n\n        Parameters:\n        mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n        mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n        Returns:\n        xt.Multiline: Constructed xsuite collider.\n\n        Notes:\n        - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n        - Builds the xsuite collider with the specified sequences and configuration.\n        - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n        \"\"\"\n        # Ensure proper types to avoid assert errors\n        if self.ver_lhc_run is not None:\n            self.ver_lhc_run = float(self.ver_lhc_run)\n        if self.ver_hllhc_optics is not None:\n            self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n        # Build xsuite collider\n        collider = xlhc.build_xsuite_collider(\n            sequence_b1=mad_b1b2.sequence.lhcb1,\n            sequence_b2=mad_b1b2.sequence.lhcb2,\n            sequence_b4=mad_b4.sequence.lhcb2,\n            beam_config=self.beam_config,\n            enable_imperfections=self.enable_imperfections,\n            enable_knob_synthesis=self.enable_knob_synthesis,\n            rename_coupling_knobs=self.rename_coupling_knobs,\n            pars_for_imperfections=self.pars_for_imperfections,\n            ver_lhc_run=self.ver_lhc_run,\n            ver_hllhc_optics=self.ver_hllhc_optics,\n        )\n        collider.build_trackers()\n\n        if self.sanity_checks:\n            collider[\"lhcb1\"].twiss(method=\"4d\")\n            collider[\"lhcb2\"].twiss(method=\"4d\")\n\n        return collider\n\n    def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Activates RF and Twiss parameters for the given collider.\n\n        This method sets the RF knobs for the collider using the values specified\n        in the `phasing` attribute. It also performs sanity checks on the collider\n        lattices if the `sanity_checks` attribute is set to True.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        # Define a RF knobs\n        collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n        collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n        collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n        if self.sanity_checks:\n            for my_line in [\"lhcb1\", \"lhcb2\"]:\n                self.check_xsuite_lattices(collider[my_line])\n\n    def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n        \"\"\"\n        Check the Twiss parameters and tune values for a given xsuite Line object.\n\n        This method computes the Twiss parameters for the provided `line` using the\n        6-dimensional method with a specified matrix stability tolerance. It then\n        prints the Twiss results at all interaction points (IPs) and the horizontal\n        (Qx) and vertical (Qy) tune values.\n\n        Args:\n            line (xt.Line): The xsuite Line object for which to compute and display\n                            the Twiss parameters and tune values.\n\n        Returns:\n            None\n        \"\"\"\n        tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n        print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n        print(tw.rows[\"ip.*\"])\n        # print qx and qy\n        print(f\"--- Now displaying Qx and Qy for line {line}---\")\n        print(tw.qx, tw.qy)\n\n    def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n        file.\n\n        Args:\n            collider (xt.Multiline): The collider object to be saved.\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an issue creating the directory or writing the file.\n\n        Notes:\n            - The method ensures that the directory specified in\n                `self.path_collider_file_for_configuration_as_output` exists.\n            - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n                storage usage.\n        \"\"\"\n        # Save collider to json, creating the folder if it does not exist\n        if \"/\" in self.path_collider_file_for_configuration_as_output:\n            os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n        collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_configuration_as_output)\n\n    @staticmethod\n    def clean_temporary_files() -&gt; None:\n        \"\"\"\n        Remove all the temporary files created in the process of building the collider.\n\n        This function deletes the following files and directories:\n        - \"mad_collider.log\"\n        - \"mad_b4.log\"\n        - \"temp\" directory\n        - \"errors\"\n        - \"acc-models-lhc\"\n        \"\"\"\n        # Remove all the temporaty files created in the process of building collider\n        os.remove(\"mad_collider.log\")\n        os.remove(\"mad_b4.log\")\n        shutil.rmtree(\"temp\")\n        os.unlink(\"errors\")\n        os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.ost","title":"<code>ost: Any</code>  <code>property</code>","text":"<p>Determines and returns the appropriate optics-specific tools (OST) based on the version of HLLHC optics or LHC run configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics-specific tools are available for the given configuration.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The appropriate OST module based on the configuration.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initializes the MadCollider class with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the following keys: - sanity_checks (bool): Flag to enable or disable sanity checks. - links (str): Path to the links configuration. - beam_config (dict): Configuration for the beam. - optics_file (str): Path to the optics file. - enable_imperfections (bool): Flag to enable or disable imperfections. - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis. - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling     knobs. - pars_for_imperfections (dict): Parameters for imperfections. - ver_lhc_run (float | None): Version of the LHC run, if applicable. - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable. - ions (bool): Flag to indicate if ions are used. - phasing (dict): Configuration for phasing. - path_collider_file_for_configuration_as_output (str): Path to the collider. - compress (bool): Flag to enable or disable compression.</p> required Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initializes the MadCollider class with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the following keys:\n            - sanity_checks (bool): Flag to enable or disable sanity checks.\n            - links (str): Path to the links configuration.\n            - beam_config (dict): Configuration for the beam.\n            - optics_file (str): Path to the optics file.\n            - enable_imperfections (bool): Flag to enable or disable imperfections.\n            - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n            - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                knobs.\n            - pars_for_imperfections (dict): Parameters for imperfections.\n            - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n            - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n            - ions (bool): Flag to indicate if ions are used.\n            - phasing (dict): Configuration for phasing.\n            - path_collider_file_for_configuration_as_output (str): Path to the collider.\n            - compress (bool): Flag to enable or disable compression.\n    \"\"\"\n    # Configuration variables\n    self.sanity_checks: bool = configuration[\"sanity_checks\"]\n    self.links: str = configuration[\"links\"]\n    self.beam_config: dict = configuration[\"beam_config\"]\n    self.optics: str = configuration[\"optics_file\"]\n    self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n    self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n    self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n    self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n    self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n    self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n    self.ions: bool = configuration[\"ions\"]\n    self.phasing: dict = configuration[\"phasing\"]\n\n    # Optics specific tools\n    self._ost = None\n\n    # Path to disk and compression\n    self.path_collider_file_for_configuration_as_output = configuration[\n        \"path_collider_file_for_configuration_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.activate_RF_and_twiss","title":"<code>activate_RF_and_twiss(collider)</code>","text":"<p>Activates RF and Twiss parameters for the given collider.</p> <p>This method sets the RF knobs for the collider using the values specified in the <code>phasing</code> attribute. It also performs sanity checks on the collider lattices if the <code>sanity_checks</code> attribute is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Activates RF and Twiss parameters for the given collider.\n\n    This method sets the RF knobs for the collider using the values specified\n    in the `phasing` attribute. It also performs sanity checks on the collider\n    lattices if the `sanity_checks` attribute is set to True.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    # Define a RF knobs\n    collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n    collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n    collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n    if self.sanity_checks:\n        for my_line in [\"lhcb1\", \"lhcb2\"]:\n            self.check_xsuite_lattices(collider[my_line])\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.build_collider","title":"<code>build_collider(mad_b1b2, mad_b4)</code>","text":"<p>Build an xsuite collider using provided MAD-X sequences and configuration.</p> <p>Parameters: mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2. mad_b4 (Madx): MAD-X instance containing sequence for beam 4.</p> <p>Returns: xt.Multiline: Constructed xsuite collider.</p> <p>Notes: - Converts <code>ver_lhc_run</code> and <code>ver_hllhc_optics</code> to float if they are not None. - Builds the xsuite collider with the specified sequences and configuration. - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n    \"\"\"\n    Build an xsuite collider using provided MAD-X sequences and configuration.\n\n    Parameters:\n    mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n    mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n    Returns:\n    xt.Multiline: Constructed xsuite collider.\n\n    Notes:\n    - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n    - Builds the xsuite collider with the specified sequences and configuration.\n    - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n    \"\"\"\n    # Ensure proper types to avoid assert errors\n    if self.ver_lhc_run is not None:\n        self.ver_lhc_run = float(self.ver_lhc_run)\n    if self.ver_hllhc_optics is not None:\n        self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n    # Build xsuite collider\n    collider = xlhc.build_xsuite_collider(\n        sequence_b1=mad_b1b2.sequence.lhcb1,\n        sequence_b2=mad_b1b2.sequence.lhcb2,\n        sequence_b4=mad_b4.sequence.lhcb2,\n        beam_config=self.beam_config,\n        enable_imperfections=self.enable_imperfections,\n        enable_knob_synthesis=self.enable_knob_synthesis,\n        rename_coupling_knobs=self.rename_coupling_knobs,\n        pars_for_imperfections=self.pars_for_imperfections,\n        ver_lhc_run=self.ver_lhc_run,\n        ver_hllhc_optics=self.ver_hllhc_optics,\n    )\n    collider.build_trackers()\n\n    if self.sanity_checks:\n        collider[\"lhcb1\"].twiss(method=\"4d\")\n        collider[\"lhcb2\"].twiss(method=\"4d\")\n\n    return collider\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.check_xsuite_lattices","title":"<code>check_xsuite_lattices(line)</code>","text":"<p>Check the Twiss parameters and tune values for a given xsuite Line object.</p> <p>This method computes the Twiss parameters for the provided <code>line</code> using the 6-dimensional method with a specified matrix stability tolerance. It then prints the Twiss results at all interaction points (IPs) and the horizontal (Qx) and vertical (Qy) tune values.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Line</code> <p>The xsuite Line object for which to compute and display             the Twiss parameters and tune values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n    \"\"\"\n    Check the Twiss parameters and tune values for a given xsuite Line object.\n\n    This method computes the Twiss parameters for the provided `line` using the\n    6-dimensional method with a specified matrix stability tolerance. It then\n    prints the Twiss results at all interaction points (IPs) and the horizontal\n    (Qx) and vertical (Qy) tune values.\n\n    Args:\n        line (xt.Line): The xsuite Line object for which to compute and display\n                        the Twiss parameters and tune values.\n\n    Returns:\n        None\n    \"\"\"\n    tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n    print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n    print(tw.rows[\"ip.*\"])\n    # print qx and qy\n    print(f\"--- Now displaying Qx and Qy for line {line}---\")\n    print(tw.qx, tw.qy)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.clean_temporary_files","title":"<code>clean_temporary_files()</code>  <code>staticmethod</code>","text":"<p>Remove all the temporary files created in the process of building the collider.</p> <p>This function deletes the following files and directories: - \"mad_collider.log\" - \"mad_b4.log\" - \"temp\" directory - \"errors\" - \"acc-models-lhc\"</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>@staticmethod\ndef clean_temporary_files() -&gt; None:\n    \"\"\"\n    Remove all the temporary files created in the process of building the collider.\n\n    This function deletes the following files and directories:\n    - \"mad_collider.log\"\n    - \"mad_b4.log\"\n    - \"temp\" directory\n    - \"errors\"\n    - \"acc-models-lhc\"\n    \"\"\"\n    # Remove all the temporaty files created in the process of building collider\n    os.remove(\"mad_collider.log\")\n    os.remove(\"mad_b4.log\")\n    shutil.rmtree(\"temp\")\n    os.unlink(\"errors\")\n    os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.prepare_mad_collider","title":"<code>prepare_mad_collider()</code>","text":"<p>Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.</p> <p>This method performs the following steps: 1. Creates the MAD-X environment using the provided links. 2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs. 3. Builds the sequences for both beams using the provided beam configuration. 4. Applies the specified optics to the beam 1/2 sequence. 5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking     the MAD-X lattices. 6. Applies the specified optics to the beam 4 sequence. 7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking     the MAD-X lattices.</p> <p>Returns:</p> Type Description <code>tuple[Madx, Madx]</code> <p>tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n    # sourcery skip: extract-duplicate-method\n    \"\"\"\n    Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n    This method performs the following steps:\n    1. Creates the MAD-X environment using the provided links.\n    2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n    3. Builds the sequences for both beams using the provided beam configuration.\n    4. Applies the specified optics to the beam 1/2 sequence.\n    5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n        the MAD-X lattices.\n    6. Applies the specified optics to the beam 4 sequence.\n    7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n        the MAD-X lattices.\n\n    Returns:\n        tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n    \"\"\"\n    # Make mad environment\n    xm.make_mad_environment(links=self.links)\n\n    # Start mad\n    mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n    mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n    # Build sequences\n    self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n    self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n    # Apply optics (only for b1b2, b4 will be generated from b1b2)\n    self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n    if self.sanity_checks:\n        mad_b1b2.use(sequence=\"lhcb1\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n        mad_b1b2.use(sequence=\"lhcb2\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n\n    # Apply optics (only for b4, just for check)\n    self.ost.apply_optics(mad_b4, optics_file=self.optics)\n    if self.sanity_checks:\n        mad_b4.use(sequence=\"lhcb2\")\n        mad_b4.twiss()\n        # ! Investigate why this is failing for run III\n        try:\n            self.ost.check_madx_lattices(mad_b4)\n        except AssertionError:\n            logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n    return mad_b1b2, mad_b4\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.MadCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider)</code>","text":"<p>Writes the collider object to disk in JSON format and optionally compresses it into a ZIP file.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an issue creating the directory or writing the file.</p> Notes <ul> <li>The method ensures that the directory specified in     <code>self.path_collider_file_for_configuration_as_output</code> exists.</li> <li>If <code>self.compress</code> is True, the JSON file is compressed into a ZIP file to reduce     storage usage.</li> </ul> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n    file.\n\n    Args:\n        collider (xt.Multiline): The collider object to be saved.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an issue creating the directory or writing the file.\n\n    Notes:\n        - The method ensures that the directory specified in\n            `self.path_collider_file_for_configuration_as_output` exists.\n        - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n            storage usage.\n    \"\"\"\n    # Save collider to json, creating the folder if it does not exist\n    if \"/\" in self.path_collider_file_for_configuration_as_output:\n        os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n    collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n    # Compress the collider file to zip to ease the load on afs\n    if self.compress:\n        compress_and_write(self.path_collider_file_for_configuration_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution","title":"<code>ParticlesDistribution</code>","text":"<p>ParticlesDistribution class to generate and manage particle distributions.</p> <p>Attributes:</p> Name Type Description <code>r_min</code> <code>int</code> <p>Minimum radial distance.</p> <code>r_max</code> <code>int</code> <p>Maximum radial distance.</p> <code>n_r</code> <code>int</code> <p>Number of radial points.</p> <code>n_angles</code> <code>int</code> <p>Number of angular points.</p> <code>n_split</code> <code>int</code> <p>Number of splits for parallelization.</p> <code>path_distribution_folder_output</code> <code>str</code> <p>Path to the folder where distributions will be saved.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>dict): Initializes the ParticlesDistribution with the given configuration.</p> <code>get_radial_list</code> <p>float | None = None, upper_crop: float | None = None) -&gt; np.ndarray: Generates a list of radial distances, optionally cropped.</p> <code>get_angular_list</code> <p>Generates a list of angular values.</p> <code>return_distribution_as_list</code> <p>bool = True, lower_crop: float | None = None, upper_crop: float | None) -&gt; list[np.ndarray]: Returns the particle distribution as a list of numpy arrays, optionally split for parallelization.</p> <code>write_particle_distribution_to_disk</code> <p>list[np.ndarray]) -&gt; list[str]: Writes the particle distribution to disk in Parquet format and returns the list of file paths.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>class ParticlesDistribution:\n    \"\"\"\n    ParticlesDistribution class to generate and manage particle distributions.\n\n    Attributes:\n        r_min (int): Minimum radial distance.\n        r_max (int): Maximum radial distance.\n        n_r (int): Number of radial points.\n        n_angles (int): Number of angular points.\n        n_split (int): Number of splits for parallelization.\n        path_distribution_folder_output (str): Path to the folder where distributions will be saved.\n\n    Methods:\n        __init__(configuration: dict):\n            Initializes the ParticlesDistribution with the given configuration.\n\n        get_radial_list(lower_crop: float | None = None, upper_crop: float | None = None)\n            -&gt; np.ndarray:\n            Generates a list of radial distances, optionally cropped.\n\n        get_angular_list() -&gt; np.ndarray:\n            Generates a list of angular values.\n\n        return_distribution_as_list(split: bool = True, lower_crop: float | None = None,\n            upper_crop: float | None) -&gt; list[np.ndarray]:\n            Returns the particle distribution as a list of numpy arrays, optionally split for\n            parallelization.\n\n        write_particle_distribution_to_disk(ll_particles: list[np.ndarray]) -&gt; list[str]:\n            Writes the particle distribution to disk in Parquet format and returns the list of file\n            paths.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initialize the particle distribution with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                - r_min (int): Minimum radius value.\n                - r_max (int): Maximum radius value.\n                - n_r (int): Number of radius points.\n                - n_angles (int): Number of angle points.\n                - n_split (int): Number of splits for parallelization.\n                - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                    saved.\n        \"\"\"\n        # Variables used to define the distribution\n        self.r_min: int = configuration[\"r_min\"]\n        self.r_max: int = configuration[\"r_max\"]\n        self.n_r: int = configuration[\"n_r\"]\n        self.n_angles: int = configuration[\"n_angles\"]\n\n        # Variables to split the distribution for parallelization\n        self.n_split: int = configuration[\"n_split\"]\n\n        # Variable to write the distribution to disk\n        self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n\n    def get_radial_list(\n        self, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of radial distances within specified bounds.\n\n        Args:\n            lower_crop (float | None): The lower bound to crop the radial distances.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound to crop the radial distances.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            np.ndarray: An array of radial distances within the specified bounds.\n        \"\"\"\n        radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n        if upper_crop:\n            radial_list = radial_list[radial_list &lt;= 7.5]\n        if lower_crop:\n            radial_list = radial_list[radial_list &gt;= 2.5]\n        return radial_list\n\n    def get_angular_list(self) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of angular values.\n\n        This method creates a list of angular values ranging from 0 to 90 degrees,\n        excluding the first and last values. The number of angles generated is\n        determined by the instance variable `self.n_angles`.\n\n        Returns:\n            numpy.ndarray: An array of angular values.\n        \"\"\"\n        return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n\n    def return_distribution_as_list(\n        self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; list[np.ndarray]:\n        \"\"\"\n        Returns the particle distribution as a list of numpy arrays.\n\n        This method generates a particle distribution by creating a Cartesian product\n        of radial and angular lists. The resulting distribution can be optionally split\n        into multiple parts for parallel computation.\n\n        Args:\n            split (bool): If True, the distribution is split into multiple parts.\n                Defaults to True.\n            lower_crop (float | None): The lower bound for cropping the radial list.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound for cropping the radial list.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n                If `split` is True, the list contains multiple arrays for parallel computation.\n                Otherwise, the list contains a single array.\n        \"\"\"\n        # Get radial list and angular list\n        radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n        angular_list = self.get_angular_list()\n\n        # Define particle distribution as a cartesian product of the radial and angular lists\n        l_particles = np.array(\n            [\n                (particle_id, ii[1], ii[0])\n                for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n            ]\n        )\n\n        # Potentially split the distribution to parallelize the computation\n        if split:\n            return list(np.array_split(l_particles, self.n_split))\n\n        return [l_particles]\n\n    def write_particle_distribution_to_disk(\n        self, ll_particles: list[list[np.ndarray]]\n    ) -&gt; list[str]:\n        \"\"\"\n        Writes a list of particle distributions to disk in Parquet format.\n\n        Args:\n            ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n                where each distribution is a list containing particle data.\n\n        Returns:\n            list[str]: A list of file paths where the particle distributions\n            have been saved.\n\n        The method creates a directory specified by `self.path_distribution_folder_output`\n        if it does not already exist. Each particle distribution is saved as a\n        Parquet file in this directory. The files are named sequentially using\n        a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n        \"\"\"\n        # Define folder to store the distributions\n        os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n        # Write the distribution to disk\n        l_path_files = []\n        for idx_chunk, l_particles in enumerate(ll_particles):\n            path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n            pd.DataFrame(\n                l_particles,\n                columns=[\n                    \"particle_id\",\n                    \"normalized amplitude in xy-plane\",\n                    \"angle in xy-plane [deg]\",\n                ],\n            ).to_parquet(path_file)\n            l_path_files.append(path_file)\n\n        return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initialize the particle distribution with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. - r_min (int): Minimum radius value. - r_max (int): Maximum radius value. - n_r (int): Number of radius points. - n_angles (int): Number of angle points. - n_split (int): Number of splits for parallelization. - path_distribution_folder_output (str): Path to the folder where the distribution will be     saved.</p> required Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initialize the particle distribution with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            - r_min (int): Minimum radius value.\n            - r_max (int): Maximum radius value.\n            - n_r (int): Number of radius points.\n            - n_angles (int): Number of angle points.\n            - n_split (int): Number of splits for parallelization.\n            - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                saved.\n    \"\"\"\n    # Variables used to define the distribution\n    self.r_min: int = configuration[\"r_min\"]\n    self.r_max: int = configuration[\"r_max\"]\n    self.n_r: int = configuration[\"n_r\"]\n    self.n_angles: int = configuration[\"n_angles\"]\n\n    # Variables to split the distribution for parallelization\n    self.n_split: int = configuration[\"n_split\"]\n\n    # Variable to write the distribution to disk\n    self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.get_angular_list","title":"<code>get_angular_list()</code>","text":"<p>Generate a list of angular values.</p> <p>This method creates a list of angular values ranging from 0 to 90 degrees, excluding the first and last values. The number of angles generated is determined by the instance variable <code>self.n_angles</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array of angular values.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_angular_list(self) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of angular values.\n\n    This method creates a list of angular values ranging from 0 to 90 degrees,\n    excluding the first and last values. The number of angles generated is\n    determined by the instance variable `self.n_angles`.\n\n    Returns:\n        numpy.ndarray: An array of angular values.\n    \"\"\"\n    return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.get_radial_list","title":"<code>get_radial_list(lower_crop=None, upper_crop=None)</code>","text":"<p>Generate a list of radial distances within specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>lower_crop</code> <code>float | None</code> <p>The lower bound to crop the radial distances. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound to crop the radial distances. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of radial distances within the specified bounds.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_radial_list(\n    self, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of radial distances within specified bounds.\n\n    Args:\n        lower_crop (float | None): The lower bound to crop the radial distances.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound to crop the radial distances.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        np.ndarray: An array of radial distances within the specified bounds.\n    \"\"\"\n    radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n    if upper_crop:\n        radial_list = radial_list[radial_list &lt;= 7.5]\n    if lower_crop:\n        radial_list = radial_list[radial_list &gt;= 2.5]\n    return radial_list\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.return_distribution_as_list","title":"<code>return_distribution_as_list(split=True, lower_crop=None, upper_crop=None)</code>","text":"<p>Returns the particle distribution as a list of numpy arrays.</p> <p>This method generates a particle distribution by creating a Cartesian product of radial and angular lists. The resulting distribution can be optionally split into multiple parts for parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>bool</code> <p>If True, the distribution is split into multiple parts. Defaults to True.</p> <code>True</code> <code>lower_crop</code> <code>float | None</code> <p>The lower bound for cropping the radial list. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound for cropping the radial list. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: A list of numpy arrays representing the particle distribution. If <code>split</code> is True, the list contains multiple arrays for parallel computation. Otherwise, the list contains a single array.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def return_distribution_as_list(\n    self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Returns the particle distribution as a list of numpy arrays.\n\n    This method generates a particle distribution by creating a Cartesian product\n    of radial and angular lists. The resulting distribution can be optionally split\n    into multiple parts for parallel computation.\n\n    Args:\n        split (bool): If True, the distribution is split into multiple parts.\n            Defaults to True.\n        lower_crop (float | None): The lower bound for cropping the radial list.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound for cropping the radial list.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n            If `split` is True, the list contains multiple arrays for parallel computation.\n            Otherwise, the list contains a single array.\n    \"\"\"\n    # Get radial list and angular list\n    radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n    angular_list = self.get_angular_list()\n\n    # Define particle distribution as a cartesian product of the radial and angular lists\n    l_particles = np.array(\n        [\n            (particle_id, ii[1], ii[0])\n            for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n        ]\n    )\n\n    # Potentially split the distribution to parallelize the computation\n    if split:\n        return list(np.array_split(l_particles, self.n_split))\n\n    return [l_particles]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.ParticlesDistribution.write_particle_distribution_to_disk","title":"<code>write_particle_distribution_to_disk(ll_particles)</code>","text":"<p>Writes a list of particle distributions to disk in Parquet format.</p> <p>Parameters:</p> Name Type Description Default <code>ll_particles</code> <code>list[list[ndarray]]</code> <p>A list of particle distributions, where each distribution is a list containing particle data.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of file paths where the particle distributions</p> <code>list[str]</code> <p>have been saved.</p> <p>The method creates a directory specified by <code>self.path_distribution_folder_output</code> if it does not already exist. Each particle distribution is saved as a Parquet file in this directory. The files are named sequentially using a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def write_particle_distribution_to_disk(\n    self, ll_particles: list[list[np.ndarray]]\n) -&gt; list[str]:\n    \"\"\"\n    Writes a list of particle distributions to disk in Parquet format.\n\n    Args:\n        ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n            where each distribution is a list containing particle data.\n\n    Returns:\n        list[str]: A list of file paths where the particle distributions\n        have been saved.\n\n    The method creates a directory specified by `self.path_distribution_folder_output`\n    if it does not already exist. Each particle distribution is saved as a\n    Parquet file in this directory. The files are named sequentially using\n    a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n    \"\"\"\n    # Define folder to store the distributions\n    os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n    # Write the distribution to disk\n    l_path_files = []\n    for idx_chunk, l_particles in enumerate(ll_particles):\n        path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n        pd.DataFrame(\n            l_particles,\n            columns=[\n                \"particle_id\",\n                \"normalized amplitude in xy-plane\",\n                \"angle in xy-plane [deg]\",\n            ],\n        ).to_parquet(path_file)\n        l_path_files.append(path_file)\n\n    return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider","title":"<code>XsuiteCollider</code>","text":"<p>XsuiteCollider is a class designed to handle the configuration and manipulation of a collider using the Xsuite library. It provides methods to load, configure, and tune the collider, as well as to perform luminosity leveling and beam-beam interaction setup.</p> <p>Attributes:</p> Name Type Description <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file to load.</p> <code>config_beambeam</code> <code>dict</code> <p>Configuration for beam-beam interactions.</p> <code>config_knobs_and_tuning</code> <code>dict</code> <p>Configuration for knobs and tuning.</p> <code>config_lumi_leveling</code> <code>dict</code> <p>Configuration for luminosity leveling.</p> <code>config_lumi_leveling_ip1_5</code> <code>dict or None</code> <p>Configuration for luminosity leveling at IP1 and IP5.</p> <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> <code>_dict_orbit_correction</code> <code>dict or None</code> <p>Dictionary for orbit correction.</p> <code>_crab</code> <code>bool or None</code> <p>Flag indicating if crab cavities are used.</p> <code>save_output_collider</code> <code>bool</code> <p>Flag indicating if the final collider should be saved.</p> <code>path_collider_file_for_tracking_as_output</code> <code>str</code> <p>Path to save the final collider.</p> <p>Methods:</p> Name Description <code>dict_orbit_correction</code> <p>Property to get the dictionary for orbit correction.</p> <code>load_collider</code> <p>Loads the collider from a file.</p> <code>install_beam_beam_wrapper</code> <p>Installs beam-beam lenses in the collider.</p> <code>set_knobs</code> <p>Sets the knobs for the collider.</p> <code>match_tune_and_chroma</code> <p>Matches the tune and chromaticity of the collider.</p> <code>set_filling_and_bunch_tracked</code> <p>Sets the filling scheme and tracks the bunch.</p> <code>compute_collision_from_scheme</code> <p>Computes the number of collisions from the filling scheme.</p> <code>crab</code> <p>Property to get the crab cavities status.</p> <code>level_all_by_separation</code> <p>Levels all IPs by separation.</p> <code>level_ip1_5_by_bunch_intensity</code> <p>Levels IP1 and IP5 by bunch intensity.</p> <code>level_ip2_8_by_separation</code> <p>Levels IP2 and IP8 by separation.</p> <code>add_linear_coupling</code> <p>Adds linear coupling to the collider.</p> <code>assert_tune_chroma_coupling</code> <p>Asserts the tune, chromaticity, and coupling of the collider.</p> <code>configure_beam_beam</code> <p>Configures the beam-beam interactions.</p> <code>record_final_luminosity</code> <p>Records the final luminosity of the collider.</p> <code>write_collider_to_disk</code> <p>Writes the collider configuration to disk.</p> <code>update_configuration_knob</code> <p>Updates a specific knob in the collider.</p> <code>return_fingerprint</code> <p>Returns a fingerprint of the collider's configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>class XsuiteCollider:\n    \"\"\"\n    XsuiteCollider is a class designed to handle the configuration and manipulation of a collider\n    using the Xsuite library. It provides methods to load, configure, and tune the collider,\n    as well as to perform luminosity leveling and beam-beam interaction setup.\n\n    Attributes:\n        path_collider_file_for_configuration_as_input (str): Path to the collider file to load.\n        config_beambeam (dict): Configuration for beam-beam interactions.\n        config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n        config_lumi_leveling (dict): Configuration for luminosity leveling.\n        config_lumi_leveling_ip1_5 (dict or None): Configuration for luminosity leveling at IP1 and\n            IP5.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n        _dict_orbit_correction (dict or None): Dictionary for orbit correction.\n        _crab (bool or None): Flag indicating if crab cavities are used.\n        save_output_collider (bool): Flag indicating if the final collider should be saved.\n        path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n\n    Methods:\n        dict_orbit_correction: Property to get the dictionary for orbit correction.\n        load_collider: Loads the collider from a file.\n        install_beam_beam_wrapper: Installs beam-beam lenses in the collider.\n        set_knobs: Sets the knobs for the collider.\n        match_tune_and_chroma: Matches the tune and chromaticity of the collider.\n        set_filling_and_bunch_tracked: Sets the filling scheme and tracks the bunch.\n        compute_collision_from_scheme: Computes the number of collisions from the filling scheme.\n        crab: Property to get the crab cavities status.\n        level_all_by_separation: Levels all IPs by separation.\n        level_ip1_5_by_bunch_intensity: Levels IP1 and IP5 by bunch intensity.\n        level_ip2_8_by_separation: Levels IP2 and IP8 by separation.\n        add_linear_coupling: Adds linear coupling to the collider.\n        assert_tune_chroma_coupling: Asserts the tune, chromaticity, and coupling of the collider.\n        configure_beam_beam: Configures the beam-beam interactions.\n        record_final_luminosity: Records the final luminosity of the collider.\n        write_collider_to_disk: Writes the collider configuration to disk.\n        update_configuration_knob: Updates a specific knob in the collider.\n        return_fingerprint: Returns a fingerprint of the collider's configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        configuration: dict,\n        path_collider_file_for_configuration_as_input: str,\n        ver_hllhc_optics: float,\n        ver_lhc_run: float,\n        ions: bool,\n    ):\n        \"\"\"\n        Initialize the XsuiteCollider class with the given configuration and parameters.\n\n        Args:\n            configuration (dict): A dictionary containing various configuration settings.\n                - config_beambeam (dict): Configuration for beam-beam interactions.\n                - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n                - config_lumi_leveling (dict): Configuration for luminosity leveling.\n                - save_output_collider (bool): Flag to save the final collider to disk.\n                - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n                - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                    IP1 and IP5.\n            path_collider_file_for_configuration_as_input (str): Path to the collider file.\n            ver_hllhc_optics (float): Version of the HL-LHC optics.\n            ver_lhc_run (float): Version of the LHC run.\n            ions (bool): Flag indicating if ions are used.\n        \"\"\"\n        # Collider file path\n        self.path_collider_file_for_configuration_as_input = (\n            path_collider_file_for_configuration_as_input\n        )\n\n        # Configuration variables\n        self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n        self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n        self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n        # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n        self.config_lumi_leveling_ip1_5 = configuration.get(\"config_lumi_leveling_ip1_5\")\n\n        # Optics version (needed to select the appropriate optics specific functions)\n        self.ver_hllhc_optics: float = ver_hllhc_optics\n        self.ver_lhc_run: float = ver_lhc_run\n        self.ions: bool = ions\n        self._dict_orbit_correction: dict | None = None\n\n        # Crab cavities\n        self._crab: bool | None = None\n\n        # Save collider to disk\n        self.save_output_collider = configuration[\"save_output_collider\"]\n        self.path_collider_file_for_tracking_as_output = configuration[\n            \"path_collider_file_for_tracking_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def dict_orbit_correction(self) -&gt; dict:\n        \"\"\"\n        Generates and returns a dictionary containing orbit correction parameters.\n\n        This method checks if the orbit correction dictionary has already been generated.\n        If not, it determines the appropriate set of orbit correction parameters based on\n        the version of HLLHC optics or LHC run provided.\n\n        Returns:\n            dict: A dictionary containing orbit correction parameters.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics specific tools are available for the provided configuration.\n        \"\"\"\n        if self._dict_orbit_correction is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._dict_orbit_correction = gen_corr_hllhc16()\n                    case 1.3:\n                        self._dict_orbit_correction = gen_corr_hllhc13()\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._dict_orbit_correction = (\n                    gen_corr_runIII_ions() if self.ions else gen_corr_runIII()\n                )\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._dict_orbit_correction\n\n    @staticmethod\n    def _load_collider(path_collider) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file using an external path.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n\n        # Correct collider file path if it is a zip file\n        if os.path.exists(f\"{path_collider}.zip\") and not path_collider.endswith(\".zip\"):\n            path_collider += \".zip\"\n\n        # Load as a json if not zip\n        if not path_collider.endswith(\".zip\"):\n            return xt.Multiline.from_json(path_collider)\n\n        # Uncompress file locally\n        logging.info(f\"Unzipping {path_collider}\")\n        with ZipFile(path_collider, \"r\") as zip_ref:\n            zip_ref.extractall()\n        final_path = os.path.basename(path_collider).replace(\".zip\", \"\")\n        return xt.Multiline.from_json(final_path)\n\n    def load_collider(self) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n        return self._load_collider(self.path_collider_file_for_configuration_as_input)\n\n    def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        This method installs beam-beam interactions in the collider with the specified\n        parameters. The beam-beam lenses are initially inactive and not configured.\n\n        Args:\n            collider (xt.Multiline): The collider object where the beam-beam interactions\n                will be installed.\n\n        Returns:\n            None\n        \"\"\"\n        # Install beam-beam lenses (inactive and not configured)\n        collider.install_beambeam_interactions(\n            clockwise_line=\"lhcb1\",\n            anticlockwise_line=\"lhcb2\",\n            ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n            delay_at_ips_slots=[0, 891, 0, 2670],\n            num_long_range_encounters_per_side=self.config_beambeam[\n                \"num_long_range_encounters_per_side\"\n            ],\n            num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n            harmonic_number=35640,\n            bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n            sigmaz=self.config_beambeam[\"sigma_z\"],\n        )\n\n    def set_knobs(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Set all knobs for the collider, including crossing angles, dispersion correction,\n        RF, crab cavities, experimental magnets, etc.\n\n        Args:\n            collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n        Returns:\n            None\n        \"\"\"\n        # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n        # experimental magnets, etc.)\n        for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n            collider.vars[kk] = vv\n\n        # Crab fix (if needed)\n        if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n            apply_crab_fix(collider, self.config_knobs_and_tuning)\n\n    def match_tune_and_chroma(\n        self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n    ) -&gt; None:\n        \"\"\"\n        This method adjusts the tune and chromaticity of the specified collider lines\n        (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n        optionally matches the linear coupling to zero.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be tuned.\n            match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n                matched to zero. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n            targets = {\n                \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n                \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n                \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n                \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n            }\n\n            xm.machine_tuning(\n                line=collider[line_name],\n                enable_closed_orbit_correction=True,\n                enable_linear_coupling_correction=match_linear_coupling_to_zero,\n                enable_tune_correction=True,\n                enable_chromaticity_correction=True,\n                knob_names=knob_names,\n                targets=targets,\n                line_co_ref=collider[f\"{line_name}_co_ref\"],\n                co_corr_config=self.dict_orbit_correction[line_name],\n            )\n\n    def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n        \"\"\"\n        Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n        This method performs the following steps:\n        1. Retrieves the filling scheme path from the configuration.\n        2. Checks if the filling scheme path needs to be obtained from the template schemes.\n        3. Loads and verifies the filling scheme, potentially converting it if necessary.\n        4. Updates the configuration with the correct filling scheme path.\n        5. Determines the number of long-range encounters to consider.\n        6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n        number of long-range interactions.\n           - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n           - Otherwise, automatically selects the worst bunch.\n        7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n        Args:\n            ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n                for beam 1. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Get the filling scheme path\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Check if the filling scheme path must be obtained from the template schemes\n        scheme_folder = pathlib.Path(__file__).parent.parent.resolve().joinpath(\"filling_schemes\")\n        if filling_scheme_path in os.listdir(scheme_folder):\n            filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Load and check filling scheme, potentially convert it\n        filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n        # Correct filling scheme in config, as it might have been converted\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Get number of LR to consider\n        n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n        # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n            # Case the bunch number has not been provided\n            worst_bunch_b1 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n            )\n            if ask_worst_bunch:\n                while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                    bool_inp = input(\n                        \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                        \" bunch with the largest number of long-range interactions? It is the bunch\"\n                        \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                    )\n                    if bool_inp == \"y\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                            worst_bunch_b1\n                        )\n                    elif bool_inp == \"n\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                            input(\"Please enter the bunch number for beam 1: \")\n                        )\n            else:\n                self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n            worst_bunch_b2 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n            )\n            # For beam 2, just select the worst bunch by default\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n\n    def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n        \"\"\"\n        This method reads a filling scheme from a JSON file specified in the configuration, converts\n        the filling scheme into boolean arrays for two beams, and calculates the number of\n        collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n        Returns:\n            tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n                IP8 respectively.\n\n        Raises:\n            ValueError: If the filling scheme file is not in JSON format.\n            AssertionError: If the length of the beam arrays is not 3564.\n        \"\"\"\n        # Get the filling scheme path (in json or csv format)\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Load the filling scheme\n        if not filling_scheme_path.endswith(\".json\"):\n            raise ValueError(\n                f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n                \" file, it should have been automatically convert when running the script\"\n                \" 001_make_folders.py. Something went wrong.\"\n            )\n\n        with open(filling_scheme_path, \"r\") as fid:\n            filling_scheme = json.load(fid)\n\n        # Extract booleans beam arrays\n        array_b1 = np.array(filling_scheme[\"beam1\"])\n        array_b2 = np.array(filling_scheme[\"beam2\"])\n\n        # Assert that the arrays have the required length, and do the convolution\n        assert len(array_b1) == len(array_b2) == 3564\n        n_collisions_ip1_and_5 = array_b1 @ array_b2\n        n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n        n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n        return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n\n    @property\n    def crab(self) -&gt; bool:\n        \"\"\"\n        This method checks the configuration settings for the presence and value of the\n        \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the\n        `_crab` attribute to True, indicating that crab cavities are active. Otherwise,\n        it sets `_crab` to False.\n\n        Returns:\n            bool: True if crab cavities are active, False otherwise.\n        \"\"\"\n        if self._crab is None:\n            # Get crab cavities\n            self._crab = False\n            if \"on_crab1\" in self.config_knobs_and_tuning[\"knob_settings\"]:\n                crab_val = float(self.config_knobs_and_tuning[\"knob_settings\"][\"on_crab1\"])\n                if abs(crab_val) &gt; 0:\n                    self._crab = True\n        return self._crab\n\n    def level_all_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n        configuration file and performs luminosity leveling using the provided collider object.\n        It also updates the separation knobs for the collider based on the new configuration.\n\n        Args:\n            n_collisions_ip2 (int): Number of collisions at interaction point 2.\n            n_collisions_ip8 (int): Number of collisions at interaction point 8.\n            collider (xt.Multiline): The collider object to be used for luminosity leveling.\n            n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        l_n_collisions = [\n            n_collisions_ip1_and_5,\n            n_collisions_ip2,\n            n_collisions_ip1_and_5,\n            n_collisions_ip8,\n        ]\n        for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n            if ip in self.config_lumi_leveling:\n                self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n            else:\n                logging.warning(f\"IP {ip} is not in the configuration\")\n\n        # ! Crabs are not handled in the following function\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip1\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip5\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def level_ip1_5_by_bunch_intensity(\n        self,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method modifies the bunch intensity to achieve the desired luminosity\n        levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and lattice\n                configuration.\n            n_collisions_ip1_and_5 (int):\n                The number of collisions in IP 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Initial intensity\n        bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n        # First level luminosity in IP 1/5 changing the intensity\n        if (\n            self.config_lumi_leveling_ip1_5 is not None\n            and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n        ):\n            logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n            # Update the number of bunches in the configuration file\n            self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n            # Do the levelling\n            bunch_intensity = luminosity_leveling_ip1_5(\n                collider,\n                self.config_lumi_leveling_ip1_5,\n                self.config_beambeam,\n                crab=self.crab,\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n        # Update the configuration\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n\n    def level_ip2_8_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n        file, performs luminosity leveling for the specified collider, and updates the separation\n        knobs for both interaction points.\n\n        Args:\n            n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n            n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n            collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n                performed.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        if \"ip2\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n        if \"ip8\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n        # Do levelling in IP2 and IP8\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n        This method adjusts the collider variables to introduce linear coupling. The specific\n        adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n        Args:\n            collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the version of the optics or run is unknown.\n\n        Notes:\n            - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n            - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n            `c_minus_re_b2` variables are adjusted.\n        \"\"\"\n        # Add linear coupling as the target in the tuning of the base collider was 0\n        # (not possible to set it the target to 0.001 for now)\n        if self.ver_lhc_run == 3.0:\n            collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n            collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        else:\n            raise ValueError(\n                f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n            )\n\n    def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Asserts that the tune, chromaticity, and linear coupling of the collider\n        match the expected values specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be checked.\n\n        Returns:\n            None\n\n        Raises:\n            AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n                the expected values within the specified tolerances.\n\n        Notes:\n            The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n            - Horizontal tune (qx)\n            - Vertical tune (qy)\n            - Horizontal chromaticity (dqx)\n            - Vertical chromaticity (dqy)\n            - Linear coupling (c_minus)\n\n        The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            tw = collider[line_name].twiss()\n            assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n                f\"tune_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n            )\n            assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n                f\"tune_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n            )\n            assert np.isclose(\n                tw.dqx,\n                self.config_knobs_and_tuning[\"dqx\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n            )\n            assert np.isclose(\n                tw.dqy,\n                self.config_knobs_and_tuning[\"dqy\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n            )\n\n            assert np.isclose(\n                tw.c_minus,\n                self.config_knobs_and_tuning[\"delta_cmr\"],\n                atol=5e-3,\n            ), (\n                f\"linear coupling is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n            )\n\n    def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Configures the beam-beam interactions for the collider.\n\n        This method sets up the beam-beam interactions by configuring the number of particles per\n        bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n        the provided configuration. Additionally, it configures the filling scheme mask and bunch\n        numbers if a filling pattern is specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        collider.configure_beambeam_interactions(\n            num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n            nemitt_x=self.config_beambeam[\"nemitt_x\"],\n            nemitt_y=self.config_beambeam[\"nemitt_y\"],\n        )\n\n        # Configure filling scheme mask and bunch numbers\n        if \"mask_with_filling_pattern\" in self.config_beambeam and (\n            \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n            and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n        ):\n            fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n            with open(fname, \"r\") as fid:\n                filling = json.load(fid)\n            filling_pattern_cw = filling[\"beam1\"]\n            filling_pattern_acw = filling[\"beam2\"]\n\n            # Initialize bunch numbers with empty values\n            i_bunch_cw = None\n            i_bunch_acw = None\n\n            # Only track bunch number if a filling pattern has been provided\n            if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n            if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n            # Note that a bunch number must be provided if a filling pattern is provided\n            # Apply filling pattern\n            collider.apply_filling_pattern(\n                filling_pattern_cw=filling_pattern_cw,\n                filling_pattern_acw=filling_pattern_acw,\n                i_bunch_cw=i_bunch_cw,\n                i_bunch_acw=i_bunch_acw,\n            )\n\n    def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n        \"\"\"\n        Records the final luminosity and pile-up for specified interaction points (IPs)\n        in the collider, both with and without beam-beam effects.\n\n        Args:\n            collider : (xt.Multiline): The collider object configured.\n            l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n                IP.\n\n        Returns:\n            None\n        \"\"\"\n        # Define IPs in which the luminosity will be computed\n        l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n        # Ensure that the final number of particles per bunch is defined, even\n        # if the leveling has been done by separation\n        if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n            self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n                \"num_particles_per_bunch\"\n            ]\n\n        def _twiss_and_compute_lumi(collider, l_n_collisions):\n            # Loop over each IP and record the luminosity\n            twiss_b1 = collider[\"lhcb1\"].twiss()\n            twiss_b2 = collider[\"lhcb2\"].twiss()\n            l_lumi = []\n            l_PU = []\n            for n_col, ip in zip(l_n_collisions, l_ip):\n                L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                    n_colliding_bunches=n_col,\n                    num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                    ip_name=ip,\n                    nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                    nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                    sigma_z=self.config_beambeam[\"sigma_z\"],\n                    twiss_b1=twiss_b1,\n                    twiss_b2=twiss_b2,\n                    crab=self.crab,\n                )\n                PU = compute_PU(\n                    L,\n                    n_col,\n                    twiss_b1[\"T_rev0\"],\n                    cross_section=self.config_beambeam[\"cross_section\"],\n                )\n\n                l_lumi.append(L)\n                l_PU.append(PU)\n\n            return l_lumi, l_PU\n\n        # Get the final luminosity in all IPs, without beam-beam\n        collider.vars[\"beambeam_scale\"] = 0\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n        # Get the final luminosity in all IPs, with beam-beam\n        collider.vars[\"beambeam_scale\"] = 1\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n\n    def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n        Args:\n            collider (Collider): The collider object to be saved.\n            full_configuration (dict): The full configuration dictionary to be deep-copied into the\n                collider's metadata.\n\n        Returns:\n            None\n        \"\"\"\n        if self.save_output_collider:\n            logging.info(\"Saving collider as json\")\n            if (\n                hasattr(collider, \"metadata\")\n                and collider.metadata is not None\n                and isinstance(collider.metadata, dict)\n            ):\n                collider.metadata.update(copy.deepcopy(full_configuration))\n            else:\n                collider.metadata = copy.deepcopy(full_configuration)\n            collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n            # Compress the collider file to zip to ease the load on afs\n            if self.compress:\n                compress_and_write(self.path_collider_file_for_tracking_as_output)\n\n    @staticmethod\n    def update_configuration_knob(\n        collider: xt.Multiline, dictionnary: dict, knob_name: str\n    ) -&gt; None:\n        \"\"\"\n        Updates the given dictionary with the final value of a specified knob from the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object containing various variables.\n            dictionnary (dict): The dictionary to be updated with the knob's final value.\n            knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n        Returns:\n            None\n        \"\"\"\n        if knob_name in collider.vars.keys():\n            dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n\n    @staticmethod\n    def return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n        \"\"\"\n        Generate a detailed fingerprint of the specified collider line. Useful to compare two\n        colliders.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the line data.\n            line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n        Returns:\n            str:\n                A formatted string containing detailed information about the collider line, including:\n                - Installed element types\n                - Tunes and chromaticity\n                - Synchrotron tune and slip factor\n                - Twiss parameters and phases at interaction points (IPs)\n                - Dispersion and crab dispersion at IPs\n                - Amplitude detuning coefficients\n                - Non-linear chromaticity\n                - Tunes and momentum compaction vs delta\n        \"\"\"\n        line = collider[line_name]\n\n        tw = line.twiss()\n        tt = line.get_table()\n\n        det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n        det_table = xt.Table(\n            {\n                \"name\": np.array(list(det.keys())),\n                \"value\": np.array(list(det.values())),\n            }\n        )\n\n        nl_chrom = line.get_non_linear_chromaticity(\n            delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n        )\n\n        out = \"\"\n\n        out += f\"Line: {line_name}\\n\"\n        out += \"\\n\"\n\n        out += \"Installed element types:\\n\"\n        out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n        out += \"\\n\"\n\n        out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n        out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n        out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n        out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += \"Twiss parameters and phases at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s betx bety alfx alfy mux muy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx dy dpx dpy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Crab dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Amplitude detuning coefficients:\\n\"\n        out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        out += \"Non-linear chromaticity:\\n\"\n        out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n        out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n        out += \"\\n\\n\"\n\n        out += \"Tunes and momentum compaction vs delta:\\n\"\n        out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        return out\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.crab","title":"<code>crab: bool</code>  <code>property</code>","text":"<p>This method checks the configuration settings for the presence and value of the \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the <code>_crab</code> attribute to True, indicating that crab cavities are active. Otherwise, it sets <code>_crab</code> to False.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if crab cavities are active, False otherwise.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.dict_orbit_correction","title":"<code>dict_orbit_correction: dict</code>  <code>property</code>","text":"<p>Generates and returns a dictionary containing orbit correction parameters.</p> <p>This method checks if the orbit correction dictionary has already been generated. If not, it determines the appropriate set of orbit correction parameters based on the version of HLLHC optics or LHC run provided.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing orbit correction parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics specific tools are available for the provided configuration.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.__init__","title":"<code>__init__(configuration, path_collider_file_for_configuration_as_input, ver_hllhc_optics, ver_lhc_run, ions)</code>","text":"<p>Initialize the XsuiteCollider class with the given configuration and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing various configuration settings. - config_beambeam (dict): Configuration for beam-beam interactions. - config_knobs_and_tuning (dict): Configuration for knobs and tuning. - config_lumi_leveling (dict): Configuration for luminosity leveling. - save_output_collider (bool): Flag to save the final collider to disk. - path_collider_file_for_tracking_as_output (str): Path to save the final collider. - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at     IP1 and IP5.</p> required <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file.</p> required <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> required <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> required <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> required Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def __init__(\n    self,\n    configuration: dict,\n    path_collider_file_for_configuration_as_input: str,\n    ver_hllhc_optics: float,\n    ver_lhc_run: float,\n    ions: bool,\n):\n    \"\"\"\n    Initialize the XsuiteCollider class with the given configuration and parameters.\n\n    Args:\n        configuration (dict): A dictionary containing various configuration settings.\n            - config_beambeam (dict): Configuration for beam-beam interactions.\n            - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n            - config_lumi_leveling (dict): Configuration for luminosity leveling.\n            - save_output_collider (bool): Flag to save the final collider to disk.\n            - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n            - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                IP1 and IP5.\n        path_collider_file_for_configuration_as_input (str): Path to the collider file.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n    \"\"\"\n    # Collider file path\n    self.path_collider_file_for_configuration_as_input = (\n        path_collider_file_for_configuration_as_input\n    )\n\n    # Configuration variables\n    self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n    self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n    self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n    # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n    self.config_lumi_leveling_ip1_5 = configuration.get(\"config_lumi_leveling_ip1_5\")\n\n    # Optics version (needed to select the appropriate optics specific functions)\n    self.ver_hllhc_optics: float = ver_hllhc_optics\n    self.ver_lhc_run: float = ver_lhc_run\n    self.ions: bool = ions\n    self._dict_orbit_correction: dict | None = None\n\n    # Crab cavities\n    self._crab: bool | None = None\n\n    # Save collider to disk\n    self.save_output_collider = configuration[\"save_output_collider\"]\n    self.path_collider_file_for_tracking_as_output = configuration[\n        \"path_collider_file_for_tracking_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.add_linear_coupling","title":"<code>add_linear_coupling(collider)</code>","text":"<p>Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.</p> <p>This method adjusts the collider variables to introduce linear coupling. The specific adjustments depend on the version of the LHC run or HL-LHC optics being used.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which linear coupling will be added.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version of the optics or run is unknown.</p> Notes <ul> <li>For LHC Run 3.0, the <code>cmrs.b1_sq</code> and <code>cmrs.b2_sq</code> variables are adjusted.</li> <li>For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the <code>c_minus_re_b1</code> and <code>c_minus_re_b2</code> variables are adjusted.</li> </ul> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n    This method adjusts the collider variables to introduce linear coupling. The specific\n    adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n    Args:\n        collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the version of the optics or run is unknown.\n\n    Notes:\n        - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n        - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n        `c_minus_re_b2` variables are adjusted.\n    \"\"\"\n    # Add linear coupling as the target in the tuning of the base collider was 0\n    # (not possible to set it the target to 0.001 for now)\n    if self.ver_lhc_run == 3.0:\n        collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n        collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    else:\n        raise ValueError(\n            f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.assert_tune_chroma_coupling","title":"<code>assert_tune_chroma_coupling(collider)</code>","text":"<p>Asserts that the tune, chromaticity, and linear coupling of the collider match the expected values specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be checked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the tune, chromaticity, or linear coupling values do not match the expected values within the specified tolerances.</p> Notes <p>The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"): - Horizontal tune (qx) - Vertical tune (qy) - Horizontal chromaticity (dqx) - Vertical chromaticity (dqy) - Linear coupling (c_minus)</p> <p>The expected values are retrieved from the <code>self.config_knobs_and_tuning</code> dictionary.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Asserts that the tune, chromaticity, and linear coupling of the collider\n    match the expected values specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be checked.\n\n    Returns:\n        None\n\n    Raises:\n        AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n            the expected values within the specified tolerances.\n\n    Notes:\n        The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n        - Horizontal tune (qx)\n        - Vertical tune (qy)\n        - Horizontal chromaticity (dqx)\n        - Vertical chromaticity (dqy)\n        - Linear coupling (c_minus)\n\n    The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        tw = collider[line_name].twiss()\n        assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n            f\"tune_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n        )\n        assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n            f\"tune_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n        )\n        assert np.isclose(\n            tw.dqx,\n            self.config_knobs_and_tuning[\"dqx\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n        )\n        assert np.isclose(\n            tw.dqy,\n            self.config_knobs_and_tuning[\"dqy\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n        )\n\n        assert np.isclose(\n            tw.c_minus,\n            self.config_knobs_and_tuning[\"delta_cmr\"],\n            atol=5e-3,\n        ), (\n            f\"linear coupling is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.compute_collision_from_scheme","title":"<code>compute_collision_from_scheme()</code>","text":"<p>This method reads a filling scheme from a JSON file specified in the configuration, converts the filling scheme into boolean arrays for two beams, and calculates the number of collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and IP8 respectively.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filling scheme file is not in JSON format.</p> <code>AssertionError</code> <p>If the length of the beam arrays is not 3564.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n    \"\"\"\n    This method reads a filling scheme from a JSON file specified in the configuration, converts\n    the filling scheme into boolean arrays for two beams, and calculates the number of\n    collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n    Returns:\n        tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n            IP8 respectively.\n\n    Raises:\n        ValueError: If the filling scheme file is not in JSON format.\n        AssertionError: If the length of the beam arrays is not 3564.\n    \"\"\"\n    # Get the filling scheme path (in json or csv format)\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Load the filling scheme\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\n            f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n            \" file, it should have been automatically convert when running the script\"\n            \" 001_make_folders.py. Something went wrong.\"\n        )\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Assert that the arrays have the required length, and do the convolution\n    assert len(array_b1) == len(array_b2) == 3564\n    n_collisions_ip1_and_5 = array_b1 @ array_b2\n    n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n    n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n    return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.configure_beam_beam","title":"<code>configure_beam_beam(collider)</code>","text":"<p>Configures the beam-beam interactions for the collider.</p> <p>This method sets up the beam-beam interactions by configuring the number of particles per bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on the provided configuration. Additionally, it configures the filling scheme mask and bunch numbers if a filling pattern is specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Configures the beam-beam interactions for the collider.\n\n    This method sets up the beam-beam interactions by configuring the number of particles per\n    bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n    the provided configuration. Additionally, it configures the filling scheme mask and bunch\n    numbers if a filling pattern is specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    collider.configure_beambeam_interactions(\n        num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n        nemitt_x=self.config_beambeam[\"nemitt_x\"],\n        nemitt_y=self.config_beambeam[\"nemitt_y\"],\n    )\n\n    # Configure filling scheme mask and bunch numbers\n    if \"mask_with_filling_pattern\" in self.config_beambeam and (\n        \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n        and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n    ):\n        fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n        with open(fname, \"r\") as fid:\n            filling = json.load(fid)\n        filling_pattern_cw = filling[\"beam1\"]\n        filling_pattern_acw = filling[\"beam2\"]\n\n        # Initialize bunch numbers with empty values\n        i_bunch_cw = None\n        i_bunch_acw = None\n\n        # Only track bunch number if a filling pattern has been provided\n        if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n        if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n        # Note that a bunch number must be provided if a filling pattern is provided\n        # Apply filling pattern\n        collider.apply_filling_pattern(\n            filling_pattern_cw=filling_pattern_cw,\n            filling_pattern_acw=filling_pattern_acw,\n            i_bunch_cw=i_bunch_cw,\n            i_bunch_acw=i_bunch_acw,\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.install_beam_beam_wrapper","title":"<code>install_beam_beam_wrapper(collider)</code>","text":"<p>This method installs beam-beam interactions in the collider with the specified parameters. The beam-beam lenses are initially inactive and not configured.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object where the beam-beam interactions will be installed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    This method installs beam-beam interactions in the collider with the specified\n    parameters. The beam-beam lenses are initially inactive and not configured.\n\n    Args:\n        collider (xt.Multiline): The collider object where the beam-beam interactions\n            will be installed.\n\n    Returns:\n        None\n    \"\"\"\n    # Install beam-beam lenses (inactive and not configured)\n    collider.install_beambeam_interactions(\n        clockwise_line=\"lhcb1\",\n        anticlockwise_line=\"lhcb2\",\n        ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n        delay_at_ips_slots=[0, 891, 0, 2670],\n        num_long_range_encounters_per_side=self.config_beambeam[\n            \"num_long_range_encounters_per_side\"\n        ],\n        num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n        harmonic_number=35640,\n        bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n        sigmaz=self.config_beambeam[\"sigma_z\"],\n    )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_all_by_separation","title":"<code>level_all_by_separation(n_collisions_ip2, n_collisions_ip8, collider, n_collisions_ip1_and_5)</code>","text":"<p>This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the configuration file and performs luminosity leveling using the provided collider object. It also updates the separation knobs for the collider based on the new configuration.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>Number of collisions at interaction point 2.</p> required <code>n_collisions_ip8</code> <code>int</code> <p>Number of collisions at interaction point 8.</p> required <code>collider</code> <code>Multiline</code> <p>The collider object to be used for luminosity leveling.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>Number of collisions at interaction points 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_all_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n    configuration file and performs luminosity leveling using the provided collider object.\n    It also updates the separation knobs for the collider based on the new configuration.\n\n    Args:\n        n_collisions_ip2 (int): Number of collisions at interaction point 2.\n        n_collisions_ip8 (int): Number of collisions at interaction point 8.\n        collider (xt.Multiline): The collider object to be used for luminosity leveling.\n        n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n        if ip in self.config_lumi_leveling:\n            self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n        else:\n            logging.warning(f\"IP {ip} is not in the configuration\")\n\n    # ! Crabs are not handled in the following function\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip1\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip5\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_ip1_5_by_bunch_intensity","title":"<code>level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)</code>","text":"<p>This method modifies the bunch intensity to achieve the desired luminosity levels in IP 1 and 5. It updates the configuration with the new intensity values.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and lattice configuration.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>The number of collisions in IP 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip1_5_by_bunch_intensity(\n    self,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method modifies the bunch intensity to achieve the desired luminosity\n    levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and lattice\n            configuration.\n        n_collisions_ip1_and_5 (int):\n            The number of collisions in IP 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Initial intensity\n    bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n    # First level luminosity in IP 1/5 changing the intensity\n    if (\n        self.config_lumi_leveling_ip1_5 is not None\n        and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n    ):\n        logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n        # Update the number of bunches in the configuration file\n        self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n        # Do the levelling\n        bunch_intensity = luminosity_leveling_ip1_5(\n            collider,\n            self.config_lumi_leveling_ip1_5,\n            self.config_beambeam,\n            crab=self.crab,\n            cross_section=self.config_beambeam[\"cross_section\"],\n        )\n\n    # Update the configuration\n    self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.level_ip2_8_by_separation","title":"<code>level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP2 and IP8 in the configuration file, performs luminosity leveling for the specified collider, and updates the separation knobs for both interaction points.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>The number of collisions at interaction point 2 (IP2).</p> required <code>n_collisions_ip8</code> <code>int</code> <p>The number of collisions at interaction point 8 (IP8).</p> required <code>collider</code> <code>Multiline</code> <p>The collider object for which the luminosity leveling is to be performed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip2_8_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n    file, performs luminosity leveling for the specified collider, and updates the separation\n    knobs for both interaction points.\n\n    Args:\n        n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n        n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n        collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n            performed.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    if \"ip2\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n    if \"ip8\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n    # Do levelling in IP2 and IP8\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.load_collider","title":"<code>load_collider()</code>","text":"<p>Load a collider configuration from a file.</p> <p>If the file path ends with \".zip\", the file is uncompressed locally and the collider configuration is loaded from the uncompressed file. Otherwise, the collider configuration is loaded directly from the file.</p> <p>Returns:</p> Type Description <code>Multiline</code> <p>xt.Multiline: The loaded collider configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def load_collider(self) -&gt; xt.Multiline:\n    \"\"\"\n    Load a collider configuration from a file.\n\n    If the file path ends with \".zip\", the file is uncompressed locally\n    and the collider configuration is loaded from the uncompressed file.\n    Otherwise, the collider configuration is loaded directly from the file.\n\n    Returns:\n        xt.Multiline: The loaded collider configuration.\n    \"\"\"\n    return self._load_collider(self.path_collider_file_for_configuration_as_input)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.match_tune_and_chroma","title":"<code>match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)</code>","text":"<p>This method adjusts the tune and chromaticity of the specified collider lines (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also optionally matches the linear coupling to zero.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be tuned.</p> required <code>match_linear_coupling_to_zero</code> <code>bool</code> <p>If True, linear coupling will be matched to zero. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def match_tune_and_chroma(\n    self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n) -&gt; None:\n    \"\"\"\n    This method adjusts the tune and chromaticity of the specified collider lines\n    (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n    optionally matches the linear coupling to zero.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be tuned.\n        match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n            matched to zero. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n        targets = {\n            \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n            \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n            \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n            \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n        }\n\n        xm.machine_tuning(\n            line=collider[line_name],\n            enable_closed_orbit_correction=True,\n            enable_linear_coupling_correction=match_linear_coupling_to_zero,\n            enable_tune_correction=True,\n            enable_chromaticity_correction=True,\n            knob_names=knob_names,\n            targets=targets,\n            line_co_ref=collider[f\"{line_name}_co_ref\"],\n            co_corr_config=self.dict_orbit_correction[line_name],\n        )\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.record_final_luminosity","title":"<code>record_final_luminosity(collider, l_n_collisions)</code>","text":"<p>Records the final luminosity and pile-up for specified interaction points (IPs) in the collider, both with and without beam-beam effects.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <p>(xt.Multiline): The collider object configured.</p> required <code>l_n_collisions</code> <code>list[int]</code> <p>A list containing the number of colliding bunches for each IP.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n    \"\"\"\n    Records the final luminosity and pile-up for specified interaction points (IPs)\n    in the collider, both with and without beam-beam effects.\n\n    Args:\n        collider : (xt.Multiline): The collider object configured.\n        l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n            IP.\n\n    Returns:\n        None\n    \"\"\"\n    # Define IPs in which the luminosity will be computed\n    l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n    # Ensure that the final number of particles per bunch is defined, even\n    # if the leveling has been done by separation\n    if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n            \"num_particles_per_bunch\"\n        ]\n\n    def _twiss_and_compute_lumi(collider, l_n_collisions):\n        # Loop over each IP and record the luminosity\n        twiss_b1 = collider[\"lhcb1\"].twiss()\n        twiss_b2 = collider[\"lhcb2\"].twiss()\n        l_lumi = []\n        l_PU = []\n        for n_col, ip in zip(l_n_collisions, l_ip):\n            L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                n_colliding_bunches=n_col,\n                num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                ip_name=ip,\n                nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                sigma_z=self.config_beambeam[\"sigma_z\"],\n                twiss_b1=twiss_b1,\n                twiss_b2=twiss_b2,\n                crab=self.crab,\n            )\n            PU = compute_PU(\n                L,\n                n_col,\n                twiss_b1[\"T_rev0\"],\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n            l_lumi.append(L)\n            l_PU.append(PU)\n\n        return l_lumi, l_PU\n\n    # Get the final luminosity in all IPs, without beam-beam\n    collider.vars[\"beambeam_scale\"] = 0\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n    # Get the final luminosity in all IPs, with beam-beam\n    collider.vars[\"beambeam_scale\"] = 1\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.return_fingerprint","title":"<code>return_fingerprint(collider, line_name='lhcb1')</code>  <code>staticmethod</code>","text":"<p>Generate a detailed fingerprint of the specified collider line. Useful to compare two colliders.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the line data.</p> required <code>line_name</code> <code>str</code> <p>The name of the line to analyze within the collider. Default to \"lhcb1\".</p> <code>'lhcb1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing detailed information about the collider line, including: - Installed element types - Tunes and chromaticity - Synchrotron tune and slip factor - Twiss parameters and phases at interaction points (IPs) - Dispersion and crab dispersion at IPs - Amplitude detuning coefficients - Non-linear chromaticity - Tunes and momentum compaction vs delta</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n    \"\"\"\n    Generate a detailed fingerprint of the specified collider line. Useful to compare two\n    colliders.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the line data.\n        line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n    Returns:\n        str:\n            A formatted string containing detailed information about the collider line, including:\n            - Installed element types\n            - Tunes and chromaticity\n            - Synchrotron tune and slip factor\n            - Twiss parameters and phases at interaction points (IPs)\n            - Dispersion and crab dispersion at IPs\n            - Amplitude detuning coefficients\n            - Non-linear chromaticity\n            - Tunes and momentum compaction vs delta\n    \"\"\"\n    line = collider[line_name]\n\n    tw = line.twiss()\n    tt = line.get_table()\n\n    det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n    det_table = xt.Table(\n        {\n            \"name\": np.array(list(det.keys())),\n            \"value\": np.array(list(det.values())),\n        }\n    )\n\n    nl_chrom = line.get_non_linear_chromaticity(\n        delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n    )\n\n    out = \"\"\n\n    out += f\"Line: {line_name}\\n\"\n    out += \"\\n\"\n\n    out += \"Installed element types:\\n\"\n    out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n    out += \"\\n\"\n\n    out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n    out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n    out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n    out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += \"Twiss parameters and phases at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s betx bety alfx alfy mux muy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx dy dpx dpy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Crab dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Amplitude detuning coefficients:\\n\"\n    out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    out += \"Non-linear chromaticity:\\n\"\n    out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n    out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n    out += \"\\n\\n\"\n\n    out += \"Tunes and momentum compaction vs delta:\\n\"\n    out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    return out\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.set_filling_and_bunch_tracked","title":"<code>set_filling_and_bunch_tracked(ask_worst_bunch=False)</code>","text":"<p>Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.</p> <p>This method performs the following steps: 1. Retrieves the filling scheme path from the configuration. 2. Checks if the filling scheme path needs to be obtained from the template schemes. 3. Loads and verifies the filling scheme, potentially converting it if necessary. 4. Updates the configuration with the correct filling scheme path. 5. Determines the number of long-range encounters to consider. 6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest number of long-range interactions.    - If <code>ask_worst_bunch</code> is True, prompts the user to confirm or provide a bunch number.    - Otherwise, automatically selects the worst bunch. 7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.</p> <p>Parameters:</p> Name Type Description Default <code>ask_worst_bunch</code> <code>bool</code> <p>If True, prompts the user to confirm or provide the bunch number for beam 1. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n    \"\"\"\n    Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n    This method performs the following steps:\n    1. Retrieves the filling scheme path from the configuration.\n    2. Checks if the filling scheme path needs to be obtained from the template schemes.\n    3. Loads and verifies the filling scheme, potentially converting it if necessary.\n    4. Updates the configuration with the correct filling scheme path.\n    5. Determines the number of long-range encounters to consider.\n    6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n    number of long-range interactions.\n       - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n       - Otherwise, automatically selects the worst bunch.\n    7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n    Args:\n        ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n            for beam 1. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the filling scheme path\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Check if the filling scheme path must be obtained from the template schemes\n    scheme_folder = pathlib.Path(__file__).parent.parent.resolve().joinpath(\"filling_schemes\")\n    if filling_scheme_path in os.listdir(scheme_folder):\n        filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Load and check filling scheme, potentially convert it\n    filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n    # Correct filling scheme in config, as it might have been converted\n    self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Get number of LR to consider\n    n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n    # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n        # Case the bunch number has not been provided\n        worst_bunch_b1 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n        )\n        if ask_worst_bunch:\n            while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                bool_inp = input(\n                    \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                    \" bunch with the largest number of long-range interactions? It is the bunch\"\n                    \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                )\n                if bool_inp == \"y\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                        worst_bunch_b1\n                    )\n                elif bool_inp == \"n\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                        input(\"Please enter the bunch number for beam 1: \")\n                    )\n        else:\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n        worst_bunch_b2 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n        )\n        # For beam 2, just select the worst bunch by default\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.set_knobs","title":"<code>set_knobs(collider)</code>","text":"<p>Set all knobs for the collider, including crossing angles, dispersion correction, RF, crab cavities, experimental magnets, etc.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which the knob settings will be applied.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_knobs(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Set all knobs for the collider, including crossing angles, dispersion correction,\n    RF, crab cavities, experimental magnets, etc.\n\n    Args:\n        collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n    Returns:\n        None\n    \"\"\"\n    # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n    # experimental magnets, etc.)\n    for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n        collider.vars[kk] = vv\n\n    # Crab fix (if needed)\n    if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n        apply_crab_fix(collider, self.config_knobs_and_tuning)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.update_configuration_knob","title":"<code>update_configuration_knob(collider, dictionnary, knob_name)</code>  <code>staticmethod</code>","text":"<p>Updates the given dictionary with the final value of a specified knob from the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing various variables.</p> required <code>dictionnary</code> <code>dict</code> <p>The dictionary to be updated with the knob's final value.</p> required <code>knob_name</code> <code>str</code> <p>The name of the knob whose value is to be retrieved and stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef update_configuration_knob(\n    collider: xt.Multiline, dictionnary: dict, knob_name: str\n) -&gt; None:\n    \"\"\"\n    Updates the given dictionary with the final value of a specified knob from the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object containing various variables.\n        dictionnary (dict): The dictionary to be updated with the knob's final value.\n        knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n    Returns:\n        None\n    \"\"\"\n    if knob_name in collider.vars.keys():\n        dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider, full_configuration)</code>","text":"<p>Writes the collider object to disk in JSON format if the save_output_collider flag is set.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Collider</code> <p>The collider object to be saved.</p> required <code>full_configuration</code> <code>dict</code> <p>The full configuration dictionary to be deep-copied into the collider's metadata.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n    Args:\n        collider (Collider): The collider object to be saved.\n        full_configuration (dict): The full configuration dictionary to be deep-copied into the\n            collider's metadata.\n\n    Returns:\n        None\n    \"\"\"\n    if self.save_output_collider:\n        logging.info(\"Saving collider as json\")\n        if (\n            hasattr(collider, \"metadata\")\n            and collider.metadata is not None\n            and isinstance(collider.metadata, dict)\n        ):\n            collider.metadata.update(copy.deepcopy(full_configuration))\n        else:\n            collider.metadata = copy.deepcopy(full_configuration)\n        collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_tracking_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking","title":"<code>XsuiteTracking</code>","text":"<p>XsuiteTracking class for managing particle tracking simulations.</p> <p>Attributes:</p> Name Type Description <code>context_str</code> <code>str</code> <p>The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").</p> <code>device_number</code> <code>int</code> <p>The device number for GPU contexts.</p> <code>_context</code> <code>Context</code> <p>The context object for the simulation.</p> <code>beam</code> <code>str</code> <p>The beam configuration.</p> <code>distribution_file</code> <code>str</code> <p>The file path to the particle data.</p> <code>delta_max</code> <code>float</code> <p>The maximum delta value for particles.</p> <code>n_turns</code> <code>int</code> <p>The number of turns for the simulation.</p> <code>nemitt_x</code> <code>float</code> <p>The normalized emittance in the x direction.</p> <code>nemitt_y</code> <code>float</code> <p>The normalized emittance in the y direction.</p> <p>Methods:</p> Name Description <code>context</code> <p>Get the context object for the simulation.</p> <code>prepare_particle_distribution_for_tracking</code> <p>Prepare the particle distribution for tracking.</p> <code>track</code> <p>Track the particles in the collider.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>class XsuiteTracking:\n    \"\"\"\n    XsuiteTracking class for managing particle tracking simulations.\n\n    Attributes:\n        context_str (str): The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").\n        device_number (int): The device number for GPU contexts.\n        _context (xo.Context): The context object for the simulation.\n        beam (str): The beam configuration.\n        distribution_file (str): The file path to the particle data.\n        delta_max (float): The maximum delta value for particles.\n        n_turns (int): The number of turns for the simulation.\n        nemitt_x (float): The normalized emittance in the x direction.\n        nemitt_y (float): The normalized emittance in the y direction.\n\n    Methods:\n        context: Get the context object for the simulation.\n        prepare_particle_distribution_for_tracking: Prepare the particle distribution for tracking.\n        track: Track the particles in the collider.\n    \"\"\"\n\n    def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n        \"\"\"\n        Initialize the tracking configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                Expected keys:\n                - \"context\": str, context string for the simulation.\n                - \"device_number\": int, device number for the simulation.\n                - \"beam\": str, beam type for the simulation.\n                - \"distribution_file\": str, path to the particle file.\n                - \"delta_max\": float, maximum delta value for the simulation.\n                - \"n_turns\": int, number of turns for the simulation.\n            nemitt_x (float): Normalized emittance in the x-plane.\n            nemitt_y (float): Normalized emittance in the y-plane.\n        \"\"\"\n        # Context parameters\n        self.context_str: str = configuration[\"context\"]\n        self.device_number: int = configuration[\"device_number\"]\n        self._context = None\n\n        # Simulation parameters\n        self.beam: str = configuration[\"beam\"]\n        self.distribution_file: str = configuration[\"distribution_file\"]\n        self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n        self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n        self.delta_max: float = configuration[\"delta_max\"]\n        self.n_turns: int = configuration[\"n_turns\"]\n\n        # Beambeam parameters\n        self.nemitt_x: float = nemitt_x\n        self.nemitt_y: float = nemitt_y\n\n    @property\n    def context(self) -&gt; Any:\n        \"\"\"\n        Returns the context for the current instance. If the context is not already set,\n        it initializes the context based on the `context_str` attribute. The context can\n        be one of the following:\n\n        - \"cupy\": Uses `xo.ContextCupy`. If `device_number` is specified, it initializes\n            the context with the given device number.\n        - \"opencl\": Uses `xo.ContextPyopencl`.\n        - \"cpu\": Uses `xo.ContextCpu`.\n        - Any other value: Logs a warning and defaults to `xo.ContextCpu`.\n\n        If `device_number` is specified but the context is not \"cupy\", a warning is logged\n        indicating that the device number will be ignored.\n\n        Returns:\n            Any: The initialized context.\n        \"\"\"\n        if self._context is None:\n            if self.device_number is not None and self.context_str not in [\"cupy\"]:\n                logging.warning(\"Device number will be ignored since context is not cupy\")\n            match self.context_str:\n                case \"cupy\":\n                    if self.device_number is not None:\n                        self._context = xo.ContextCupy(device=self.device_number)\n                    else:\n                        self._context = xo.ContextCupy()\n                case \"opencl\":\n                    self._context = xo.ContextPyopencl()\n                case \"cpu\":\n                    self._context = xo.ContextCpu()\n                case _:\n                    logging.warning(\"Context not recognized, using cpu\")\n                    self._context = xo.ContextCpu()\n        return self._context\n\n    # ? I removed type hints for the output as I get an unclear linting error\n    # TODO: Check the proper type hints for the output\n    def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n        \"\"\"\n        Prepare a particle distribution for tracking in the collider.\n\n        This method reads particle data from a parquet file, processes the data to\n        generate normalized amplitudes and angles, and then builds particles for\n        tracking in the collider. If the context is set to use GPU, the collider\n        trackers are reset and rebuilt accordingly.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and\n                tracking information.\n\n        Returns:\n            tuple: A tuple containing:\n                - xp.Particles: The particles ready for tracking.\n                - np.ndarray: Array of particle IDs.\n                - np.ndarray: Array of normalized amplitudes in the xy-plane.\n                - np.ndarray: Array of angles in the xy-plane in radians.\n        \"\"\"\n        # Reset the tracker to go to GPU if needed\n        if self.context_str in [\"cupy\", \"opencl\"]:\n            collider.discard_trackers()\n            collider.build_trackers(_context=self.context)\n\n        particle_df = pd.read_parquet(self.particle_path)\n\n        r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n        theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n        A1_in_sigma = r_vect * np.cos(theta_vect)\n        A2_in_sigma = r_vect * np.sin(theta_vect)\n\n        particles = collider[self.beam].build_particles(\n            x_norm=A1_in_sigma,\n            y_norm=A2_in_sigma,\n            delta=self.delta_max,\n            scale_with_transverse_norm_emitt=(\n                self.nemitt_x,\n                self.nemitt_y,\n            ),\n            _context=self.context,\n        )\n\n        particle_id = particle_df.particle_id.values\n        return particles, particle_id, r_vect, theta_vect\n\n    def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n        \"\"\"\n        Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beamline to be tracked.\n            particles (xp.Particles): The particles to be tracked.\n\n        Returns:\n            dict: A dictionary representation of the tracked particles.\n        \"\"\"\n        # Optimize line for tracking\n        collider[self.beam].optimize_for_tracking()\n\n        # Track\n        num_turns = self.n_turns\n        a = time.time()\n        collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n        b = time.time()\n\n        logging.info(f\"Elapsed time: {b-a} s\")\n        logging.info(\n            f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n        )\n\n        return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.context","title":"<code>context: Any</code>  <code>property</code>","text":"<p>Returns the context for the current instance. If the context is not already set, it initializes the context based on the <code>context_str</code> attribute. The context can be one of the following:</p> <ul> <li>\"cupy\": Uses <code>xo.ContextCupy</code>. If <code>device_number</code> is specified, it initializes     the context with the given device number.</li> <li>\"opencl\": Uses <code>xo.ContextPyopencl</code>.</li> <li>\"cpu\": Uses <code>xo.ContextCpu</code>.</li> <li>Any other value: Logs a warning and defaults to <code>xo.ContextCpu</code>.</li> </ul> <p>If <code>device_number</code> is specified but the context is not \"cupy\", a warning is logged indicating that the device number will be ignored.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized context.</p>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.__init__","title":"<code>__init__(configuration, nemitt_x, nemitt_y)</code>","text":"<p>Initialize the tracking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. Expected keys: - \"context\": str, context string for the simulation. - \"device_number\": int, device number for the simulation. - \"beam\": str, beam type for the simulation. - \"distribution_file\": str, path to the particle file. - \"delta_max\": float, maximum delta value for the simulation. - \"n_turns\": int, number of turns for the simulation.</p> required <code>nemitt_x</code> <code>float</code> <p>Normalized emittance in the x-plane.</p> required <code>nemitt_y</code> <code>float</code> <p>Normalized emittance in the y-plane.</p> required Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n    \"\"\"\n    Initialize the tracking configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            Expected keys:\n            - \"context\": str, context string for the simulation.\n            - \"device_number\": int, device number for the simulation.\n            - \"beam\": str, beam type for the simulation.\n            - \"distribution_file\": str, path to the particle file.\n            - \"delta_max\": float, maximum delta value for the simulation.\n            - \"n_turns\": int, number of turns for the simulation.\n        nemitt_x (float): Normalized emittance in the x-plane.\n        nemitt_y (float): Normalized emittance in the y-plane.\n    \"\"\"\n    # Context parameters\n    self.context_str: str = configuration[\"context\"]\n    self.device_number: int = configuration[\"device_number\"]\n    self._context = None\n\n    # Simulation parameters\n    self.beam: str = configuration[\"beam\"]\n    self.distribution_file: str = configuration[\"distribution_file\"]\n    self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n    self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n    self.delta_max: float = configuration[\"delta_max\"]\n    self.n_turns: int = configuration[\"n_turns\"]\n\n    # Beambeam parameters\n    self.nemitt_x: float = nemitt_x\n    self.nemitt_y: float = nemitt_y\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.prepare_particle_distribution_for_tracking","title":"<code>prepare_particle_distribution_for_tracking(collider)</code>","text":"<p>Prepare a particle distribution for tracking in the collider.</p> <p>This method reads particle data from a parquet file, processes the data to generate normalized amplitudes and angles, and then builds particles for tracking in the collider. If the context is set to use GPU, the collider trackers are reset and rebuilt accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and tracking information.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - xp.Particles: The particles ready for tracking. - np.ndarray: Array of particle IDs. - np.ndarray: Array of normalized amplitudes in the xy-plane. - np.ndarray: Array of angles in the xy-plane in radians.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n    \"\"\"\n    Prepare a particle distribution for tracking in the collider.\n\n    This method reads particle data from a parquet file, processes the data to\n    generate normalized amplitudes and angles, and then builds particles for\n    tracking in the collider. If the context is set to use GPU, the collider\n    trackers are reset and rebuilt accordingly.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and\n            tracking information.\n\n    Returns:\n        tuple: A tuple containing:\n            - xp.Particles: The particles ready for tracking.\n            - np.ndarray: Array of particle IDs.\n            - np.ndarray: Array of normalized amplitudes in the xy-plane.\n            - np.ndarray: Array of angles in the xy-plane in radians.\n    \"\"\"\n    # Reset the tracker to go to GPU if needed\n    if self.context_str in [\"cupy\", \"opencl\"]:\n        collider.discard_trackers()\n        collider.build_trackers(_context=self.context)\n\n    particle_df = pd.read_parquet(self.particle_path)\n\n    r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n    theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n    A1_in_sigma = r_vect * np.cos(theta_vect)\n    A2_in_sigma = r_vect * np.sin(theta_vect)\n\n    particles = collider[self.beam].build_particles(\n        x_norm=A1_in_sigma,\n        y_norm=A2_in_sigma,\n        delta=self.delta_max,\n        scale_with_transverse_norm_emitt=(\n            self.nemitt_x,\n            self.nemitt_y,\n        ),\n        _context=self.context,\n    )\n\n    particle_id = particle_df.particle_id.values\n    return particles, particle_id, r_vect, theta_vect\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.XsuiteTracking.track","title":"<code>track(collider, particles)</code>","text":"<p>Tracks particles through a collider for a specified number of turns and logs the elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beamline to be tracked.</p> required <code>particles</code> <code>Particles</code> <p>The particles to be tracked.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tracked particles.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n    \"\"\"\n    Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beamline to be tracked.\n        particles (xp.Particles): The particles to be tracked.\n\n    Returns:\n        dict: A dictionary representation of the tracked particles.\n    \"\"\"\n    # Optimize line for tracking\n    collider[self.beam].optimize_for_tracking()\n\n    # Track\n    num_turns = self.n_turns\n    a = time.time()\n    collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n    b = time.time()\n\n    logging.info(f\"Elapsed time: {b-a} s\")\n    logging.info(\n        f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n    )\n\n    return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/generate/index.html#study_da.generate.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html","title":"generate_scan","text":"<p>This class is used to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan","title":"<code>GenerateScan</code>","text":"<p>A class to generate a study (along with the corresponding tree) from a parameter file, and potentially a set of template files.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> <code>ryaml</code> <code>YAML</code> <p>The YAML parser.</p> <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary of common parameters across generations.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the generation scan with a configuration file or dictionary.</p> <code>render</code> <p>Renders the study file using a template.</p> <code>write</code> <p>Writes the study file to disk.</p> <code>generate_render_write</code> <p>Generates, renders, and writes the study file.</p> <code>get_dic_parametric_scans</code> <p>Retrieves dictionaries of parametric scan values.</p> <code>parse_parameter_space</code> <p>Parses the parameter space for a given parameter.</p> <code>browse_and_collect_parameter_space</code> <p>Browses and collects the parameter space for a given generation.</p> <code>postprocess_parameter_lists</code> <p>Postprocesses the parameter lists.</p> <code>create_scans</code> <p>Creates study files for parametric scans.</p> <code>complete_tree</code> <p>Completes the tree structure of the study dictionary.</p> <code>write_tree</code> <p>Writes the study tree structure to a YAML file.</p> <code>create_study_for_current_gen</code> <p>Creates study files for the current generation.</p> <code>create_study</code> <p>Creates study files for the entire study.</p> <code>eval_conditions</code> <p>Evaluates the conditions to filter out some parameter values.</p> <code>filter_for_concomitant_parameters</code> <p>Filters the conditions for concomitant parameters.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>class GenerateScan:\n    \"\"\"\n    A class to generate a study (along with the corresponding tree) from a parameter file,\n    and potentially a set of template files.\n\n    Attributes:\n        config (dict): The configuration dictionary.\n        ryaml (yaml.YAML): The YAML parser.\n        dic_common_parameters (dict): Dictionary of common parameters across generations.\n\n    Methods:\n        __init__(): Initializes the generation scan with a configuration file or dictionary.\n        render(): Renders the study file using a template.\n        write(): Writes the study file to disk.\n        generate_render_write(): Generates, renders, and writes the study file.\n        get_dic_parametric_scans(): Retrieves dictionaries of parametric scan values.\n        parse_parameter_space(): Parses the parameter space for a given parameter.\n        browse_and_collect_parameter_space(): Browses and collects the parameter space for a given\n            generation.\n        postprocess_parameter_lists(): Postprocesses the parameter lists.\n        create_scans(): Creates study files for parametric scans.\n        complete_tree(): Completes the tree structure of the study dictionary.\n        write_tree(): Writes the study tree structure to a YAML file.\n        create_study_for_current_gen(): Creates study files for the current generation.\n        create_study(): Creates study files for the entire study.\n        eval_conditions(): Evaluates the conditions to filter out some parameter values.\n        filter_for_concomitant_parameters(): Filters the conditions for concomitant parameters.\n    \"\"\"\n\n    def __init__(\n        self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n    ):  # sourcery skip: remove-redundant-if\n        \"\"\"\n        Initialize the generation scan with a configuration file or dictionary.\n\n        Args:\n            path_config (Optional[str]): Path to the configuration file for the scan.\n                Default is None.\n            dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n                Default is None.\n\n        Raises:\n            ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n        \"\"\"\n        # Load the study configuration from file or dictionary\n        if dic_scan is None and path_config is None:\n            raise ValueError(\n                \"Either a path to the configuration file or a dictionary must be provided.\"\n            )\n        elif dic_scan is not None and path_config is not None:\n            raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n        elif path_config is not None:\n            self.config, self.ryaml = load_dic_from_path(path_config)\n        elif dic_scan is not None:\n            self.config = dic_scan\n            self.ryaml = yaml.YAML()\n        else:\n            raise ValueError(\"An unexpected error occurred.\")\n\n        # Parameters common across all generations (e.g. for parallelization)\n        self.dic_common_parameters: dict[str, Any] = {}\n\n        # Path to the tree file\n        self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n\n    def render(\n        self,\n        str_parameters: str,\n        template_path: str,\n        dependencies: Optional[dict[str, str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Renders the study file using a template.\n\n        Args:\n            str_parameters (str): The string representation of parameters to declare/mutate.\n            template_path (str): The path to the template file.\n            dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n        Returns:\n            str: The rendered study file.\n        \"\"\"\n\n        # Handle mutable default argument\n        if dependencies is None:\n            dependencies = {}\n\n        # Generate generations from template\n        directory_path = os.path.dirname(template_path)\n        template_name = os.path.basename(template_path)\n        environment = Environment(loader=FileSystemLoader(directory_path))\n        template = environment.get_template(template_name)\n\n        return template.render(parameters=str_parameters, **dependencies)\n\n    def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n        \"\"\"\n        Writes the study file to disk.\n\n        Args:\n            study_str (str): The study file string.\n            file_path (str): The path to write the study file.\n            format_with_black (bool, optional): Whether to format the output file with black.\n                Defaults to True.\n        \"\"\"\n\n        # Format the string with black\n        if format_with_black:\n            study_str = format_str(study_str, mode=FileMode())\n\n        # Make folder if it doesn't exist\n        folder = os.path.dirname(file_path)\n        if folder != \"\":\n            os.makedirs(folder, exist_ok=True)\n\n        with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n            file.write(study_str)\n\n    def generate_render_write(\n        self,\n        gen_name: str,\n        study_path: str,\n        template_path: str,\n        dic_mutated_parameters: dict[str, Any] = {},\n    ) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n        \"\"\"\n        Generates, renders, and writes the study file.\n\n        Args:\n            gen_name (str): The name of the generation.\n            study_path (str): The path to the study folder.\n            template_path (str): The path to the template folder.\n            dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n                Defaults to {}.\n\n        Returns:\n            tuple[str, list[str]]: The study file string and the list of study paths.\n        \"\"\"\n\n        directory_path_gen = f\"{study_path}\"\n        if not directory_path_gen.endswith(\"/\"):\n            directory_path_gen += \"/\"\n        file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n        logging.info(f'Now rendering generation \"{file_path_gen}\"')\n        # Generate the string of parameters\n        str_parameters = \"{\"\n        for key, value in dic_mutated_parameters.items():\n            if isinstance(value, str):\n                str_parameters += f\"'{key}' : '{value}', \"\n            else:\n                str_parameters += f\"'{key}' : {value}, \"\n        str_parameters += \"}\"\n\n        # Adapt the dict of dependencies to the current generation\n        dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n        # Always load configuration from above generation\n        depth_gen = 1\n        # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n        dic_dependencies = {\n            key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n        }\n\n        # Render and write the study file\n        study_str = self.render(\n            str_parameters,\n            template_path=template_path,\n            dependencies=dic_dependencies,\n        )\n\n        self.write(study_str, file_path_gen)\n        return [directory_path_gen]\n\n    def get_dic_parametric_scans(\n        self, generation: str\n    ) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n        \"\"\"\n        Retrieves dictionaries of parametric scan values.\n\n        Args:\n            generation: The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n                scan values, another dictionnary with better naming for the tree creation, and an\n                array of conditions to filter out some parameter values.\n        \"\"\"\n\n        if generation == \"base\":\n            raise ValueError(\"Generation 'base' should not have scans.\")\n\n        # Remember common parameters as they might be used across generations\n        if \"common_parameters\" in self.config[\"structure\"][generation]:\n            self.dic_common_parameters[generation] = {}\n            for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n                self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                    generation\n                ][\"common_parameters\"][parameter]\n\n        # Check that the generation has scans\n        if (\n            \"scans\" not in self.config[\"structure\"][generation]\n            or self.config[\"structure\"][generation][\"scans\"] is None\n        ):\n            dic_parameter_lists = {\"\": [generation]}\n            dic_parameter_lists_for_naming = {\"\": [generation]}\n            array_conditions = None\n            ll_concomitant_parameters = []\n        else:\n            # Browse and collect the parameter space for the generation\n            (\n                dic_parameter_lists,\n                dic_parameter_lists_for_naming,\n                dic_subvariables,\n                ll_concomitant_parameters,\n                l_conditions,\n            ) = self.browse_and_collect_parameter_space(generation)\n\n            # Get the dimension corresponding to each parameter\n            dic_dimension_indices = {\n                parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n            }\n\n            # Generate array of conditions to filter out some of the values later\n            # Is an array of True values if no conditions are present\n            array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n            # Filter for concomitant parameters\n            array_conditions = self.filter_for_concomitant_parameters(\n                array_conditions, ll_concomitant_parameters, dic_dimension_indices\n            )\n\n            # Postprocess the parameter lists and update the dictionaries\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n                dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n            )\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            array_conditions,\n        )\n\n    def parse_parameter_space(\n        self,\n        parameter: str,\n        dic_curr_parameter: dict[str, Any],\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Parses the parameter space for a given parameter.\n\n        Args:\n            parameter (str): The parameter name.\n            dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n            dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n        \"\"\"\n\n        if \"linspace\" in dic_curr_parameter:\n            parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"logspace\" in dic_curr_parameter:\n            parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"path_list\" in dic_curr_parameter:\n            l_values_path_list = dic_curr_parameter[\"path_list\"]\n            parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n            dic_parameter_lists_for_naming[parameter] = [\n                f\"{n:02d}\" for n, path in enumerate(parameter_list)\n            ]\n        elif \"list\" in dic_curr_parameter:\n            parameter_list = dic_curr_parameter[\"list\"]\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        elif \"expression\" in dic_curr_parameter:\n            parameter_list = np.round(\n                eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n                8,\n            )\n            dic_parameter_lists_for_naming[parameter] = parameter_list\n        else:\n            raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n        dic_parameter_lists[parameter] = np.array(parameter_list)\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def browse_and_collect_parameter_space(\n        self,\n        generation: str,\n    ) -&gt; tuple[\n        dict[str, Any],\n        dict[str, Any],\n        dict[str, Any],\n        list[list[str]],\n        list[str],\n    ]:\n        \"\"\"\n        Browses and collects the parameter space for a given generation.\n\n        Args:\n            generation (str): The generation name.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n                dictionaries of parameter lists.\n        \"\"\"\n\n        l_conditions = []\n        ll_concomitant_parameters = []\n        dic_subvariables = {}\n        dic_parameter_lists = {}\n        dic_parameter_lists_for_naming = {}\n        for parameter in self.config[\"structure\"][generation][\"scans\"]:\n            dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n            # Parse the parameter space\n            dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n                parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n            )\n\n            # Store potential subvariables\n            if \"subvariables\" in dic_curr_parameter:\n                dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n            # Save the condition if it exists\n            if \"condition\" in dic_curr_parameter:\n                l_conditions.append(dic_curr_parameter[\"condition\"])\n\n            # Save the concomitant parameters if they exist\n            if \"concomitant\" in dic_curr_parameter:\n                if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                    dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n                for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                    # Assert that the parameters list have the same size\n                    assert len(dic_parameter_lists[parameter]) == len(\n                        dic_parameter_lists[concomitant_parameter]\n                    ), (\n                        f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                        \"same size.\"\n                    )\n                # Add to the list for filtering later\n                ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n        return (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        )\n\n    def postprocess_parameter_lists(\n        self,\n        dic_parameter_lists: dict[str, Any],\n        dic_parameter_lists_for_naming: dict[str, Any],\n        dic_subvariables: dict[str, Any],\n    ) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        Post-processes parameter lists by ensuring values are not numpy types and handling nested\n        parameters.\n\n        Args:\n            dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n            dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n                for naming.\n            dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n                parameters.\n\n        Returns:\n            tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n                parameter lists for naming.\n        \"\"\"\n        for parameter, parameter_list in dic_parameter_lists.items():\n            parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n            # Ensure that all values are not numpy types (to avoid serialization issues)\n            parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n            # Handle nested parameters\n            parameter_list_updated = (\n                convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n                if parameter in dic_subvariables\n                else parameter_list\n            )\n            # Update the dictionaries\n            dic_parameter_lists[parameter] = parameter_list_updated\n            dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n        return dic_parameter_lists, dic_parameter_lists_for_naming\n\n    def create_scans(\n        self,\n        generation: str,\n        generation_path: str,\n        template_path: str,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for parametric scans.\n\n        Args:\n            generation (str): The generation name.\n            generation_path (str): The path to the layer folder.\n            template_path (str): The path to the template folder.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        if dic_parameter_lists is None:\n            # Get dictionnary of parametric values being scanned\n            dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n                self.get_dic_parametric_scans(generation)\n            )\n        else:\n            if dic_parameter_lists_for_naming is None:\n                dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n            array_conditions = None\n\n        # Generate render write for cartesian product of all parameters\n        l_study_path = []\n        logging.info(\n            f\"Now generation cartesian product of all parameters for generation: {generation}\"\n        )\n        for l_values, l_values_for_naming, l_idx in zip(\n            itertools.product(*dic_parameter_lists.values()),\n            itertools.product(*dic_parameter_lists_for_naming.values()),\n            itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()]),\n        ):\n            # Check the idx to keep if conditions are present\n            if array_conditions is not None and not array_conditions[l_idx]:\n                continue\n\n            # Create the path for the study\n            dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n            dic_mutated_parameters_for_naming = dict(\n                zip(dic_parameter_lists.keys(), l_values_for_naming)\n            )\n            suffix_path = \"_\".join(\n                [\n                    f\"{parameter}_{value}\"\n                    for parameter, value in dic_mutated_parameters_for_naming.items()\n                ]\n            )\n\n            # Remove '_' at the beginning of the suffix path if needed (e.g. for generation)\n            suffix_path = suffix_path.removeprefix(\"_\")\n            # Create final path\n            path = generation_path + suffix_path + \"/\"\n\n            # Add common parameters\n            if generation in self.dic_common_parameters:\n                dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n            # Remove \"\" from mutated parameters, if it's in the dictionary\n            # as it's only used when no scan is done\n            if \"\" in dic_mutated_parameters:\n                dic_mutated_parameters.pop(\"\")\n\n            # Generate the study for current generation\n            self.generate_render_write(\n                generation,\n                path,\n                template_path,\n                dic_mutated_parameters=dic_mutated_parameters,\n            )\n\n            # Append the list of study paths to build the tree later on\n            l_study_path.append(path)\n\n        if not l_study_path:\n            logging.warning(\n                f\"No study paths were created for generation {generation}.\"\n                \"Please check the conditions.\"\n            )\n\n        return l_study_path\n\n    def complete_tree(\n        self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n    ) -&gt; dict:\n        \"\"\"\n        Completes the tree structure of the study dictionary.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n            l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n            gen (str): The generation name.\n\n        Returns:\n            dict: The updated dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(f\"Completing the tree structure for generation: {gen}\")\n        for path_next in l_study_path_next_gen:\n            nested_set(\n                dictionary_tree,\n                path_next.split(\"/\")[1:-1] + [gen],\n                {\"file\": f\"{path_next}{gen}.py\"},\n            )\n\n        return dictionary_tree\n\n    def write_tree(self, dictionary_tree: dict):\n        \"\"\"\n        Writes the study tree structure to a YAML file.\n\n        Args:\n            dictionary_tree (dict): The dictionary representing the study tree structure.\n        \"\"\"\n        logging.info(\"Writing the tree structure to a YAML file.\")\n        ryaml = yaml.YAML()\n        with open(self.path_tree, \"w\") as yaml_file:\n            ryaml.indent(sequence=4, offset=2)\n            ryaml.dump(dictionary_tree, yaml_file)\n\n    def create_study_for_current_gen(\n        self,\n        generation: str,\n        study_path: str,\n        dic_parameter_lists: Optional[dict[str, Any]] = None,\n        dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Creates study files for the current generation.\n\n        Args:\n            generation (str): The name of the current generation.\n            study_path (str): The path to the study folder.\n\n        Returns:\n            tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n        \"\"\"\n        executable_path = self.config[\"structure\"][generation][\"executable\"]\n        path_local_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/template_scripts/\"\n\n        # Check if the executable path corresponds to a file\n        if not os.path.isfile(executable_path):\n            # Check if the executable path corresponds to a file in the template folder\n            executable_path_template = f\"{path_local_template}{executable_path}\"\n            if not os.path.isfile(executable_path_template):\n                raise FileNotFoundError(\n                    f\"Executable file {executable_path} not found locally nor in the study-da \"\n                    \"template folder.\"\n                )\n            else:\n                executable_path = executable_path_template\n\n        # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n        if dic_parameter_lists is not None:\n            # Recursively convert all numpy types to standard types\n            clean_dic(dic_parameter_lists)\n            logging.info(\"An external dictionary of parameters was provided.\")\n        else:\n            logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n        return self.create_scans(\n            generation,\n            study_path,\n            executable_path,\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n        )\n\n    def create_study(\n        self,\n        tree_file: bool = True,\n        force_overwrite: bool = False,\n        dic_parameter_all_gen: Optional[dict[str, dict[str, Any]]] = None,\n        dic_parameter_all_gen_naming: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        l_study_path = [self.config[\"name\"] + \"/\"]\n        dictionary_tree = {}\n        \"\"\"\n        Creates study files for the entire study.\n\n        Args:\n            tree_file (bool, optional): Whether to write the study tree structure to a YAML file. \n                Defaults to True.\n            force_overwrite (bool, optional): Whether to overwrite existing study files. \n                Defaults to False.\n\n        Returns:\n            list[str]: The list of study file strings.\n        \"\"\"\n        # Raise an error if dic_parameter_all_gen_naming is not None while dic_parameter_all_gen is None\n        if dic_parameter_all_gen is None and dic_parameter_all_gen_naming is not None:\n            raise ValueError(\n                \"If dic_parameter_all_gen_naming is defined, dic_parameter_all_gen must be defined.\"\n            )\n\n        # Remove existing study if force_overwrite\n        if os.path.exists(self.config[\"name\"]):\n            if not force_overwrite:\n                logging.info(\n                    f\"Study {self.config['name']} already exists. Set force_overwrite to True to \"\n                    \"overwrite. Continuing without overwriting.\"\n                )\n                return\n            shutil.rmtree(self.config[\"name\"])\n\n        # Browse through the generations\n        l_generations = list(self.config[\"structure\"].keys())\n        for idx, generation in enumerate(l_generations):\n            l_study_path_all_next_generation = []\n            logging.info(f\"Taking care of generation: {generation}\")\n            for study_path in l_study_path:\n                if dic_parameter_all_gen is None or generation not in dic_parameter_all_gen:\n                    dic_parameter_current_gen = None\n                    dic_parameter_naming_current_gen = None\n                else:\n                    dic_parameter_current_gen = dic_parameter_all_gen[generation]\n                    if (\n                        dic_parameter_all_gen_naming is not None\n                        and generation in dic_parameter_all_gen_naming\n                    ):\n                        dic_parameter_naming_current_gen = dic_parameter_all_gen_naming[generation]\n                    else:\n                        dic_parameter_naming_current_gen = None\n\n                # Get list of paths for the children of the current study\n                l_study_path_next_generation = self.create_study_for_current_gen(\n                    generation,\n                    study_path,\n                    dic_parameter_current_gen,\n                    dic_parameter_naming_current_gen,\n                )\n                # Update tree\n                dictionary_tree = self.complete_tree(\n                    dictionary_tree, l_study_path_next_generation, generation\n                )\n                # Complete list of paths for the children of all studies (of the current generation)\n                l_study_path_all_next_generation.extend(l_study_path_next_generation)\n\n            # Update study path for next later\n            l_study_path = l_study_path_all_next_generation\n\n        # Add dependencies to the study\n        if \"dependencies\" in self.config:\n            for dependency, path in self.config[\"dependencies\"].items():\n                shutil.copy2(path, self.config[\"name\"])\n\n        if tree_file:\n            self.write_tree(dictionary_tree)\n\n    @staticmethod\n    def eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"\n        Evaluates the conditions to filter out some parameter values.\n\n        Args:\n            l_condition (list[str]): The list of conditions.\n            dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n        Returns:\n            np.ndarray: The array of conditions.\n        \"\"\"\n        # Initialize the array of parameters as a meshgrid of all parameters\n        l_parameters = list(dic_parameter_lists.values())\n        meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n        # Associate the parameters to their names\n        dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n        # Evaluate the conditions and take the intersection of all conditions\n        array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n        for condition in l_condition:\n            array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n        return array_conditions\n\n    @staticmethod\n    def filter_for_concomitant_parameters(\n        array_conditions: np.ndarray,\n        ll_concomitant_parameters: list[list[str]],\n        dic_dimension_indices: dict[str, int],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Filters the conditions for concomitant parameters.\n\n        Args:\n            array_conditions (np.ndarray): The array of conditions.\n            ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n            dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n        Returns:\n            np.ndarray: The filtered array of conditions.\n        \"\"\"\n\n        # Return the array of conditions if no concomitant parameters\n        if not ll_concomitant_parameters:\n            return array_conditions\n\n        # Get the indices of the concomitant parameters\n        ll_idx_concomitant_parameters = [\n            [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n            for concomitant_parameters in ll_concomitant_parameters\n        ]\n\n        # Browse all the values of array_conditions\n        for idx, _ in np.ndenumerate(array_conditions):\n            # Check if the value is on the diagonal of the concomitant parameters\n            for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n                if any(\n                    idx[i] != idx[j]\n                    for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n                ):\n                    array_conditions[idx] = False\n                    break\n\n        return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.__init__","title":"<code>__init__(path_config=None, dic_scan=None)</code>","text":"<p>Initialize the generation scan with a configuration file or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path_config</code> <code>Optional[str]</code> <p>Path to the configuration file for the scan. Default is None.</p> <code>None</code> <code>dic_scan</code> <code>Optional[dict[str, Any]]</code> <p>Dictionary containing the scan configuration. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both of <code>path_config</code> and <code>dic_scan</code> are provided.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def __init__(\n    self, path_config: Optional[str] = None, dic_scan: Optional[dict[str, Any]] = None\n):  # sourcery skip: remove-redundant-if\n    \"\"\"\n    Initialize the generation scan with a configuration file or dictionary.\n\n    Args:\n        path_config (Optional[str]): Path to the configuration file for the scan.\n            Default is None.\n        dic_scan (Optional[dict[str, Any]]): Dictionary containing the scan configuration.\n            Default is None.\n\n    Raises:\n        ValueError: If neither or both of `path_config` and `dic_scan` are provided.\n    \"\"\"\n    # Load the study configuration from file or dictionary\n    if dic_scan is None and path_config is None:\n        raise ValueError(\n            \"Either a path to the configuration file or a dictionary must be provided.\"\n        )\n    elif dic_scan is not None and path_config is not None:\n        raise ValueError(\"Only one of the configuration file or dictionary must be provided.\")\n    elif path_config is not None:\n        self.config, self.ryaml = load_dic_from_path(path_config)\n    elif dic_scan is not None:\n        self.config = dic_scan\n        self.ryaml = yaml.YAML()\n    else:\n        raise ValueError(\"An unexpected error occurred.\")\n\n    # Parameters common across all generations (e.g. for parallelization)\n    self.dic_common_parameters: dict[str, Any] = {}\n\n    # Path to the tree file\n    self.path_tree = self.config[\"name\"] + \"/\" + \"tree.yaml\"\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.browse_and_collect_parameter_space","title":"<code>browse_and_collect_parameter_space(generation)</code>","text":"<p>Browses and collects the parameter space for a given generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]], list[str]]</code> <p>tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def browse_and_collect_parameter_space(\n    self,\n    generation: str,\n) -&gt; tuple[\n    dict[str, Any],\n    dict[str, Any],\n    dict[str, Any],\n    list[list[str]],\n    list[str],\n]:\n    \"\"\"\n    Browses and collects the parameter space for a given generation.\n\n    Args:\n        generation (str): The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], dict[str, Any], list[list[str]]]: The updated\n            dictionaries of parameter lists.\n    \"\"\"\n\n    l_conditions = []\n    ll_concomitant_parameters = []\n    dic_subvariables = {}\n    dic_parameter_lists = {}\n    dic_parameter_lists_for_naming = {}\n    for parameter in self.config[\"structure\"][generation][\"scans\"]:\n        dic_curr_parameter = self.config[\"structure\"][generation][\"scans\"][parameter]\n\n        # Parse the parameter space\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.parse_parameter_space(\n            parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming\n        )\n\n        # Store potential subvariables\n        if \"subvariables\" in dic_curr_parameter:\n            dic_subvariables[parameter] = dic_curr_parameter[\"subvariables\"]\n\n        # Save the condition if it exists\n        if \"condition\" in dic_curr_parameter:\n            l_conditions.append(dic_curr_parameter[\"condition\"])\n\n        # Save the concomitant parameters if they exist\n        if \"concomitant\" in dic_curr_parameter:\n            if not isinstance(dic_curr_parameter[\"concomitant\"], list):\n                dic_curr_parameter[\"concomitant\"] = [dic_curr_parameter[\"concomitant\"]]\n            for concomitant_parameter in dic_curr_parameter[\"concomitant\"]:\n                # Assert that the parameters list have the same size\n                assert len(dic_parameter_lists[parameter]) == len(\n                    dic_parameter_lists[concomitant_parameter]\n                ), (\n                    f\"Parameters {parameter} and {concomitant_parameter} must have the \"\n                    \"same size.\"\n                )\n            # Add to the list for filtering later\n            ll_concomitant_parameters.append([parameter] + dic_curr_parameter[\"concomitant\"])\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        dic_subvariables,\n        ll_concomitant_parameters,\n        l_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.complete_tree","title":"<code>complete_tree(dictionary_tree, l_study_path_next_gen, gen)</code>","text":"<p>Completes the tree structure of the study dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required <code>l_study_path_next_gen</code> <code>list[str]</code> <p>The list of study paths for the next gen.</p> required <code>gen</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary representing the study tree structure.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def complete_tree(\n    self, dictionary_tree: dict, l_study_path_next_gen: list[str], gen: str\n) -&gt; dict:\n    \"\"\"\n    Completes the tree structure of the study dictionary.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n        l_study_path_next_gen (list[str]): The list of study paths for the next gen.\n        gen (str): The generation name.\n\n    Returns:\n        dict: The updated dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(f\"Completing the tree structure for generation: {gen}\")\n    for path_next in l_study_path_next_gen:\n        nested_set(\n            dictionary_tree,\n            path_next.split(\"/\")[1:-1] + [gen],\n            {\"file\": f\"{path_next}{gen}.py\"},\n        )\n\n    return dictionary_tree\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.create_scans","title":"<code>create_scans(generation, generation_path, template_path, dic_parameter_lists=None, dic_parameter_lists_for_naming=None)</code>","text":"<p>Creates study files for parametric scans.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <code>generation_path</code> <code>str</code> <p>The path to the layer folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_scans(\n    self,\n    generation: str,\n    generation_path: str,\n    template_path: str,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for parametric scans.\n\n    Args:\n        generation (str): The generation name.\n        generation_path (str): The path to the layer folder.\n        template_path (str): The path to the template folder.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    if dic_parameter_lists is None:\n        # Get dictionnary of parametric values being scanned\n        dic_parameter_lists, dic_parameter_lists_for_naming, array_conditions = (\n            self.get_dic_parametric_scans(generation)\n        )\n    else:\n        if dic_parameter_lists_for_naming is None:\n            dic_parameter_lists_for_naming = copy.deepcopy(dic_parameter_lists)\n        array_conditions = None\n\n    # Generate render write for cartesian product of all parameters\n    l_study_path = []\n    logging.info(\n        f\"Now generation cartesian product of all parameters for generation: {generation}\"\n    )\n    for l_values, l_values_for_naming, l_idx in zip(\n        itertools.product(*dic_parameter_lists.values()),\n        itertools.product(*dic_parameter_lists_for_naming.values()),\n        itertools.product(*[range(len(x)) for x in dic_parameter_lists.values()]),\n    ):\n        # Check the idx to keep if conditions are present\n        if array_conditions is not None and not array_conditions[l_idx]:\n            continue\n\n        # Create the path for the study\n        dic_mutated_parameters = dict(zip(dic_parameter_lists.keys(), l_values))\n        dic_mutated_parameters_for_naming = dict(\n            zip(dic_parameter_lists.keys(), l_values_for_naming)\n        )\n        suffix_path = \"_\".join(\n            [\n                f\"{parameter}_{value}\"\n                for parameter, value in dic_mutated_parameters_for_naming.items()\n            ]\n        )\n\n        # Remove '_' at the beginning of the suffix path if needed (e.g. for generation)\n        suffix_path = suffix_path.removeprefix(\"_\")\n        # Create final path\n        path = generation_path + suffix_path + \"/\"\n\n        # Add common parameters\n        if generation in self.dic_common_parameters:\n            dic_mutated_parameters |= self.dic_common_parameters[generation]\n\n        # Remove \"\" from mutated parameters, if it's in the dictionary\n        # as it's only used when no scan is done\n        if \"\" in dic_mutated_parameters:\n            dic_mutated_parameters.pop(\"\")\n\n        # Generate the study for current generation\n        self.generate_render_write(\n            generation,\n            path,\n            template_path,\n            dic_mutated_parameters=dic_mutated_parameters,\n        )\n\n        # Append the list of study paths to build the tree later on\n        l_study_path.append(path)\n\n    if not l_study_path:\n        logging.warning(\n            f\"No study paths were created for generation {generation}.\"\n            \"Please check the conditions.\"\n        )\n\n    return l_study_path\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.create_study_for_current_gen","title":"<code>create_study_for_current_gen(generation, study_path, dic_parameter_lists=None, dic_parameter_lists_for_naming=None)</code>","text":"<p>Creates study files for the current generation.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The name of the current generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the study folder.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[list[str], list[str]]: The list of study file strings and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def create_study_for_current_gen(\n    self,\n    generation: str,\n    study_path: str,\n    dic_parameter_lists: Optional[dict[str, Any]] = None,\n    dic_parameter_lists_for_naming: Optional[dict[str, Any]] = None,\n) -&gt; list[str]:\n    \"\"\"\n    Creates study files for the current generation.\n\n    Args:\n        generation (str): The name of the current generation.\n        study_path (str): The path to the study folder.\n\n    Returns:\n        tuple[list[str], list[str]]: The list of study file strings and the list of study paths.\n    \"\"\"\n    executable_path = self.config[\"structure\"][generation][\"executable\"]\n    path_local_template = f\"{os.path.dirname(inspect.getfile(GenerateScan))}/template_scripts/\"\n\n    # Check if the executable path corresponds to a file\n    if not os.path.isfile(executable_path):\n        # Check if the executable path corresponds to a file in the template folder\n        executable_path_template = f\"{path_local_template}{executable_path}\"\n        if not os.path.isfile(executable_path_template):\n            raise FileNotFoundError(\n                f\"Executable file {executable_path} not found locally nor in the study-da \"\n                \"template folder.\"\n            )\n        else:\n            executable_path = executable_path_template\n\n    # Ensure that the values in dic_parameter_lists can be dumped with ryaml\n    if dic_parameter_lists is not None:\n        # Recursively convert all numpy types to standard types\n        clean_dic(dic_parameter_lists)\n        logging.info(\"An external dictionary of parameters was provided.\")\n    else:\n        logging.info(\"Creating the dictionnary of parameters from the configuration file.\")\n\n    return self.create_scans(\n        generation,\n        study_path,\n        executable_path,\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.eval_conditions","title":"<code>eval_conditions(l_condition, dic_parameter_lists)</code>  <code>staticmethod</code>","text":"<p>Evaluates the conditions to filter out some parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>l_condition</code> <code>list[str]</code> <p>The list of conditions.</p> required <code>dic_parameter_lists</code> <code>dict[str</code> <p>Any]): The dictionary of parameter lists.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef eval_conditions(l_condition: list[str], dic_parameter_lists: dict[str, Any]) -&gt; np.ndarray:\n    \"\"\"\n    Evaluates the conditions to filter out some parameter values.\n\n    Args:\n        l_condition (list[str]): The list of conditions.\n        dic_parameter_lists (dict[str: Any]): The dictionary of parameter lists.\n\n    Returns:\n        np.ndarray: The array of conditions.\n    \"\"\"\n    # Initialize the array of parameters as a meshgrid of all parameters\n    l_parameters = list(dic_parameter_lists.values())\n    meshgrid = np.meshgrid(*l_parameters, indexing=\"ij\")\n\n    # Associate the parameters to their names\n    dic_param_mesh = dict(zip(dic_parameter_lists.keys(), meshgrid))\n\n    # Evaluate the conditions and take the intersection of all conditions\n    array_conditions = np.ones_like(meshgrid[0], dtype=bool)\n    for condition in l_condition:\n        array_conditions = array_conditions &amp; eval(condition, dic_param_mesh)\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.filter_for_concomitant_parameters","title":"<code>filter_for_concomitant_parameters(array_conditions, ll_concomitant_parameters, dic_dimension_indices)</code>  <code>staticmethod</code>","text":"<p>Filters the conditions for concomitant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>array_conditions</code> <code>ndarray</code> <p>The array of conditions.</p> required <code>ll_concomitant_parameters</code> <code>list[list[str]]</code> <p>The list of concomitant parameters.</p> required <code>dic_dimension_indices</code> <code>dict[str, int]</code> <p>The dictionary of dimension indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The filtered array of conditions.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>@staticmethod\ndef filter_for_concomitant_parameters(\n    array_conditions: np.ndarray,\n    ll_concomitant_parameters: list[list[str]],\n    dic_dimension_indices: dict[str, int],\n) -&gt; np.ndarray:\n    \"\"\"\n    Filters the conditions for concomitant parameters.\n\n    Args:\n        array_conditions (np.ndarray): The array of conditions.\n        ll_concomitant_parameters (list[list[str]]): The list of concomitant parameters.\n        dic_dimension_indices (dict[str, int]): The dictionary of dimension indices.\n\n    Returns:\n        np.ndarray: The filtered array of conditions.\n    \"\"\"\n\n    # Return the array of conditions if no concomitant parameters\n    if not ll_concomitant_parameters:\n        return array_conditions\n\n    # Get the indices of the concomitant parameters\n    ll_idx_concomitant_parameters = [\n        [dic_dimension_indices[parameter] for parameter in concomitant_parameters]\n        for concomitant_parameters in ll_concomitant_parameters\n    ]\n\n    # Browse all the values of array_conditions\n    for idx, _ in np.ndenumerate(array_conditions):\n        # Check if the value is on the diagonal of the concomitant parameters\n        for l_idx_concomitant_parameter in ll_idx_concomitant_parameters:\n            if any(\n                idx[i] != idx[j]\n                for i, j in itertools.combinations(l_idx_concomitant_parameter, 2)\n            ):\n                array_conditions[idx] = False\n                break\n\n    return array_conditions\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.generate_render_write","title":"<code>generate_render_write(gen_name, study_path, template_path, dic_mutated_parameters={})</code>","text":"<p>Generates, renders, and writes the study file.</p> <p>Parameters:</p> Name Type Description Default <code>gen_name</code> <code>str</code> <p>The name of the generation.</p> required <code>study_path</code> <code>str</code> <p>The path to the study folder.</p> required <code>template_path</code> <code>str</code> <p>The path to the template folder.</p> required <code>dic_mutated_parameters</code> <code>dict[str, Any]</code> <p>The dictionary of mutated parameters. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, list[str]]: The study file string and the list of study paths.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def generate_render_write(\n    self,\n    gen_name: str,\n    study_path: str,\n    template_path: str,\n    dic_mutated_parameters: dict[str, Any] = {},\n) -&gt; list[str]:  # sourcery skip: default-mutable-arg\n    \"\"\"\n    Generates, renders, and writes the study file.\n\n    Args:\n        gen_name (str): The name of the generation.\n        study_path (str): The path to the study folder.\n        template_path (str): The path to the template folder.\n        dic_mutated_parameters (dict[str, Any], optional): The dictionary of mutated parameters.\n            Defaults to {}.\n\n    Returns:\n        tuple[str, list[str]]: The study file string and the list of study paths.\n    \"\"\"\n\n    directory_path_gen = f\"{study_path}\"\n    if not directory_path_gen.endswith(\"/\"):\n        directory_path_gen += \"/\"\n    file_path_gen = f\"{directory_path_gen}{gen_name}.py\"\n    logging.info(f'Now rendering generation \"{file_path_gen}\"')\n    # Generate the string of parameters\n    str_parameters = \"{\"\n    for key, value in dic_mutated_parameters.items():\n        if isinstance(value, str):\n            str_parameters += f\"'{key}' : '{value}', \"\n        else:\n            str_parameters += f\"'{key}' : {value}, \"\n    str_parameters += \"}\"\n\n    # Adapt the dict of dependencies to the current generation\n    dic_dependencies = self.config[\"dependencies\"] if \"dependencies\" in self.config else {}\n    # Always load configuration from above generation\n    depth_gen = 1\n    # Initial dependencies are always copied at the root of the study (hence value.split(\"/\")[-1])\n    dic_dependencies = {\n        key: \"../\" * depth_gen + value.split(\"/\")[-1] for key, value in dic_dependencies.items()\n    }\n\n    # Render and write the study file\n    study_str = self.render(\n        str_parameters,\n        template_path=template_path,\n        dependencies=dic_dependencies,\n    )\n\n    self.write(study_str, file_path_gen)\n    return [directory_path_gen]\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.get_dic_parametric_scans","title":"<code>get_dic_parametric_scans(generation)</code>","text":"<p>Retrieves dictionaries of parametric scan values.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generation name.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any], ndarray | None]</code> <p>tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric scan values, another dictionnary with better naming for the tree creation, and an array of conditions to filter out some parameter values.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def get_dic_parametric_scans(\n    self, generation: str\n) -&gt; tuple[dict[str, Any], dict[str, Any], np.ndarray | None]:\n    \"\"\"\n    Retrieves dictionaries of parametric scan values.\n\n    Args:\n        generation: The generation name.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any], np.ndarray|None]: The dictionaries of parametric\n            scan values, another dictionnary with better naming for the tree creation, and an\n            array of conditions to filter out some parameter values.\n    \"\"\"\n\n    if generation == \"base\":\n        raise ValueError(\"Generation 'base' should not have scans.\")\n\n    # Remember common parameters as they might be used across generations\n    if \"common_parameters\" in self.config[\"structure\"][generation]:\n        self.dic_common_parameters[generation] = {}\n        for parameter in self.config[\"structure\"][generation][\"common_parameters\"]:\n            self.dic_common_parameters[generation][parameter] = self.config[\"structure\"][\n                generation\n            ][\"common_parameters\"][parameter]\n\n    # Check that the generation has scans\n    if (\n        \"scans\" not in self.config[\"structure\"][generation]\n        or self.config[\"structure\"][generation][\"scans\"] is None\n    ):\n        dic_parameter_lists = {\"\": [generation]}\n        dic_parameter_lists_for_naming = {\"\": [generation]}\n        array_conditions = None\n        ll_concomitant_parameters = []\n    else:\n        # Browse and collect the parameter space for the generation\n        (\n            dic_parameter_lists,\n            dic_parameter_lists_for_naming,\n            dic_subvariables,\n            ll_concomitant_parameters,\n            l_conditions,\n        ) = self.browse_and_collect_parameter_space(generation)\n\n        # Get the dimension corresponding to each parameter\n        dic_dimension_indices = {\n            parameter: idx for idx, parameter in enumerate(dic_parameter_lists)\n        }\n\n        # Generate array of conditions to filter out some of the values later\n        # Is an array of True values if no conditions are present\n        array_conditions = self.eval_conditions(l_conditions, dic_parameter_lists)\n\n        # Filter for concomitant parameters\n        array_conditions = self.filter_for_concomitant_parameters(\n            array_conditions, ll_concomitant_parameters, dic_dimension_indices\n        )\n\n        # Postprocess the parameter lists and update the dictionaries\n        dic_parameter_lists, dic_parameter_lists_for_naming = self.postprocess_parameter_lists(\n            dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables\n        )\n\n    return (\n        dic_parameter_lists,\n        dic_parameter_lists_for_naming,\n        array_conditions,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.parse_parameter_space","title":"<code>parse_parameter_space(parameter, dic_curr_parameter, dic_parameter_lists, dic_parameter_lists_for_naming)</code>","text":"<p>Parses the parameter space for a given parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>str</code> <p>The parameter name.</p> required <code>dic_curr_parameter</code> <code>dict[str, Any]</code> <p>The dictionary of current parameter values.</p> required <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>The dictionary of parameter lists for naming.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def parse_parameter_space(\n    self,\n    parameter: str,\n    dic_curr_parameter: dict[str, Any],\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Parses the parameter space for a given parameter.\n\n    Args:\n        parameter (str): The parameter name.\n        dic_curr_parameter (dict[str, Any]): The dictionary of current parameter values.\n        dic_parameter_lists (dict[str, Any]): The dictionary of parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): The dictionary of parameter lists for naming.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: The updated dictionaries of parameter lists.\n    \"\"\"\n\n    if \"linspace\" in dic_curr_parameter:\n        parameter_list = linspace(dic_curr_parameter[\"linspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"logspace\" in dic_curr_parameter:\n        parameter_list = logspace(dic_curr_parameter[\"logspace\"])\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"path_list\" in dic_curr_parameter:\n        l_values_path_list = dic_curr_parameter[\"path_list\"]\n        parameter_list = list_values_path(l_values_path_list, self.dic_common_parameters)\n        dic_parameter_lists_for_naming[parameter] = [\n            f\"{n:02d}\" for n, path in enumerate(parameter_list)\n        ]\n    elif \"list\" in dic_curr_parameter:\n        parameter_list = dic_curr_parameter[\"list\"]\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    elif \"expression\" in dic_curr_parameter:\n        parameter_list = np.round(\n            eval(dic_curr_parameter[\"expression\"], copy.deepcopy(dic_parameter_lists)),\n            8,\n        )\n        dic_parameter_lists_for_naming[parameter] = parameter_list\n    else:\n        raise ValueError(f\"Scanning method for parameter {parameter} is not recognized.\")\n\n    dic_parameter_lists[parameter] = np.array(parameter_list)\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.postprocess_parameter_lists","title":"<code>postprocess_parameter_lists(dic_parameter_lists, dic_parameter_lists_for_naming, dic_subvariables)</code>","text":"<p>Post-processes parameter lists by ensuring values are not numpy types and handling nested parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dic_parameter_lists</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists.</p> required <code>dic_parameter_lists_for_naming</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter lists for naming.</p> required <code>dic_subvariables</code> <code>dict[str, Any]</code> <p>Dictionary containing subvariables for nested parameters.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Any], dict[str, Any]]</code> <p>tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and parameter lists for naming.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def postprocess_parameter_lists(\n    self,\n    dic_parameter_lists: dict[str, Any],\n    dic_parameter_lists_for_naming: dict[str, Any],\n    dic_subvariables: dict[str, Any],\n) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    \"\"\"\n    Post-processes parameter lists by ensuring values are not numpy types and handling nested\n    parameters.\n\n    Args:\n        dic_parameter_lists (dict[str, Any]): Dictionary containing parameter lists.\n        dic_parameter_lists_for_naming (dict[str, Any]): Dictionary containing parameter lists\n            for naming.\n        dic_subvariables (dict[str, Any]): Dictionary containing subvariables for nested\n            parameters.\n\n    Returns:\n        tuple[dict[str, Any], dict[str, Any]]: Updated dictionaries of parameter lists and\n            parameter lists for naming.\n    \"\"\"\n    for parameter, parameter_list in dic_parameter_lists.items():\n        parameter_list_for_naming = dic_parameter_lists_for_naming[parameter]\n\n        # Ensure that all values are not numpy types (to avoid serialization issues)\n        parameter_list = [x.item() if isinstance(x, np.generic) else x for x in parameter_list]\n\n        # Handle nested parameters\n        parameter_list_updated = (\n            convert_for_subvariables(dic_subvariables[parameter], parameter_list)\n            if parameter in dic_subvariables\n            else parameter_list\n        )\n        # Update the dictionaries\n        dic_parameter_lists[parameter] = parameter_list_updated\n        dic_parameter_lists_for_naming[parameter] = parameter_list_for_naming\n\n    return dic_parameter_lists, dic_parameter_lists_for_naming\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.render","title":"<code>render(str_parameters, template_path, dependencies=None)</code>","text":"<p>Renders the study file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>str_parameters</code> <code>str</code> <p>The string representation of parameters to declare/mutate.</p> required <code>template_path</code> <code>str</code> <p>The path to the template file.</p> required <code>dependencies</code> <code>dict[str, str]</code> <p>The dictionary of dependencies. Defaults to {}.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rendered study file.</p> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def render(\n    self,\n    str_parameters: str,\n    template_path: str,\n    dependencies: Optional[dict[str, str]] = None,\n) -&gt; str:\n    \"\"\"\n    Renders the study file using a template.\n\n    Args:\n        str_parameters (str): The string representation of parameters to declare/mutate.\n        template_path (str): The path to the template file.\n        dependencies (dict[str, str], optional): The dictionary of dependencies. Defaults to {}.\n\n    Returns:\n        str: The rendered study file.\n    \"\"\"\n\n    # Handle mutable default argument\n    if dependencies is None:\n        dependencies = {}\n\n    # Generate generations from template\n    directory_path = os.path.dirname(template_path)\n    template_name = os.path.basename(template_path)\n    environment = Environment(loader=FileSystemLoader(directory_path))\n    template = environment.get_template(template_name)\n\n    return template.render(parameters=str_parameters, **dependencies)\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.write","title":"<code>write(study_str, file_path, format_with_black=True)</code>","text":"<p>Writes the study file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>study_str</code> <code>str</code> <p>The study file string.</p> required <code>file_path</code> <code>str</code> <p>The path to write the study file.</p> required <code>format_with_black</code> <code>bool</code> <p>Whether to format the output file with black. Defaults to True.</p> <code>True</code> Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write(self, study_str: str, file_path: str, format_with_black: bool = True):\n    \"\"\"\n    Writes the study file to disk.\n\n    Args:\n        study_str (str): The study file string.\n        file_path (str): The path to write the study file.\n        format_with_black (bool, optional): Whether to format the output file with black.\n            Defaults to True.\n    \"\"\"\n\n    # Format the string with black\n    if format_with_black:\n        study_str = format_str(study_str, mode=FileMode())\n\n    # Make folder if it doesn't exist\n    folder = os.path.dirname(file_path)\n    if folder != \"\":\n        os.makedirs(folder, exist_ok=True)\n\n    with open(file_path, mode=\"w\", encoding=\"utf-8\") as file:\n        file.write(study_str)\n</code></pre>"},{"location":"reference/study_da/generate/generate_scan.html#study_da.generate.generate_scan.GenerateScan.write_tree","title":"<code>write_tree(dictionary_tree)</code>","text":"<p>Writes the study tree structure to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_tree</code> <code>dict</code> <p>The dictionary representing the study tree structure.</p> required Source code in <code>study_da/generate/generate_scan.py</code> <pre><code>def write_tree(self, dictionary_tree: dict):\n    \"\"\"\n    Writes the study tree structure to a YAML file.\n\n    Args:\n        dictionary_tree (dict): The dictionary representing the study tree structure.\n    \"\"\"\n    logging.info(\"Writing the tree structure to a YAML file.\")\n    ryaml = yaml.YAML()\n    with open(self.path_tree, \"w\") as yaml_file:\n        ryaml.indent(sequence=4, offset=2)\n        ryaml.dump(dictionary_tree, yaml_file)\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html","title":"parameter_space","text":"<p>This module provides functions to generate parameter spaces for studies.</p> <p>Functions:</p> Name Description <code>convert_for_subvariables</code> <p>list[str], parameter_list: list) -&gt; list: Convert the parameter list to a list of dictionaries with subvariables as keys.</p> <code>linspace</code> <p>list) -&gt; np.ndarray: Generate a list of evenly spaced values over a specified interval.</p> <code>logspace</code> <p>list) -&gt; np.ndarray: Generate a list of values that are evenly spaced on a log scale.</p> <code>list_values_path</code> <p>list[str], dic_common_parameters: dict[str, Any]) -&gt; list[str]: Generate a list of path names from an initial path name.</p>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.convert_for_subvariables","title":"<code>convert_for_subvariables(l_subvariables, parameter_list)</code>","text":"<p>Convert the parameter list to a list of dictionaries with subvariables as keys.</p> <p>Parameters:</p> Name Type Description Default <code>l_subvariables</code> <code>list[str]</code> <p>List of subvariables.</p> required <code>parameter_list</code> <code>list</code> <p>List with the parameter values.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of dictionaries with subvariables as keys.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def convert_for_subvariables(l_subvariables: list[str], parameter_list: list) -&gt; list:\n    \"\"\"Convert the parameter list to a list of dictionaries with subvariables as keys.\n\n    Args:\n        l_subvariables (list[str]): List of subvariables.\n        parameter_list (list): List with the parameter values.\n\n    Returns:\n        list: List of dictionaries with subvariables as keys.\n    \"\"\"\n    return [{subvar: value for subvar in l_subvariables} for value in parameter_list]\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.linspace","title":"<code>linspace(l_values_linspace)</code>","text":"<p>Generate a list of evenly spaced values over a specified interval.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_linspace</code> <code>list</code> <p>List with the values for the linspace function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: List of evenly spaced values.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def linspace(l_values_linspace: list) -&gt; np.ndarray:\n    \"\"\"Generate a list of evenly spaced values over a specified interval.\n\n    Args:\n        l_values_linspace (list): List with the values for the linspace function.\n\n    Returns:\n        np.ndarray: List of evenly spaced values.\"\"\"\n\n    # Check that all values in the list are floats or integers\n    if not all(isinstance(value, (float, int)) for value in l_values_linspace):\n        raise ValueError(\n            \"All values in the list for the linspace function must be floats or integers.\"\n        )\n    return np.round(\n        np.linspace(\n            l_values_linspace[0],\n            l_values_linspace[1],\n            l_values_linspace[2],\n            endpoint=True,\n        ),\n        5,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.list_values_path","title":"<code>list_values_path(l_values_path_list, dic_common_parameters)</code>","text":"<p>Generate a list of path names from an inital path name.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_path_list</code> <code>list</code> <p>List with the initial path names and number of paths.</p> required <code>dic_common_parameters</code> <code>dict</code> <p>Dictionary with the parameters common to the whole study.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of final path values from the initial paths.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def list_values_path(\n    l_values_path_list: list[str], dic_common_parameters: dict[str, Any]\n) -&gt; list[str]:\n    \"\"\"Generate a list of path names from an inital path name.\n\n    Args:\n        l_values_path_list (list): List with the initial path names and number of paths.\n        dic_common_parameters (dict): Dictionary with the parameters common to the whole study.\n\n    Returns:\n        list: List of final path values from the initial paths.\n    \"\"\"\n    # Check that all values in the list are strings\n    if not all(isinstance(value, str) for value in l_values_path_list):\n        raise ValueError(\n            \"All values in the list for the list_values_path function must be strings.\"\n        )\n    n_path_arg = l_values_path_list[1]\n    n_path = find_item_in_dic(dic_common_parameters, n_path_arg)\n    if n_path is None:\n        raise ValueError(f\"Parameter {n_path_arg} is not defined in the scan configuration.\")\n    return [l_values_path_list[0].replace(\"____\", f\"{n:02d}\") for n in range(n_path)]\n</code></pre>"},{"location":"reference/study_da/generate/parameter_space.html#study_da.generate.parameter_space.logspace","title":"<code>logspace(l_values_logspace)</code>","text":"<p>Generate a list of values that are evenly spaced on a log scale.</p> <p>Parameters:</p> Name Type Description Default <code>l_values_logspace</code> <code>list</code> <p>List with the values for the logspace function.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: List of values that are evenly spaced on a log scale.</p> Source code in <code>study_da/generate/parameter_space.py</code> <pre><code>def logspace(l_values_logspace: list) -&gt; np.ndarray:\n    \"\"\"Generate a list of values that are evenly spaced on a log scale.\n\n    Args:\n        l_values_logspace (list): List with the values for the logspace function.\n\n    Returns:\n        np.ndarray: List of values that are evenly spaced on a log scale.\n    \"\"\"\n\n    # Check that all values in the list are floats or integers\n    if not all(isinstance(value, (float, int)) for value in l_values_logspace):\n        raise ValueError(\n            \"All values in the list for the logspace function must be floats or integers.\"\n        )\n    return np.round(\n        np.logspace(\n            l_values_logspace[0],\n            l_values_logspace[1],\n            l_values_logspace[2],\n            endpoint=True,\n        ),\n        5,\n    )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/index.html","title":"master_classes","text":""},{"location":"reference/study_da/generate/master_classes/mad_collider.html","title":"mad_collider","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider","title":"<code>MadCollider</code>","text":"<p>MadCollider class is responsible for setting up and managing the collider environment using MAD-X and xsuite.</p> <p>Attributes:</p> Name Type Description <code>sanity_checks</code> <code>bool</code> <p>Flag to enable or disable sanity checks.</p> <code>links</code> <code>str</code> <p>Path to the links configuration.</p> <code>beam_config</code> <code>dict</code> <p>Configuration for the beam.</p> <code>optics</code> <code>str</code> <p>Path to the optics file.</p> <code>enable_imperfections</code> <code>bool</code> <p>Flag to enable or disable imperfections.</p> <code>enable_knob_synthesis</code> <code>bool</code> <p>Flag to enable or disable knob synthesis.</p> <code>rename_coupling_knobs</code> <code>bool</code> <p>Flag to enable or disable renaming of coupling knobs.</p> <code>pars_for_imperfections</code> <code>dict</code> <p>Parameters for imperfections.</p> <code>ver_lhc_run</code> <code>float | None</code> <p>Version of LHC run.</p> <code>ver_hllhc_optics</code> <code>float | None</code> <p>Version of HL-LHC optics.</p> <code>ions</code> <code>bool</code> <p>Flag to indicate if ions are used.</p> <code>phasing</code> <code>dict</code> <p>Phasing configuration.</p> <code>path_collider_file_for_configuration_as_output</code> <code>str</code> <p>Path to save the collider.</p> <code>compress</code> <code>bool</code> <p>Flag to enable or disable compression of collider file.</p> <p>Methods:</p> Name Description <code>ost</code> <p>Property to get the appropriate optics specific tools.</p> <code>prepare_mad_collider</code> <p>Prepares the MAD-X collider environment.</p> <code>build_collider</code> <p>Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.</p> <code>activate_RF_and_twiss</code> <p>xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.</p> <code>check_xsuite_lattices</code> <p>xt.Line) -&gt; None: Checks the xsuite lattices.</p> <code>write_collider_to_disk</code> <p>xt.Multiline) -&gt; None: Writes the collider to disk and optionally compresses it.</p> <code>clean_temporary_files</code> <p>Cleans up temporary files created during the process.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>class MadCollider:\n    \"\"\"\n    MadCollider class is responsible for setting up and managing the collider environment using\n    MAD-X and xsuite.\n\n    Attributes:\n        sanity_checks (bool): Flag to enable or disable sanity checks.\n        links (str): Path to the links configuration.\n        beam_config (dict): Configuration for the beam.\n        optics (str): Path to the optics file.\n        enable_imperfections (bool): Flag to enable or disable imperfections.\n        enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n        rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling knobs.\n        pars_for_imperfections (dict): Parameters for imperfections.\n        ver_lhc_run (float | None): Version of LHC run.\n        ver_hllhc_optics (float | None): Version of HL-LHC optics.\n        ions (bool): Flag to indicate if ions are used.\n        phasing (dict): Phasing configuration.\n        path_collider_file_for_configuration_as_output (str): Path to save the collider.\n        compress (bool): Flag to enable or disable compression of collider file.\n\n    Methods:\n        ost: Property to get the appropriate optics specific tools.\n        prepare_mad_collider() -&gt; tuple[Madx, Madx]: Prepares the MAD-X collider environment.\n        build_collider(mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline: Builds the xsuite collider.\n        activate_RF_and_twiss(collider: xt.Multiline) -&gt; None: Activates RF and performs twiss analysis.\n        check_xsuite_lattices(line: xt.Line) -&gt; None: Checks the xsuite lattices.\n        write_collider_to_disk(collider: xt.Multiline) -&gt; None: Writes the collider to disk and\n            optionally compresses it.\n        clean_temporary_files() -&gt; None: Cleans up temporary files created during the process.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initializes the MadCollider class with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the following keys:\n                - sanity_checks (bool): Flag to enable or disable sanity checks.\n                - links (str): Path to the links configuration.\n                - beam_config (dict): Configuration for the beam.\n                - optics_file (str): Path to the optics file.\n                - enable_imperfections (bool): Flag to enable or disable imperfections.\n                - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n                - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                    knobs.\n                - pars_for_imperfections (dict): Parameters for imperfections.\n                - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n                - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n                - ions (bool): Flag to indicate if ions are used.\n                - phasing (dict): Configuration for phasing.\n                - path_collider_file_for_configuration_as_output (str): Path to the collider.\n                - compress (bool): Flag to enable or disable compression.\n        \"\"\"\n        # Configuration variables\n        self.sanity_checks: bool = configuration[\"sanity_checks\"]\n        self.links: str = configuration[\"links\"]\n        self.beam_config: dict = configuration[\"beam_config\"]\n        self.optics: str = configuration[\"optics_file\"]\n        self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n        self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n        self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n        self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n        self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n        self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n        self.ions: bool = configuration[\"ions\"]\n        self.phasing: dict = configuration[\"phasing\"]\n\n        # Optics specific tools\n        self._ost = None\n\n        # Path to disk and compression\n        self.path_collider_file_for_configuration_as_output = configuration[\n            \"path_collider_file_for_configuration_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def ost(self) -&gt; Any:\n        \"\"\"\n        Determines and returns the appropriate optics-specific tools (OST) based on the\n        version of HLLHC optics or LHC run configuration.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics-specific tools are available for the given configuration.\n\n        Returns:\n            Any: The appropriate OST module based on the configuration.\n        \"\"\"\n        if self._ost is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._ost = ost_hllhc16\n                    case 1.3:\n                        self._ost = ost_hllhc13\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._ost = ost_runIII_ions if self.ions else ost_runIII\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._ost\n\n    def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n        # sourcery skip: extract-duplicate-method\n        \"\"\"\n        Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n        This method performs the following steps:\n        1. Creates the MAD-X environment using the provided links.\n        2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n        3. Builds the sequences for both beams using the provided beam configuration.\n        4. Applies the specified optics to the beam 1/2 sequence.\n        5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n            the MAD-X lattices.\n        6. Applies the specified optics to the beam 4 sequence.\n        7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n            the MAD-X lattices.\n\n        Returns:\n            tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n        \"\"\"\n        # Make mad environment\n        xm.make_mad_environment(links=self.links)\n\n        # Start mad\n        mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n        mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n        # Build sequences\n        self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n        self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n        # Apply optics (only for b1b2, b4 will be generated from b1b2)\n        self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n        if self.sanity_checks:\n            mad_b1b2.use(sequence=\"lhcb1\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n            mad_b1b2.use(sequence=\"lhcb2\")\n            mad_b1b2.twiss()\n            self.ost.check_madx_lattices(mad_b1b2)\n\n        # Apply optics (only for b4, just for check)\n        self.ost.apply_optics(mad_b4, optics_file=self.optics)\n        if self.sanity_checks:\n            mad_b4.use(sequence=\"lhcb2\")\n            mad_b4.twiss()\n            # ! Investigate why this is failing for run III\n            try:\n                self.ost.check_madx_lattices(mad_b4)\n            except AssertionError:\n                logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n        return mad_b1b2, mad_b4\n\n    def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n        \"\"\"\n        Build an xsuite collider using provided MAD-X sequences and configuration.\n\n        Parameters:\n        mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n        mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n        Returns:\n        xt.Multiline: Constructed xsuite collider.\n\n        Notes:\n        - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n        - Builds the xsuite collider with the specified sequences and configuration.\n        - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n        \"\"\"\n        # Ensure proper types to avoid assert errors\n        if self.ver_lhc_run is not None:\n            self.ver_lhc_run = float(self.ver_lhc_run)\n        if self.ver_hllhc_optics is not None:\n            self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n        # Build xsuite collider\n        collider = xlhc.build_xsuite_collider(\n            sequence_b1=mad_b1b2.sequence.lhcb1,\n            sequence_b2=mad_b1b2.sequence.lhcb2,\n            sequence_b4=mad_b4.sequence.lhcb2,\n            beam_config=self.beam_config,\n            enable_imperfections=self.enable_imperfections,\n            enable_knob_synthesis=self.enable_knob_synthesis,\n            rename_coupling_knobs=self.rename_coupling_knobs,\n            pars_for_imperfections=self.pars_for_imperfections,\n            ver_lhc_run=self.ver_lhc_run,\n            ver_hllhc_optics=self.ver_hllhc_optics,\n        )\n        collider.build_trackers()\n\n        if self.sanity_checks:\n            collider[\"lhcb1\"].twiss(method=\"4d\")\n            collider[\"lhcb2\"].twiss(method=\"4d\")\n\n        return collider\n\n    def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Activates RF and Twiss parameters for the given collider.\n\n        This method sets the RF knobs for the collider using the values specified\n        in the `phasing` attribute. It also performs sanity checks on the collider\n        lattices if the `sanity_checks` attribute is set to True.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        # Define a RF knobs\n        collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n        collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n        collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n        if self.sanity_checks:\n            for my_line in [\"lhcb1\", \"lhcb2\"]:\n                self.check_xsuite_lattices(collider[my_line])\n\n    def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n        \"\"\"\n        Check the Twiss parameters and tune values for a given xsuite Line object.\n\n        This method computes the Twiss parameters for the provided `line` using the\n        6-dimensional method with a specified matrix stability tolerance. It then\n        prints the Twiss results at all interaction points (IPs) and the horizontal\n        (Qx) and vertical (Qy) tune values.\n\n        Args:\n            line (xt.Line): The xsuite Line object for which to compute and display\n                            the Twiss parameters and tune values.\n\n        Returns:\n            None\n        \"\"\"\n        tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n        print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n        print(tw.rows[\"ip.*\"])\n        # print qx and qy\n        print(f\"--- Now displaying Qx and Qy for line {line}---\")\n        print(tw.qx, tw.qy)\n\n    def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n        file.\n\n        Args:\n            collider (xt.Multiline): The collider object to be saved.\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an issue creating the directory or writing the file.\n\n        Notes:\n            - The method ensures that the directory specified in\n                `self.path_collider_file_for_configuration_as_output` exists.\n            - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n                storage usage.\n        \"\"\"\n        # Save collider to json, creating the folder if it does not exist\n        if \"/\" in self.path_collider_file_for_configuration_as_output:\n            os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n        collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_configuration_as_output)\n\n    @staticmethod\n    def clean_temporary_files() -&gt; None:\n        \"\"\"\n        Remove all the temporary files created in the process of building the collider.\n\n        This function deletes the following files and directories:\n        - \"mad_collider.log\"\n        - \"mad_b4.log\"\n        - \"temp\" directory\n        - \"errors\"\n        - \"acc-models-lhc\"\n        \"\"\"\n        # Remove all the temporaty files created in the process of building collider\n        os.remove(\"mad_collider.log\")\n        os.remove(\"mad_b4.log\")\n        shutil.rmtree(\"temp\")\n        os.unlink(\"errors\")\n        os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.ost","title":"<code>ost: Any</code>  <code>property</code>","text":"<p>Determines and returns the appropriate optics-specific tools (OST) based on the version of HLLHC optics or LHC run configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics-specific tools are available for the given configuration.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The appropriate OST module based on the configuration.</p>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initializes the MadCollider class with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the following keys: - sanity_checks (bool): Flag to enable or disable sanity checks. - links (str): Path to the links configuration. - beam_config (dict): Configuration for the beam. - optics_file (str): Path to the optics file. - enable_imperfections (bool): Flag to enable or disable imperfections. - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis. - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling     knobs. - pars_for_imperfections (dict): Parameters for imperfections. - ver_lhc_run (float | None): Version of the LHC run, if applicable. - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable. - ions (bool): Flag to indicate if ions are used. - phasing (dict): Configuration for phasing. - path_collider_file_for_configuration_as_output (str): Path to the collider. - compress (bool): Flag to enable or disable compression.</p> required Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initializes the MadCollider class with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the following keys:\n            - sanity_checks (bool): Flag to enable or disable sanity checks.\n            - links (str): Path to the links configuration.\n            - beam_config (dict): Configuration for the beam.\n            - optics_file (str): Path to the optics file.\n            - enable_imperfections (bool): Flag to enable or disable imperfections.\n            - enable_knob_synthesis (bool): Flag to enable or disable knob synthesis.\n            - rename_coupling_knobs (bool): Flag to enable or disable renaming of coupling\n                knobs.\n            - pars_for_imperfections (dict): Parameters for imperfections.\n            - ver_lhc_run (float | None): Version of the LHC run, if applicable.\n            - ver_hllhc_optics (float | None): Version of the HL-LHC optics, if applicable.\n            - ions (bool): Flag to indicate if ions are used.\n            - phasing (dict): Configuration for phasing.\n            - path_collider_file_for_configuration_as_output (str): Path to the collider.\n            - compress (bool): Flag to enable or disable compression.\n    \"\"\"\n    # Configuration variables\n    self.sanity_checks: bool = configuration[\"sanity_checks\"]\n    self.links: str = configuration[\"links\"]\n    self.beam_config: dict = configuration[\"beam_config\"]\n    self.optics: str = configuration[\"optics_file\"]\n    self.enable_imperfections: bool = configuration[\"enable_imperfections\"]\n    self.enable_knob_synthesis: bool = configuration[\"enable_knob_synthesis\"]\n    self.rename_coupling_knobs: bool = configuration[\"rename_coupling_knobs\"]\n    self.pars_for_imperfections: dict = configuration[\"pars_for_imperfections\"]\n    self.ver_lhc_run: float | None = configuration[\"ver_lhc_run\"]\n    self.ver_hllhc_optics: float | None = configuration[\"ver_hllhc_optics\"]\n    self.ions: bool = configuration[\"ions\"]\n    self.phasing: dict = configuration[\"phasing\"]\n\n    # Optics specific tools\n    self._ost = None\n\n    # Path to disk and compression\n    self.path_collider_file_for_configuration_as_output = configuration[\n        \"path_collider_file_for_configuration_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.activate_RF_and_twiss","title":"<code>activate_RF_and_twiss(collider)</code>","text":"<p>Activates RF and Twiss parameters for the given collider.</p> <p>This method sets the RF knobs for the collider using the values specified in the <code>phasing</code> attribute. It also performs sanity checks on the collider lattices if the <code>sanity_checks</code> attribute is set to True.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def activate_RF_and_twiss(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Activates RF and Twiss parameters for the given collider.\n\n    This method sets the RF knobs for the collider using the values specified\n    in the `phasing` attribute. It also performs sanity checks on the collider\n    lattices if the `sanity_checks` attribute is set to True.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    # Define a RF knobs\n    collider.vars[\"vrf400\"] = self.phasing[\"vrf400\"]\n    collider.vars[\"lagrf400.b1\"] = self.phasing[\"lagrf400.b1\"]\n    collider.vars[\"lagrf400.b2\"] = self.phasing[\"lagrf400.b2\"]\n\n    if self.sanity_checks:\n        for my_line in [\"lhcb1\", \"lhcb2\"]:\n            self.check_xsuite_lattices(collider[my_line])\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.build_collider","title":"<code>build_collider(mad_b1b2, mad_b4)</code>","text":"<p>Build an xsuite collider using provided MAD-X sequences and configuration.</p> <p>Parameters: mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2. mad_b4 (Madx): MAD-X instance containing sequence for beam 4.</p> <p>Returns: xt.Multiline: Constructed xsuite collider.</p> <p>Notes: - Converts <code>ver_lhc_run</code> and <code>ver_hllhc_optics</code> to float if they are not None. - Builds the xsuite collider with the specified sequences and configuration. - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def build_collider(self, mad_b1b2: Madx, mad_b4: Madx) -&gt; xt.Multiline:\n    \"\"\"\n    Build an xsuite collider using provided MAD-X sequences and configuration.\n\n    Parameters:\n    mad_b1b2 (Madx): MAD-X instance containing sequences for beam 1 and beam 2.\n    mad_b4 (Madx): MAD-X instance containing sequence for beam 4.\n\n    Returns:\n    xt.Multiline: Constructed xsuite collider.\n\n    Notes:\n    - Converts `ver_lhc_run` and `ver_hllhc_optics` to float if they are not None.\n    - Builds the xsuite collider with the specified sequences and configuration.\n    - Optionally performs sanity checks by computing Twiss parameters for beam 1 and beam 2.\n    \"\"\"\n    # Ensure proper types to avoid assert errors\n    if self.ver_lhc_run is not None:\n        self.ver_lhc_run = float(self.ver_lhc_run)\n    if self.ver_hllhc_optics is not None:\n        self.ver_hllhc_optics = float(self.ver_hllhc_optics)\n\n    # Build xsuite collider\n    collider = xlhc.build_xsuite_collider(\n        sequence_b1=mad_b1b2.sequence.lhcb1,\n        sequence_b2=mad_b1b2.sequence.lhcb2,\n        sequence_b4=mad_b4.sequence.lhcb2,\n        beam_config=self.beam_config,\n        enable_imperfections=self.enable_imperfections,\n        enable_knob_synthesis=self.enable_knob_synthesis,\n        rename_coupling_knobs=self.rename_coupling_knobs,\n        pars_for_imperfections=self.pars_for_imperfections,\n        ver_lhc_run=self.ver_lhc_run,\n        ver_hllhc_optics=self.ver_hllhc_optics,\n    )\n    collider.build_trackers()\n\n    if self.sanity_checks:\n        collider[\"lhcb1\"].twiss(method=\"4d\")\n        collider[\"lhcb2\"].twiss(method=\"4d\")\n\n    return collider\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.check_xsuite_lattices","title":"<code>check_xsuite_lattices(line)</code>","text":"<p>Check the Twiss parameters and tune values for a given xsuite Line object.</p> <p>This method computes the Twiss parameters for the provided <code>line</code> using the 6-dimensional method with a specified matrix stability tolerance. It then prints the Twiss results at all interaction points (IPs) and the horizontal (Qx) and vertical (Qy) tune values.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>Line</code> <p>The xsuite Line object for which to compute and display             the Twiss parameters and tune values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def check_xsuite_lattices(self, line: xt.Line) -&gt; None:\n    \"\"\"\n    Check the Twiss parameters and tune values for a given xsuite Line object.\n\n    This method computes the Twiss parameters for the provided `line` using the\n    6-dimensional method with a specified matrix stability tolerance. It then\n    prints the Twiss results at all interaction points (IPs) and the horizontal\n    (Qx) and vertical (Qy) tune values.\n\n    Args:\n        line (xt.Line): The xsuite Line object for which to compute and display\n                        the Twiss parameters and tune values.\n\n    Returns:\n        None\n    \"\"\"\n    tw = line.twiss(method=\"6d\", matrix_stability_tol=100)\n    print(f\"--- Now displaying Twiss result at all IPS for line {line}---\")\n    print(tw.rows[\"ip.*\"])\n    # print qx and qy\n    print(f\"--- Now displaying Qx and Qy for line {line}---\")\n    print(tw.qx, tw.qy)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.clean_temporary_files","title":"<code>clean_temporary_files()</code>  <code>staticmethod</code>","text":"<p>Remove all the temporary files created in the process of building the collider.</p> <p>This function deletes the following files and directories: - \"mad_collider.log\" - \"mad_b4.log\" - \"temp\" directory - \"errors\" - \"acc-models-lhc\"</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>@staticmethod\ndef clean_temporary_files() -&gt; None:\n    \"\"\"\n    Remove all the temporary files created in the process of building the collider.\n\n    This function deletes the following files and directories:\n    - \"mad_collider.log\"\n    - \"mad_b4.log\"\n    - \"temp\" directory\n    - \"errors\"\n    - \"acc-models-lhc\"\n    \"\"\"\n    # Remove all the temporaty files created in the process of building collider\n    os.remove(\"mad_collider.log\")\n    os.remove(\"mad_b4.log\")\n    shutil.rmtree(\"temp\")\n    os.unlink(\"errors\")\n    os.unlink(\"acc-models-lhc\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.prepare_mad_collider","title":"<code>prepare_mad_collider()</code>","text":"<p>Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.</p> <p>This method performs the following steps: 1. Creates the MAD-X environment using the provided links. 2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs. 3. Builds the sequences for both beams using the provided beam configuration. 4. Applies the specified optics to the beam 1/2 sequence. 5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking     the MAD-X lattices. 6. Applies the specified optics to the beam 4 sequence. 7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking     the MAD-X lattices.</p> <p>Returns:</p> Type Description <code>tuple[Madx, Madx]</code> <p>tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.</p> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def prepare_mad_collider(self) -&gt; tuple[Madx, Madx]:\n    # sourcery skip: extract-duplicate-method\n    \"\"\"\n    Prepares the MAD-X collider environment and sequences for beam 1/2 and beam 4.\n\n    This method performs the following steps:\n    1. Creates the MAD-X environment using the provided links.\n    2. Initializes MAD-X instances for beam 1/2 and beam 4 with respective command logs.\n    3. Builds the sequences for both beams using the provided beam configuration.\n    4. Applies the specified optics to the beam 1/2 sequence.\n    5. Optionally performs sanity checks on the beam 1/2 sequence by running TWISS and checking\n        the MAD-X lattices.\n    6. Applies the specified optics to the beam 4 sequence.\n    7. Optionally performs sanity checks on the beam 4 sequence by running TWISS and checking\n        the MAD-X lattices.\n\n    Returns:\n        tuple[Madx, Madx]: A tuple containing the MAD-X instances for beam 1/2 and beam 4.\n    \"\"\"\n    # Make mad environment\n    xm.make_mad_environment(links=self.links)\n\n    # Start mad\n    mad_b1b2 = Madx(command_log=\"mad_collider.log\")\n    mad_b4 = Madx(command_log=\"mad_b4.log\")\n\n    # Build sequences\n    self.ost.build_sequence(mad_b1b2, mylhcbeam=1, beam_config=self.beam_config)\n    self.ost.build_sequence(mad_b4, mylhcbeam=4, beam_config=self.beam_config)\n\n    # Apply optics (only for b1b2, b4 will be generated from b1b2)\n    self.ost.apply_optics(mad_b1b2, optics_file=self.optics)\n\n    if self.sanity_checks:\n        mad_b1b2.use(sequence=\"lhcb1\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n        mad_b1b2.use(sequence=\"lhcb2\")\n        mad_b1b2.twiss()\n        self.ost.check_madx_lattices(mad_b1b2)\n\n    # Apply optics (only for b4, just for check)\n    self.ost.apply_optics(mad_b4, optics_file=self.optics)\n    if self.sanity_checks:\n        mad_b4.use(sequence=\"lhcb2\")\n        mad_b4.twiss()\n        # ! Investigate why this is failing for run III\n        try:\n            self.ost.check_madx_lattices(mad_b4)\n        except AssertionError:\n            logging.warning(\"Some sanity checks have failed during the madx lattice check\")\n\n    return mad_b1b2, mad_b4\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/mad_collider.html#study_da.generate.master_classes.mad_collider.MadCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider)</code>","text":"<p>Writes the collider object to disk in JSON format and optionally compresses it into a ZIP file.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to be saved.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an issue creating the directory or writing the file.</p> Notes <ul> <li>The method ensures that the directory specified in     <code>self.path_collider_file_for_configuration_as_output</code> exists.</li> <li>If <code>self.compress</code> is True, the JSON file is compressed into a ZIP file to reduce     storage usage.</li> </ul> Source code in <code>study_da/generate/master_classes/mad_collider.py</code> <pre><code>def write_collider_to_disk(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format and optionally compresses it into a ZIP\n    file.\n\n    Args:\n        collider (xt.Multiline): The collider object to be saved.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an issue creating the directory or writing the file.\n\n    Notes:\n        - The method ensures that the directory specified in\n            `self.path_collider_file_for_configuration_as_output` exists.\n        - If `self.compress` is True, the JSON file is compressed into a ZIP file to reduce\n            storage usage.\n    \"\"\"\n    # Save collider to json, creating the folder if it does not exist\n    if \"/\" in self.path_collider_file_for_configuration_as_output:\n        os.makedirs(self.path_collider_file_for_configuration_as_output, exist_ok=True)\n    collider.to_json(self.path_collider_file_for_configuration_as_output)\n\n    # Compress the collider file to zip to ease the load on afs\n    if self.compress:\n        compress_and_write(self.path_collider_file_for_configuration_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html","title":"particles_distribution","text":"<p>This class is used to define and write to disk the particles distribution.</p>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution","title":"<code>ParticlesDistribution</code>","text":"<p>ParticlesDistribution class to generate and manage particle distributions.</p> <p>Attributes:</p> Name Type Description <code>r_min</code> <code>int</code> <p>Minimum radial distance.</p> <code>r_max</code> <code>int</code> <p>Maximum radial distance.</p> <code>n_r</code> <code>int</code> <p>Number of radial points.</p> <code>n_angles</code> <code>int</code> <p>Number of angular points.</p> <code>n_split</code> <code>int</code> <p>Number of splits for parallelization.</p> <code>path_distribution_folder_output</code> <code>str</code> <p>Path to the folder where distributions will be saved.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>dict): Initializes the ParticlesDistribution with the given configuration.</p> <code>get_radial_list</code> <p>float | None = None, upper_crop: float | None = None) -&gt; np.ndarray: Generates a list of radial distances, optionally cropped.</p> <code>get_angular_list</code> <p>Generates a list of angular values.</p> <code>return_distribution_as_list</code> <p>bool = True, lower_crop: float | None = None, upper_crop: float | None) -&gt; list[np.ndarray]: Returns the particle distribution as a list of numpy arrays, optionally split for parallelization.</p> <code>write_particle_distribution_to_disk</code> <p>list[np.ndarray]) -&gt; list[str]: Writes the particle distribution to disk in Parquet format and returns the list of file paths.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>class ParticlesDistribution:\n    \"\"\"\n    ParticlesDistribution class to generate and manage particle distributions.\n\n    Attributes:\n        r_min (int): Minimum radial distance.\n        r_max (int): Maximum radial distance.\n        n_r (int): Number of radial points.\n        n_angles (int): Number of angular points.\n        n_split (int): Number of splits for parallelization.\n        path_distribution_folder_output (str): Path to the folder where distributions will be saved.\n\n    Methods:\n        __init__(configuration: dict):\n            Initializes the ParticlesDistribution with the given configuration.\n\n        get_radial_list(lower_crop: float | None = None, upper_crop: float | None = None)\n            -&gt; np.ndarray:\n            Generates a list of radial distances, optionally cropped.\n\n        get_angular_list() -&gt; np.ndarray:\n            Generates a list of angular values.\n\n        return_distribution_as_list(split: bool = True, lower_crop: float | None = None,\n            upper_crop: float | None) -&gt; list[np.ndarray]:\n            Returns the particle distribution as a list of numpy arrays, optionally split for\n            parallelization.\n\n        write_particle_distribution_to_disk(ll_particles: list[np.ndarray]) -&gt; list[str]:\n            Writes the particle distribution to disk in Parquet format and returns the list of file\n            paths.\n    \"\"\"\n\n    def __init__(self, configuration: dict):\n        \"\"\"\n        Initialize the particle distribution with the given configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                - r_min (int): Minimum radius value.\n                - r_max (int): Maximum radius value.\n                - n_r (int): Number of radius points.\n                - n_angles (int): Number of angle points.\n                - n_split (int): Number of splits for parallelization.\n                - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                    saved.\n        \"\"\"\n        # Variables used to define the distribution\n        self.r_min: int = configuration[\"r_min\"]\n        self.r_max: int = configuration[\"r_max\"]\n        self.n_r: int = configuration[\"n_r\"]\n        self.n_angles: int = configuration[\"n_angles\"]\n\n        # Variables to split the distribution for parallelization\n        self.n_split: int = configuration[\"n_split\"]\n\n        # Variable to write the distribution to disk\n        self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n\n    def get_radial_list(\n        self, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of radial distances within specified bounds.\n\n        Args:\n            lower_crop (float | None): The lower bound to crop the radial distances.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound to crop the radial distances.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            np.ndarray: An array of radial distances within the specified bounds.\n        \"\"\"\n        radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n        if upper_crop:\n            radial_list = radial_list[radial_list &lt;= 7.5]\n        if lower_crop:\n            radial_list = radial_list[radial_list &gt;= 2.5]\n        return radial_list\n\n    def get_angular_list(self) -&gt; np.ndarray:\n        \"\"\"\n        Generate a list of angular values.\n\n        This method creates a list of angular values ranging from 0 to 90 degrees,\n        excluding the first and last values. The number of angles generated is\n        determined by the instance variable `self.n_angles`.\n\n        Returns:\n            numpy.ndarray: An array of angular values.\n        \"\"\"\n        return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n\n    def return_distribution_as_list(\n        self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n    ) -&gt; list[np.ndarray]:\n        \"\"\"\n        Returns the particle distribution as a list of numpy arrays.\n\n        This method generates a particle distribution by creating a Cartesian product\n        of radial and angular lists. The resulting distribution can be optionally split\n        into multiple parts for parallel computation.\n\n        Args:\n            split (bool): If True, the distribution is split into multiple parts.\n                Defaults to True.\n            lower_crop (float | None): The lower bound for cropping the radial list.\n                If None, no lower cropping is applied. Defaults to None.\n            upper_crop (float | None): The upper bound for cropping the radial list.\n                If None, no upper cropping is applied. Defaults to None.\n\n        Returns:\n            list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n                If `split` is True, the list contains multiple arrays for parallel computation.\n                Otherwise, the list contains a single array.\n        \"\"\"\n        # Get radial list and angular list\n        radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n        angular_list = self.get_angular_list()\n\n        # Define particle distribution as a cartesian product of the radial and angular lists\n        l_particles = np.array(\n            [\n                (particle_id, ii[1], ii[0])\n                for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n            ]\n        )\n\n        # Potentially split the distribution to parallelize the computation\n        if split:\n            return list(np.array_split(l_particles, self.n_split))\n\n        return [l_particles]\n\n    def write_particle_distribution_to_disk(\n        self, ll_particles: list[list[np.ndarray]]\n    ) -&gt; list[str]:\n        \"\"\"\n        Writes a list of particle distributions to disk in Parquet format.\n\n        Args:\n            ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n                where each distribution is a list containing particle data.\n\n        Returns:\n            list[str]: A list of file paths where the particle distributions\n            have been saved.\n\n        The method creates a directory specified by `self.path_distribution_folder_output`\n        if it does not already exist. Each particle distribution is saved as a\n        Parquet file in this directory. The files are named sequentially using\n        a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n        \"\"\"\n        # Define folder to store the distributions\n        os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n        # Write the distribution to disk\n        l_path_files = []\n        for idx_chunk, l_particles in enumerate(ll_particles):\n            path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n            pd.DataFrame(\n                l_particles,\n                columns=[\n                    \"particle_id\",\n                    \"normalized amplitude in xy-plane\",\n                    \"angle in xy-plane [deg]\",\n                ],\n            ).to_parquet(path_file)\n            l_path_files.append(path_file)\n\n        return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.__init__","title":"<code>__init__(configuration)</code>","text":"<p>Initialize the particle distribution with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. - r_min (int): Minimum radius value. - r_max (int): Maximum radius value. - n_r (int): Number of radius points. - n_angles (int): Number of angle points. - n_split (int): Number of splits for parallelization. - path_distribution_folder_output (str): Path to the folder where the distribution will be     saved.</p> required Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def __init__(self, configuration: dict):\n    \"\"\"\n    Initialize the particle distribution with the given configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            - r_min (int): Minimum radius value.\n            - r_max (int): Maximum radius value.\n            - n_r (int): Number of radius points.\n            - n_angles (int): Number of angle points.\n            - n_split (int): Number of splits for parallelization.\n            - path_distribution_folder_output (str): Path to the folder where the distribution will be\n                saved.\n    \"\"\"\n    # Variables used to define the distribution\n    self.r_min: int = configuration[\"r_min\"]\n    self.r_max: int = configuration[\"r_max\"]\n    self.n_r: int = configuration[\"n_r\"]\n    self.n_angles: int = configuration[\"n_angles\"]\n\n    # Variables to split the distribution for parallelization\n    self.n_split: int = configuration[\"n_split\"]\n\n    # Variable to write the distribution to disk\n    self.path_distribution_folder_output: str = configuration[\"path_distribution_folder_output\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.get_angular_list","title":"<code>get_angular_list()</code>","text":"<p>Generate a list of angular values.</p> <p>This method creates a list of angular values ranging from 0 to 90 degrees, excluding the first and last values. The number of angles generated is determined by the instance variable <code>self.n_angles</code>.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: An array of angular values.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_angular_list(self) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of angular values.\n\n    This method creates a list of angular values ranging from 0 to 90 degrees,\n    excluding the first and last values. The number of angles generated is\n    determined by the instance variable `self.n_angles`.\n\n    Returns:\n        numpy.ndarray: An array of angular values.\n    \"\"\"\n    return np.linspace(0, 90, self.n_angles + 2)[1:-1]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.get_radial_list","title":"<code>get_radial_list(lower_crop=None, upper_crop=None)</code>","text":"<p>Generate a list of radial distances within specified bounds.</p> <p>Parameters:</p> Name Type Description Default <code>lower_crop</code> <code>float | None</code> <p>The lower bound to crop the radial distances. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound to crop the radial distances. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of radial distances within the specified bounds.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def get_radial_list(\n    self, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a list of radial distances within specified bounds.\n\n    Args:\n        lower_crop (float | None): The lower bound to crop the radial distances.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound to crop the radial distances.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        np.ndarray: An array of radial distances within the specified bounds.\n    \"\"\"\n    radial_list = np.linspace(self.r_min, self.r_max, self.n_r, endpoint=False)\n    if upper_crop:\n        radial_list = radial_list[radial_list &lt;= 7.5]\n    if lower_crop:\n        radial_list = radial_list[radial_list &gt;= 2.5]\n    return radial_list\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.return_distribution_as_list","title":"<code>return_distribution_as_list(split=True, lower_crop=None, upper_crop=None)</code>","text":"<p>Returns the particle distribution as a list of numpy arrays.</p> <p>This method generates a particle distribution by creating a Cartesian product of radial and angular lists. The resulting distribution can be optionally split into multiple parts for parallel computation.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>bool</code> <p>If True, the distribution is split into multiple parts. Defaults to True.</p> <code>True</code> <code>lower_crop</code> <code>float | None</code> <p>The lower bound for cropping the radial list. If None, no lower cropping is applied. Defaults to None.</p> <code>None</code> <code>upper_crop</code> <code>float | None</code> <p>The upper bound for cropping the radial list. If None, no upper cropping is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ndarray]</code> <p>list[np.ndarray]: A list of numpy arrays representing the particle distribution. If <code>split</code> is True, the list contains multiple arrays for parallel computation. Otherwise, the list contains a single array.</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def return_distribution_as_list(\n    self, split: bool = True, lower_crop: float | None = None, upper_crop: float | None = None\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Returns the particle distribution as a list of numpy arrays.\n\n    This method generates a particle distribution by creating a Cartesian product\n    of radial and angular lists. The resulting distribution can be optionally split\n    into multiple parts for parallel computation.\n\n    Args:\n        split (bool): If True, the distribution is split into multiple parts.\n            Defaults to True.\n        lower_crop (float | None): The lower bound for cropping the radial list.\n            If None, no lower cropping is applied. Defaults to None.\n        upper_crop (float | None): The upper bound for cropping the radial list.\n            If None, no upper cropping is applied. Defaults to None.\n\n    Returns:\n        list[np.ndarray]: A list of numpy arrays representing the particle distribution.\n            If `split` is True, the list contains multiple arrays for parallel computation.\n            Otherwise, the list contains a single array.\n    \"\"\"\n    # Get radial list and angular list\n    radial_list = self.get_radial_list(lower_crop=lower_crop, upper_crop=upper_crop)\n    angular_list = self.get_angular_list()\n\n    # Define particle distribution as a cartesian product of the radial and angular lists\n    l_particles = np.array(\n        [\n            (particle_id, ii[1], ii[0])\n            for particle_id, ii in enumerate(itertools.product(angular_list, radial_list))\n        ]\n    )\n\n    # Potentially split the distribution to parallelize the computation\n    if split:\n        return list(np.array_split(l_particles, self.n_split))\n\n    return [l_particles]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/particles_distribution.html#study_da.generate.master_classes.particles_distribution.ParticlesDistribution.write_particle_distribution_to_disk","title":"<code>write_particle_distribution_to_disk(ll_particles)</code>","text":"<p>Writes a list of particle distributions to disk in Parquet format.</p> <p>Parameters:</p> Name Type Description Default <code>ll_particles</code> <code>list[list[ndarray]]</code> <p>A list of particle distributions, where each distribution is a list containing particle data.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of file paths where the particle distributions</p> <code>list[str]</code> <p>have been saved.</p> <p>The method creates a directory specified by <code>self.path_distribution_folder_output</code> if it does not already exist. Each particle distribution is saved as a Parquet file in this directory. The files are named sequentially using a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).</p> Source code in <code>study_da/generate/master_classes/particles_distribution.py</code> <pre><code>def write_particle_distribution_to_disk(\n    self, ll_particles: list[list[np.ndarray]]\n) -&gt; list[str]:\n    \"\"\"\n    Writes a list of particle distributions to disk in Parquet format.\n\n    Args:\n        ll_particles (list[list[np.ndarray]]): A list of particle distributions,\n            where each distribution is a list containing particle data.\n\n    Returns:\n        list[str]: A list of file paths where the particle distributions\n        have been saved.\n\n    The method creates a directory specified by `self.path_distribution_folder_output`\n    if it does not already exist. Each particle distribution is saved as a\n    Parquet file in this directory. The files are named sequentially using\n    a zero-padded index (e.g., '00.parquet', '01.parquet', etc.).\n    \"\"\"\n    # Define folder to store the distributions\n    os.makedirs(self.path_distribution_folder_output, exist_ok=True)\n\n    # Write the distribution to disk\n    l_path_files = []\n    for idx_chunk, l_particles in enumerate(ll_particles):\n        path_file = f\"{self.path_distribution_folder_output}/{idx_chunk:02}.parquet\"\n        pd.DataFrame(\n            l_particles,\n            columns=[\n                \"particle_id\",\n                \"normalized amplitude in xy-plane\",\n                \"angle in xy-plane [deg]\",\n            ],\n        ).to_parquet(path_file)\n        l_path_files.append(path_file)\n\n    return l_path_files\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html","title":"scheme_utils","text":"<p>This class is used to inspect and compute some properties of the filling scheme.</p>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.get_worst_bunch","title":"<code>get_worst_bunch(filling_scheme_path, number_of_LR_to_consider=26, beam='beam_1')</code>","text":""},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.get_worst_bunch--adapted-from-httpsgithubcompycompletefillingpatternsblob5f28d1a99e9a2ef7cc5c171d0cab6679946309e8fillingpatternsbbfunctionspyl233","title":"Adapted from https://github.com/PyCOMPLETE/FillingPatterns/blob/5f28d1a99e9a2ef7cc5c171d0cab6679946309e8/fillingpatterns/bbFunctions.py#L233","text":"<p>Given a filling scheme, containing two arrays of booleans representing the trains of bunches for the two beams, this function returns the worst bunch for each beam, according to their collision schedule.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <code>number_of_LR_to_consider</code> <code>int</code> <p>Number of long range collisions to consider. Defaults to 26.</p> <code>26</code> <code>beam</code> <code>str</code> <p>Beam for which to compute the worst bunch. Defaults to \"beam_1\".</p> <code>'beam_1'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The worst bunch for the specified beam.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def get_worst_bunch(\n    filling_scheme_path: str, number_of_LR_to_consider: int = 26, beam=\"beam_1\"\n) -&gt; int:\n    \"\"\"\n    # Adapted from https://github.com/PyCOMPLETE/FillingPatterns/blob/5f28d1a99e9a2ef7cc5c171d0cab6679946309e8/fillingpatterns/bbFunctions.py#L233\n    Given a filling scheme, containing two arrays of booleans representing the trains of bunches for\n    the two beams, this function returns the worst bunch for each beam, according to their collision\n    schedule.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n        number_of_LR_to_consider (int): Number of long range collisions to consider. Defaults to 26.\n        beam (str): Beam for which to compute the worst bunch. Defaults to \"beam_1\".\n\n    Returns:\n        int: The worst bunch for the specified beam.\n\n    \"\"\"\n\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\"Only json filling schemes are supported\")\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Get bunches index\n    B1_bunches_index = np.flatnonzero(array_b1)\n    B2_bunches_index = np.flatnonzero(array_b2)\n\n    # Compute the number of long range collisions per bunch\n    l_long_range_per_bunch = _compute_LR_per_bunch(\n        array_b1, array_b2, B1_bunches_index, B2_bunches_index, number_of_LR_to_consider, beam=beam\n    )\n\n    # Get the worst bunch for both beams\n    if beam == \"beam_1\":\n        worst_bunch = B1_bunches_index[np.argmax(l_long_range_per_bunch)]\n    elif beam == \"beam_2\":\n        worst_bunch = B2_bunches_index[np.argmax(l_long_range_per_bunch)]\n    else:\n        raise ValueError(\"beam must be either 'beam_1' or 'beam_2\")\n\n    # Need to explicitly convert to int for json serialization\n    return int(worst_bunch)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.load_and_check_filling_scheme","title":"<code>load_and_check_filling_scheme(filling_scheme_path)</code>","text":"<p>Load and check the filling scheme from a JSON file. Convert the filling scheme to the correct format if needed.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the converted filling scheme file.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def load_and_check_filling_scheme(filling_scheme_path: str) -&gt; str:\n    \"\"\"Load and check the filling scheme from a JSON file. Convert the filling scheme to the correct\n    format if needed.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n\n    Returns:\n        str: Path to the converted filling scheme file.\n    \"\"\"\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\"Filling scheme must be in json format\")\n\n    # Check that the converted filling scheme doesn't already exist\n    filling_scheme_path_converted = filling_scheme_path.replace(\".json\", \"_converted.json\")\n    if os.path.exists(filling_scheme_path_converted):\n        return filling_scheme_path_converted\n\n    with open(filling_scheme_path, \"r\") as fid:\n        d_filling_scheme = json.load(fid)\n\n    if \"beam1\" in d_filling_scheme.keys() and \"beam2\" in d_filling_scheme.keys():\n        # If the filling scheme not already in the correct format, convert\n        if \"schemebeam1\" in d_filling_scheme.keys() or \"schemebeam2\" in d_filling_scheme.keys():\n            d_filling_scheme[\"beam1\"] = d_filling_scheme[\"schemebeam1\"]\n            d_filling_scheme[\"beam2\"] = d_filling_scheme[\"schemebeam2\"]\n            # Delete all the other keys\n            d_filling_scheme = {\n                k: v for k, v in d_filling_scheme.items() if k in [\"beam1\", \"beam2\"]\n            }\n            # Dump the dictionary back to the file\n            with open(filling_scheme_path_converted, \"w\") as fid:\n                json.dump(d_filling_scheme, fid)\n\n            # Else, do nothing\n\n    else:\n        # One can potentially use b1_array, b2_array to scan the bunches later\n        b1_array, b2_array = reformat_filling_scheme_from_lpc(\n            filling_scheme_path, filling_scheme_path_converted\n        )\n        filling_scheme_path = filling_scheme_path_converted\n\n    return filling_scheme_path\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/scheme_utils.html#study_da.generate.master_classes.scheme_utils.reformat_filling_scheme_from_lpc","title":"<code>reformat_filling_scheme_from_lpc(filling_scheme_path, filling_scheme_path_converted)</code>","text":"<p>This function is used to convert the filling scheme from the LPC to the format used in the xtrack library. The filling scheme from the LPC is a list of bunches for each beam, where each bunch is represented by a 1 in the list. The function converts this list to a list of indices of the filled bunches. The function also returns the indices of the filled bunches for each beam.</p> <p>Parameters:</p> Name Type Description Default <code>filling_scheme_path</code> <code>str</code> <p>Path to the filling scheme file.</p> required <code>filling_scheme_path_converted</code> <code>str</code> <p>Path to the converted filling scheme file.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: Indices of the filled bunches for each beam.</p> Source code in <code>study_da/generate/master_classes/scheme_utils.py</code> <pre><code>def reformat_filling_scheme_from_lpc(\n    filling_scheme_path: str, filling_scheme_path_converted: str\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    This function is used to convert the filling scheme from the LPC to the format used in the\n    xtrack library. The filling scheme from the LPC is a list of bunches for each beam, where each\n    bunch is represented by a 1 in the list. The function converts this list to a list of indices\n    of the filled bunches. The function also returns the indices of the filled bunches for each beam.\n\n    Args:\n        filling_scheme_path (str): Path to the filling scheme file.\n        filling_scheme_path_converted (str): Path to the converted filling scheme file.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Indices of the filled bunches for each beam.\n    \"\"\"\n\n    # Load the filling scheme directly if json\n    with open(filling_scheme_path, \"r\") as fid:\n        data = json.load(fid)\n\n    # Take the first fill number\n    fill_number = list(data[\"fills\"].keys())[0]\n\n    # Do the conversion (Matteo's code)\n    B1 = np.zeros(3564)\n    B2 = np.zeros(3564)\n    l_lines = data[\"fills\"][f\"{fill_number}\"][\"csv\"].split(\"\\n\")\n    for idx, line in enumerate(l_lines):\n        # First time one encounters a line with 'Slot' in it, start indexing\n        if \"Slot\" in line:\n            # B1 is initially empty\n            if np.sum(B1) == 0:\n                for line_2 in l_lines[idx + 1 :]:\n                    l_line = line_2.split(\",\")\n                    if len(l_line) &gt; 1:\n                        slot = l_line[1]\n                        B1[int(slot)] = 1\n                    else:\n                        break\n\n            elif np.sum(B2) == 0:\n                for line_2 in l_lines[idx + 1 :]:\n                    l_line = line_2.split(\",\")\n                    if len(l_line) &gt; 1:\n                        slot = l_line[1]\n                        B2[int(slot)] = 1\n                    else:\n                        break\n            else:\n                break\n\n    data_json = {\"beam1\": [int(ii) for ii in B1], \"beam2\": [int(ii) for ii in B2]}\n\n    with open(filling_scheme_path_converted, \"w\") as file_bool:\n        json.dump(data_json, file_bool)\n    return B1, B2\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/utils.html","title":"utils","text":"<p>This module provides utility functions for file compression.</p> <p>Functions:</p> Name Description <code>compress_and_write</code> <p>str) -&gt; str: Compresses a file using ZIP compression and writes it to disk, then removes the original uncompressed file.</p> Imports <p>os: Provides a way of using operating system dependent functionality like reading or writing to     the file system. zipfile: Provides tools to create, read, write, append, and list a ZIP file.</p>"},{"location":"reference/study_da/generate/master_classes/utils.html#study_da.generate.master_classes.utils.compress_and_write","title":"<code>compress_and_write(path_to_file)</code>","text":"<p>Compress a file and write it to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_file</code> <code>str</code> <p>Path to the file to compress.</p> required <p>Returns:     path_to_output (str): Path to the output file.</p> Source code in <code>study_da/generate/master_classes/utils.py</code> <pre><code>def compress_and_write(path_to_file: str) -&gt; str:\n    \"\"\"Compress a file and write it to disk.\n\n    Args:\n        path_to_file (str): Path to the file to compress.\n    Returns:\n        path_to_output (str): Path to the output file.\n\n    \"\"\"\n    with ZipFile(\n        f\"{path_to_file}.zip\",\n        \"w\",\n        ZIP_DEFLATED,\n        compresslevel=9,\n    ) as zipf:\n        zipf.write(path_to_file)\n\n    # Remove the uncompressed file\n    os.remove(path_to_file)\n\n    return f\"{path_to_file}.zip\"\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html","title":"xsuite_collider","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider","title":"<code>XsuiteCollider</code>","text":"<p>XsuiteCollider is a class designed to handle the configuration and manipulation of a collider using the Xsuite library. It provides methods to load, configure, and tune the collider, as well as to perform luminosity leveling and beam-beam interaction setup.</p> <p>Attributes:</p> Name Type Description <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file to load.</p> <code>config_beambeam</code> <code>dict</code> <p>Configuration for beam-beam interactions.</p> <code>config_knobs_and_tuning</code> <code>dict</code> <p>Configuration for knobs and tuning.</p> <code>config_lumi_leveling</code> <code>dict</code> <p>Configuration for luminosity leveling.</p> <code>config_lumi_leveling_ip1_5</code> <code>dict or None</code> <p>Configuration for luminosity leveling at IP1 and IP5.</p> <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> <code>_dict_orbit_correction</code> <code>dict or None</code> <p>Dictionary for orbit correction.</p> <code>_crab</code> <code>bool or None</code> <p>Flag indicating if crab cavities are used.</p> <code>save_output_collider</code> <code>bool</code> <p>Flag indicating if the final collider should be saved.</p> <code>path_collider_file_for_tracking_as_output</code> <code>str</code> <p>Path to save the final collider.</p> <p>Methods:</p> Name Description <code>dict_orbit_correction</code> <p>Property to get the dictionary for orbit correction.</p> <code>load_collider</code> <p>Loads the collider from a file.</p> <code>install_beam_beam_wrapper</code> <p>Installs beam-beam lenses in the collider.</p> <code>set_knobs</code> <p>Sets the knobs for the collider.</p> <code>match_tune_and_chroma</code> <p>Matches the tune and chromaticity of the collider.</p> <code>set_filling_and_bunch_tracked</code> <p>Sets the filling scheme and tracks the bunch.</p> <code>compute_collision_from_scheme</code> <p>Computes the number of collisions from the filling scheme.</p> <code>crab</code> <p>Property to get the crab cavities status.</p> <code>level_all_by_separation</code> <p>Levels all IPs by separation.</p> <code>level_ip1_5_by_bunch_intensity</code> <p>Levels IP1 and IP5 by bunch intensity.</p> <code>level_ip2_8_by_separation</code> <p>Levels IP2 and IP8 by separation.</p> <code>add_linear_coupling</code> <p>Adds linear coupling to the collider.</p> <code>assert_tune_chroma_coupling</code> <p>Asserts the tune, chromaticity, and coupling of the collider.</p> <code>configure_beam_beam</code> <p>Configures the beam-beam interactions.</p> <code>record_final_luminosity</code> <p>Records the final luminosity of the collider.</p> <code>write_collider_to_disk</code> <p>Writes the collider configuration to disk.</p> <code>update_configuration_knob</code> <p>Updates a specific knob in the collider.</p> <code>return_fingerprint</code> <p>Returns a fingerprint of the collider's configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>class XsuiteCollider:\n    \"\"\"\n    XsuiteCollider is a class designed to handle the configuration and manipulation of a collider\n    using the Xsuite library. It provides methods to load, configure, and tune the collider,\n    as well as to perform luminosity leveling and beam-beam interaction setup.\n\n    Attributes:\n        path_collider_file_for_configuration_as_input (str): Path to the collider file to load.\n        config_beambeam (dict): Configuration for beam-beam interactions.\n        config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n        config_lumi_leveling (dict): Configuration for luminosity leveling.\n        config_lumi_leveling_ip1_5 (dict or None): Configuration for luminosity leveling at IP1 and\n            IP5.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n        _dict_orbit_correction (dict or None): Dictionary for orbit correction.\n        _crab (bool or None): Flag indicating if crab cavities are used.\n        save_output_collider (bool): Flag indicating if the final collider should be saved.\n        path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n\n    Methods:\n        dict_orbit_correction: Property to get the dictionary for orbit correction.\n        load_collider: Loads the collider from a file.\n        install_beam_beam_wrapper: Installs beam-beam lenses in the collider.\n        set_knobs: Sets the knobs for the collider.\n        match_tune_and_chroma: Matches the tune and chromaticity of the collider.\n        set_filling_and_bunch_tracked: Sets the filling scheme and tracks the bunch.\n        compute_collision_from_scheme: Computes the number of collisions from the filling scheme.\n        crab: Property to get the crab cavities status.\n        level_all_by_separation: Levels all IPs by separation.\n        level_ip1_5_by_bunch_intensity: Levels IP1 and IP5 by bunch intensity.\n        level_ip2_8_by_separation: Levels IP2 and IP8 by separation.\n        add_linear_coupling: Adds linear coupling to the collider.\n        assert_tune_chroma_coupling: Asserts the tune, chromaticity, and coupling of the collider.\n        configure_beam_beam: Configures the beam-beam interactions.\n        record_final_luminosity: Records the final luminosity of the collider.\n        write_collider_to_disk: Writes the collider configuration to disk.\n        update_configuration_knob: Updates a specific knob in the collider.\n        return_fingerprint: Returns a fingerprint of the collider's configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        configuration: dict,\n        path_collider_file_for_configuration_as_input: str,\n        ver_hllhc_optics: float,\n        ver_lhc_run: float,\n        ions: bool,\n    ):\n        \"\"\"\n        Initialize the XsuiteCollider class with the given configuration and parameters.\n\n        Args:\n            configuration (dict): A dictionary containing various configuration settings.\n                - config_beambeam (dict): Configuration for beam-beam interactions.\n                - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n                - config_lumi_leveling (dict): Configuration for luminosity leveling.\n                - save_output_collider (bool): Flag to save the final collider to disk.\n                - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n                - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                    IP1 and IP5.\n            path_collider_file_for_configuration_as_input (str): Path to the collider file.\n            ver_hllhc_optics (float): Version of the HL-LHC optics.\n            ver_lhc_run (float): Version of the LHC run.\n            ions (bool): Flag indicating if ions are used.\n        \"\"\"\n        # Collider file path\n        self.path_collider_file_for_configuration_as_input = (\n            path_collider_file_for_configuration_as_input\n        )\n\n        # Configuration variables\n        self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n        self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n        self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n        # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n        self.config_lumi_leveling_ip1_5 = configuration.get(\"config_lumi_leveling_ip1_5\")\n\n        # Optics version (needed to select the appropriate optics specific functions)\n        self.ver_hllhc_optics: float = ver_hllhc_optics\n        self.ver_lhc_run: float = ver_lhc_run\n        self.ions: bool = ions\n        self._dict_orbit_correction: dict | None = None\n\n        # Crab cavities\n        self._crab: bool | None = None\n\n        # Save collider to disk\n        self.save_output_collider = configuration[\"save_output_collider\"]\n        self.path_collider_file_for_tracking_as_output = configuration[\n            \"path_collider_file_for_tracking_as_output\"\n        ]\n        self.compress = configuration[\"compress\"]\n\n    @property\n    def dict_orbit_correction(self) -&gt; dict:\n        \"\"\"\n        Generates and returns a dictionary containing orbit correction parameters.\n\n        This method checks if the orbit correction dictionary has already been generated.\n        If not, it determines the appropriate set of orbit correction parameters based on\n        the version of HLLHC optics or LHC run provided.\n\n        Returns:\n            dict: A dictionary containing orbit correction parameters.\n\n        Raises:\n            ValueError: If both `ver_hllhc_optics` and `ver_lhc_run` are defined.\n            ValueError: If no optics specific tools are available for the provided configuration.\n        \"\"\"\n        if self._dict_orbit_correction is None:\n            # Check that version is well defined\n            if self.ver_hllhc_optics is not None and self.ver_lhc_run is not None:\n                raise ValueError(\"Only one of ver_hllhc_optics and ver_lhc_run can be defined\")\n\n            # Get the appropriate optics_specific_tools\n            if self.ver_hllhc_optics is not None:\n                match self.ver_hllhc_optics:\n                    case 1.6:\n                        self._dict_orbit_correction = gen_corr_hllhc16()\n                    case 1.3:\n                        self._dict_orbit_correction = gen_corr_hllhc13()\n                    case _:\n                        raise ValueError(\"No optics specific tools for this configuration\")\n            elif self.ver_lhc_run == 3.0:\n                self._dict_orbit_correction = (\n                    gen_corr_runIII_ions() if self.ions else gen_corr_runIII()\n                )\n            else:\n                raise ValueError(\"No optics specific tools for the provided configuration\")\n\n        return self._dict_orbit_correction\n\n    @staticmethod\n    def _load_collider(path_collider) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file using an external path.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n\n        # Correct collider file path if it is a zip file\n        if os.path.exists(f\"{path_collider}.zip\") and not path_collider.endswith(\".zip\"):\n            path_collider += \".zip\"\n\n        # Load as a json if not zip\n        if not path_collider.endswith(\".zip\"):\n            return xt.Multiline.from_json(path_collider)\n\n        # Uncompress file locally\n        logging.info(f\"Unzipping {path_collider}\")\n        with ZipFile(path_collider, \"r\") as zip_ref:\n            zip_ref.extractall()\n        final_path = os.path.basename(path_collider).replace(\".zip\", \"\")\n        return xt.Multiline.from_json(final_path)\n\n    def load_collider(self) -&gt; xt.Multiline:\n        \"\"\"\n        Load a collider configuration from a file.\n\n        If the file path ends with \".zip\", the file is uncompressed locally\n        and the collider configuration is loaded from the uncompressed file.\n        Otherwise, the collider configuration is loaded directly from the file.\n\n        Returns:\n            xt.Multiline: The loaded collider configuration.\n        \"\"\"\n        return self._load_collider(self.path_collider_file_for_configuration_as_input)\n\n    def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        This method installs beam-beam interactions in the collider with the specified\n        parameters. The beam-beam lenses are initially inactive and not configured.\n\n        Args:\n            collider (xt.Multiline): The collider object where the beam-beam interactions\n                will be installed.\n\n        Returns:\n            None\n        \"\"\"\n        # Install beam-beam lenses (inactive and not configured)\n        collider.install_beambeam_interactions(\n            clockwise_line=\"lhcb1\",\n            anticlockwise_line=\"lhcb2\",\n            ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n            delay_at_ips_slots=[0, 891, 0, 2670],\n            num_long_range_encounters_per_side=self.config_beambeam[\n                \"num_long_range_encounters_per_side\"\n            ],\n            num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n            harmonic_number=35640,\n            bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n            sigmaz=self.config_beambeam[\"sigma_z\"],\n        )\n\n    def set_knobs(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Set all knobs for the collider, including crossing angles, dispersion correction,\n        RF, crab cavities, experimental magnets, etc.\n\n        Args:\n            collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n        Returns:\n            None\n        \"\"\"\n        # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n        # experimental magnets, etc.)\n        for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n            collider.vars[kk] = vv\n\n        # Crab fix (if needed)\n        if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n            apply_crab_fix(collider, self.config_knobs_and_tuning)\n\n    def match_tune_and_chroma(\n        self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n    ) -&gt; None:\n        \"\"\"\n        This method adjusts the tune and chromaticity of the specified collider lines\n        (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n        optionally matches the linear coupling to zero.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be tuned.\n            match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n                matched to zero. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n            targets = {\n                \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n                \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n                \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n                \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n            }\n\n            xm.machine_tuning(\n                line=collider[line_name],\n                enable_closed_orbit_correction=True,\n                enable_linear_coupling_correction=match_linear_coupling_to_zero,\n                enable_tune_correction=True,\n                enable_chromaticity_correction=True,\n                knob_names=knob_names,\n                targets=targets,\n                line_co_ref=collider[f\"{line_name}_co_ref\"],\n                co_corr_config=self.dict_orbit_correction[line_name],\n            )\n\n    def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n        \"\"\"\n        Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n        This method performs the following steps:\n        1. Retrieves the filling scheme path from the configuration.\n        2. Checks if the filling scheme path needs to be obtained from the template schemes.\n        3. Loads and verifies the filling scheme, potentially converting it if necessary.\n        4. Updates the configuration with the correct filling scheme path.\n        5. Determines the number of long-range encounters to consider.\n        6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n        number of long-range interactions.\n           - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n           - Otherwise, automatically selects the worst bunch.\n        7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n        Args:\n            ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n                for beam 1. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        # Get the filling scheme path\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Check if the filling scheme path must be obtained from the template schemes\n        scheme_folder = pathlib.Path(__file__).parent.parent.resolve().joinpath(\"filling_schemes\")\n        if filling_scheme_path in os.listdir(scheme_folder):\n            filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Load and check filling scheme, potentially convert it\n        filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n        # Correct filling scheme in config, as it might have been converted\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n        # Get number of LR to consider\n        n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n        # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n            # Case the bunch number has not been provided\n            worst_bunch_b1 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n            )\n            if ask_worst_bunch:\n                while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                    bool_inp = input(\n                        \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                        \" bunch with the largest number of long-range interactions? It is the bunch\"\n                        \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                    )\n                    if bool_inp == \"y\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                            worst_bunch_b1\n                        )\n                    elif bool_inp == \"n\":\n                        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                            input(\"Please enter the bunch number for beam 1: \")\n                        )\n            else:\n                self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n        if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n            worst_bunch_b2 = get_worst_bunch(\n                filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n            )\n            # For beam 2, just select the worst bunch by default\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n\n    def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n        \"\"\"\n        This method reads a filling scheme from a JSON file specified in the configuration, converts\n        the filling scheme into boolean arrays for two beams, and calculates the number of\n        collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n        Returns:\n            tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n                IP8 respectively.\n\n        Raises:\n            ValueError: If the filling scheme file is not in JSON format.\n            AssertionError: If the length of the beam arrays is not 3564.\n        \"\"\"\n        # Get the filling scheme path (in json or csv format)\n        filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n        # Load the filling scheme\n        if not filling_scheme_path.endswith(\".json\"):\n            raise ValueError(\n                f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n                \" file, it should have been automatically convert when running the script\"\n                \" 001_make_folders.py. Something went wrong.\"\n            )\n\n        with open(filling_scheme_path, \"r\") as fid:\n            filling_scheme = json.load(fid)\n\n        # Extract booleans beam arrays\n        array_b1 = np.array(filling_scheme[\"beam1\"])\n        array_b2 = np.array(filling_scheme[\"beam2\"])\n\n        # Assert that the arrays have the required length, and do the convolution\n        assert len(array_b1) == len(array_b2) == 3564\n        n_collisions_ip1_and_5 = array_b1 @ array_b2\n        n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n        n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n        return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n\n    @property\n    def crab(self) -&gt; bool:\n        \"\"\"\n        This method checks the configuration settings for the presence and value of the\n        \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the\n        `_crab` attribute to True, indicating that crab cavities are active. Otherwise,\n        it sets `_crab` to False.\n\n        Returns:\n            bool: True if crab cavities are active, False otherwise.\n        \"\"\"\n        if self._crab is None:\n            # Get crab cavities\n            self._crab = False\n            if \"on_crab1\" in self.config_knobs_and_tuning[\"knob_settings\"]:\n                crab_val = float(self.config_knobs_and_tuning[\"knob_settings\"][\"on_crab1\"])\n                if abs(crab_val) &gt; 0:\n                    self._crab = True\n        return self._crab\n\n    def level_all_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n        configuration file and performs luminosity leveling using the provided collider object.\n        It also updates the separation knobs for the collider based on the new configuration.\n\n        Args:\n            n_collisions_ip2 (int): Number of collisions at interaction point 2.\n            n_collisions_ip8 (int): Number of collisions at interaction point 8.\n            collider (xt.Multiline): The collider object to be used for luminosity leveling.\n            n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        l_n_collisions = [\n            n_collisions_ip1_and_5,\n            n_collisions_ip2,\n            n_collisions_ip1_and_5,\n            n_collisions_ip8,\n        ]\n        for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n            if ip in self.config_lumi_leveling:\n                self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n            else:\n                logging.warning(f\"IP {ip} is not in the configuration\")\n\n        # ! Crabs are not handled in the following function\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip1\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip5\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def level_ip1_5_by_bunch_intensity(\n        self,\n        collider: xt.Multiline,\n        n_collisions_ip1_and_5: int,\n    ) -&gt; None:\n        \"\"\"\n        This method modifies the bunch intensity to achieve the desired luminosity\n        levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and lattice\n                configuration.\n            n_collisions_ip1_and_5 (int):\n                The number of collisions in IP 1 and 5.\n\n        Returns:\n            None\n        \"\"\"\n        # Initial intensity\n        bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n        # First level luminosity in IP 1/5 changing the intensity\n        if (\n            self.config_lumi_leveling_ip1_5 is not None\n            and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n        ):\n            logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n            # Update the number of bunches in the configuration file\n            self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n            # Do the levelling\n            bunch_intensity = luminosity_leveling_ip1_5(\n                collider,\n                self.config_lumi_leveling_ip1_5,\n                self.config_beambeam,\n                crab=self.crab,\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n        # Update the configuration\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n\n    def level_ip2_8_by_separation(\n        self,\n        n_collisions_ip2: int,\n        n_collisions_ip8: int,\n        collider: xt.Multiline,\n    ) -&gt; None:\n        \"\"\"\n        This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n        file, performs luminosity leveling for the specified collider, and updates the separation\n        knobs for both interaction points.\n\n        Args:\n            n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n            n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n            collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n                performed.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the number of bunches in the configuration file\n        if \"ip2\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n        if \"ip8\" in self.config_lumi_leveling:\n            self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n        # Do levelling in IP2 and IP8\n        xm.lhc.luminosity_leveling(  # type: ignore\n            collider,\n            config_lumi_leveling=self.config_lumi_leveling,\n            config_beambeam=self.config_beambeam,\n        )\n\n        # Update configuration\n        if \"ip2\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n        if \"ip8\" in self.config_lumi_leveling:\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n            self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n\n    def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n        This method adjusts the collider variables to introduce linear coupling. The specific\n        adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n        Args:\n            collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the version of the optics or run is unknown.\n\n        Notes:\n            - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n            - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n            `c_minus_re_b2` variables are adjusted.\n        \"\"\"\n        # Add linear coupling as the target in the tuning of the base collider was 0\n        # (not possible to set it the target to 0.001 for now)\n        if self.ver_lhc_run == 3.0:\n            collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n            collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n            collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        else:\n            raise ValueError(\n                f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n            )\n\n    def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Asserts that the tune, chromaticity, and linear coupling of the collider\n        match the expected values specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the lines to be checked.\n\n        Returns:\n            None\n\n        Raises:\n            AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n                the expected values within the specified tolerances.\n\n        Notes:\n            The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n            - Horizontal tune (qx)\n            - Vertical tune (qy)\n            - Horizontal chromaticity (dqx)\n            - Vertical chromaticity (dqy)\n            - Linear coupling (c_minus)\n\n        The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n        \"\"\"\n        for line_name in [\"lhcb1\", \"lhcb2\"]:\n            tw = collider[line_name].twiss()\n            assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n                f\"tune_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n            )\n            assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n                f\"tune_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n            )\n            assert np.isclose(\n                tw.dqx,\n                self.config_knobs_and_tuning[\"dqx\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_x is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n            )\n            assert np.isclose(\n                tw.dqy,\n                self.config_knobs_and_tuning[\"dqy\"][line_name],\n                rtol=1e-2,\n            ), (\n                f\"chromaticity_y is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n            )\n\n            assert np.isclose(\n                tw.c_minus,\n                self.config_knobs_and_tuning[\"delta_cmr\"],\n                atol=5e-3,\n            ), (\n                f\"linear coupling is not correct for {line_name}. Expected\"\n                f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n            )\n\n    def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n        \"\"\"\n        Configures the beam-beam interactions for the collider.\n\n        This method sets up the beam-beam interactions by configuring the number of particles per\n        bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n        the provided configuration. Additionally, it configures the filling scheme mask and bunch\n        numbers if a filling pattern is specified in the configuration.\n\n        Args:\n            collider (xt.Multiline): The collider object to configure.\n\n        Returns:\n            None\n        \"\"\"\n        collider.configure_beambeam_interactions(\n            num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n            nemitt_x=self.config_beambeam[\"nemitt_x\"],\n            nemitt_y=self.config_beambeam[\"nemitt_y\"],\n        )\n\n        # Configure filling scheme mask and bunch numbers\n        if \"mask_with_filling_pattern\" in self.config_beambeam and (\n            \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n            and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n        ):\n            fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n            with open(fname, \"r\") as fid:\n                filling = json.load(fid)\n            filling_pattern_cw = filling[\"beam1\"]\n            filling_pattern_acw = filling[\"beam2\"]\n\n            # Initialize bunch numbers with empty values\n            i_bunch_cw = None\n            i_bunch_acw = None\n\n            # Only track bunch number if a filling pattern has been provided\n            if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n            if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n                i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n            # Note that a bunch number must be provided if a filling pattern is provided\n            # Apply filling pattern\n            collider.apply_filling_pattern(\n                filling_pattern_cw=filling_pattern_cw,\n                filling_pattern_acw=filling_pattern_acw,\n                i_bunch_cw=i_bunch_cw,\n                i_bunch_acw=i_bunch_acw,\n            )\n\n    def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n        \"\"\"\n        Records the final luminosity and pile-up for specified interaction points (IPs)\n        in the collider, both with and without beam-beam effects.\n\n        Args:\n            collider : (xt.Multiline): The collider object configured.\n            l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n                IP.\n\n        Returns:\n            None\n        \"\"\"\n        # Define IPs in which the luminosity will be computed\n        l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n        # Ensure that the final number of particles per bunch is defined, even\n        # if the leveling has been done by separation\n        if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n            self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n                \"num_particles_per_bunch\"\n            ]\n\n        def _twiss_and_compute_lumi(collider, l_n_collisions):\n            # Loop over each IP and record the luminosity\n            twiss_b1 = collider[\"lhcb1\"].twiss()\n            twiss_b2 = collider[\"lhcb2\"].twiss()\n            l_lumi = []\n            l_PU = []\n            for n_col, ip in zip(l_n_collisions, l_ip):\n                L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                    n_colliding_bunches=n_col,\n                    num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                    ip_name=ip,\n                    nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                    nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                    sigma_z=self.config_beambeam[\"sigma_z\"],\n                    twiss_b1=twiss_b1,\n                    twiss_b2=twiss_b2,\n                    crab=self.crab,\n                )\n                PU = compute_PU(\n                    L,\n                    n_col,\n                    twiss_b1[\"T_rev0\"],\n                    cross_section=self.config_beambeam[\"cross_section\"],\n                )\n\n                l_lumi.append(L)\n                l_PU.append(PU)\n\n            return l_lumi, l_PU\n\n        # Get the final luminosity in all IPs, without beam-beam\n        collider.vars[\"beambeam_scale\"] = 0\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n        # Get the final luminosity in all IPs, with beam-beam\n        collider.vars[\"beambeam_scale\"] = 1\n        l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n        # Update configuration\n        for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n            self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n            self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n\n    def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n        \"\"\"\n        Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n        Args:\n            collider (Collider): The collider object to be saved.\n            full_configuration (dict): The full configuration dictionary to be deep-copied into the\n                collider's metadata.\n\n        Returns:\n            None\n        \"\"\"\n        if self.save_output_collider:\n            logging.info(\"Saving collider as json\")\n            if (\n                hasattr(collider, \"metadata\")\n                and collider.metadata is not None\n                and isinstance(collider.metadata, dict)\n            ):\n                collider.metadata.update(copy.deepcopy(full_configuration))\n            else:\n                collider.metadata = copy.deepcopy(full_configuration)\n            collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n            # Compress the collider file to zip to ease the load on afs\n            if self.compress:\n                compress_and_write(self.path_collider_file_for_tracking_as_output)\n\n    @staticmethod\n    def update_configuration_knob(\n        collider: xt.Multiline, dictionnary: dict, knob_name: str\n    ) -&gt; None:\n        \"\"\"\n        Updates the given dictionary with the final value of a specified knob from the collider.\n\n        Args:\n            collider (xt.Multiline): The collider object containing various variables.\n            dictionnary (dict): The dictionary to be updated with the knob's final value.\n            knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n        Returns:\n            None\n        \"\"\"\n        if knob_name in collider.vars.keys():\n            dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n\n    @staticmethod\n    def return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n        \"\"\"\n        Generate a detailed fingerprint of the specified collider line. Useful to compare two\n        colliders.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the line data.\n            line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n        Returns:\n            str:\n                A formatted string containing detailed information about the collider line, including:\n                - Installed element types\n                - Tunes and chromaticity\n                - Synchrotron tune and slip factor\n                - Twiss parameters and phases at interaction points (IPs)\n                - Dispersion and crab dispersion at IPs\n                - Amplitude detuning coefficients\n                - Non-linear chromaticity\n                - Tunes and momentum compaction vs delta\n        \"\"\"\n        line = collider[line_name]\n\n        tw = line.twiss()\n        tt = line.get_table()\n\n        det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n        det_table = xt.Table(\n            {\n                \"name\": np.array(list(det.keys())),\n                \"value\": np.array(list(det.values())),\n            }\n        )\n\n        nl_chrom = line.get_non_linear_chromaticity(\n            delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n        )\n\n        out = \"\"\n\n        out += f\"Line: {line_name}\\n\"\n        out += \"\\n\"\n\n        out += \"Installed element types:\\n\"\n        out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n        out += \"\\n\"\n\n        out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n        out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n        out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n        out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n        out += \"\\n\"\n\n        out += \"Twiss parameters and phases at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s betx bety alfx alfy mux muy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx dy dpx dpy\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Crab dispersion at IPs:\\n\"\n        out += (\n            tw.rows[\"ip.*\"]\n            .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n            .show(output=str, max_col_width=int(1e6), digits=8)\n        )\n        out += \"\\n\\n\"\n\n        out += \"Amplitude detuning coefficients:\\n\"\n        out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        out += \"Non-linear chromaticity:\\n\"\n        out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n        out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n        out += \"\\n\\n\"\n\n        out += \"Tunes and momentum compaction vs delta:\\n\"\n        out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n        out += \"\\n\\n\"\n\n        return out\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.crab","title":"<code>crab: bool</code>  <code>property</code>","text":"<p>This method checks the configuration settings for the presence and value of the \"on_crab1\" knob. If the knob is present and its value is non-zero, it sets the <code>_crab</code> attribute to True, indicating that crab cavities are active. Otherwise, it sets <code>_crab</code> to False.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if crab cavities are active, False otherwise.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.dict_orbit_correction","title":"<code>dict_orbit_correction: dict</code>  <code>property</code>","text":"<p>Generates and returns a dictionary containing orbit correction parameters.</p> <p>This method checks if the orbit correction dictionary has already been generated. If not, it determines the appropriate set of orbit correction parameters based on the version of HLLHC optics or LHC run provided.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing orbit correction parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>ver_hllhc_optics</code> and <code>ver_lhc_run</code> are defined.</p> <code>ValueError</code> <p>If no optics specific tools are available for the provided configuration.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.__init__","title":"<code>__init__(configuration, path_collider_file_for_configuration_as_input, ver_hllhc_optics, ver_lhc_run, ions)</code>","text":"<p>Initialize the XsuiteCollider class with the given configuration and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing various configuration settings. - config_beambeam (dict): Configuration for beam-beam interactions. - config_knobs_and_tuning (dict): Configuration for knobs and tuning. - config_lumi_leveling (dict): Configuration for luminosity leveling. - save_output_collider (bool): Flag to save the final collider to disk. - path_collider_file_for_tracking_as_output (str): Path to save the final collider. - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at     IP1 and IP5.</p> required <code>path_collider_file_for_configuration_as_input</code> <code>str</code> <p>Path to the collider file.</p> required <code>ver_hllhc_optics</code> <code>float</code> <p>Version of the HL-LHC optics.</p> required <code>ver_lhc_run</code> <code>float</code> <p>Version of the LHC run.</p> required <code>ions</code> <code>bool</code> <p>Flag indicating if ions are used.</p> required Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def __init__(\n    self,\n    configuration: dict,\n    path_collider_file_for_configuration_as_input: str,\n    ver_hllhc_optics: float,\n    ver_lhc_run: float,\n    ions: bool,\n):\n    \"\"\"\n    Initialize the XsuiteCollider class with the given configuration and parameters.\n\n    Args:\n        configuration (dict): A dictionary containing various configuration settings.\n            - config_beambeam (dict): Configuration for beam-beam interactions.\n            - config_knobs_and_tuning (dict): Configuration for knobs and tuning.\n            - config_lumi_leveling (dict): Configuration for luminosity leveling.\n            - save_output_collider (bool): Flag to save the final collider to disk.\n            - path_collider_file_for_tracking_as_output (str): Path to save the final collider.\n            - config_lumi_leveling_ip1_5 (optional): Configuration for luminosity leveling at\n                IP1 and IP5.\n        path_collider_file_for_configuration_as_input (str): Path to the collider file.\n        ver_hllhc_optics (float): Version of the HL-LHC optics.\n        ver_lhc_run (float): Version of the LHC run.\n        ions (bool): Flag indicating if ions are used.\n    \"\"\"\n    # Collider file path\n    self.path_collider_file_for_configuration_as_input = (\n        path_collider_file_for_configuration_as_input\n    )\n\n    # Configuration variables\n    self.config_beambeam: dict[str, Any] = configuration[\"config_beambeam\"]\n    self.config_knobs_and_tuning: dict[str, Any] = configuration[\"config_knobs_and_tuning\"]\n    self.config_lumi_leveling: dict[str, Any] = configuration[\"config_lumi_leveling\"]\n\n    # self.config_lumi_leveling_ip1_5 will be None if not present in the configuration\n    self.config_lumi_leveling_ip1_5 = configuration.get(\"config_lumi_leveling_ip1_5\")\n\n    # Optics version (needed to select the appropriate optics specific functions)\n    self.ver_hllhc_optics: float = ver_hllhc_optics\n    self.ver_lhc_run: float = ver_lhc_run\n    self.ions: bool = ions\n    self._dict_orbit_correction: dict | None = None\n\n    # Crab cavities\n    self._crab: bool | None = None\n\n    # Save collider to disk\n    self.save_output_collider = configuration[\"save_output_collider\"]\n    self.path_collider_file_for_tracking_as_output = configuration[\n        \"path_collider_file_for_tracking_as_output\"\n    ]\n    self.compress = configuration[\"compress\"]\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.add_linear_coupling","title":"<code>add_linear_coupling(collider)</code>","text":"<p>Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.</p> <p>This method adjusts the collider variables to introduce linear coupling. The specific adjustments depend on the version of the LHC run or HL-LHC optics being used.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which linear coupling will be added.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version of the optics or run is unknown.</p> Notes <ul> <li>For LHC Run 3.0, the <code>cmrs.b1_sq</code> and <code>cmrs.b2_sq</code> variables are adjusted.</li> <li>For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the <code>c_minus_re_b1</code> and <code>c_minus_re_b2</code> variables are adjusted.</li> </ul> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def add_linear_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Adds linear coupling to the collider based on the version of the LHC run or HL-LHC optics.\n\n    This method adjusts the collider variables to introduce linear coupling. The specific\n    adjustments depend on the version of the LHC run or HL-LHC optics being used.\n\n    Args:\n        collider (xt.Multiline): The collider object to which linear coupling will be added.\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the version of the optics or run is unknown.\n\n    Notes:\n        - For LHC Run 3.0, the `cmrs.b1_sq` and `cmrs.b2_sq` variables are adjusted.\n        - For HL-LHC optics versions 1.6, 1.5, 1.4, and 1.3, the `c_minus_re_b1` and\n        `c_minus_re_b2` variables are adjusted.\n    \"\"\"\n    # Add linear coupling as the target in the tuning of the base collider was 0\n    # (not possible to set it the target to 0.001 for now)\n    if self.ver_lhc_run == 3.0:\n        collider.vars[\"cmrs.b1_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"cmrs.b2_sq\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    elif self.ver_hllhc_optics in [1.6, 1.5, 1.4, 1.3]:\n        collider.vars[\"c_minus_re_b1\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n        collider.vars[\"c_minus_re_b2\"] += self.config_knobs_and_tuning[\"delta_cmr\"]\n    else:\n        raise ValueError(\n            f\"Unknown version of the optics/run: {self.ver_hllhc_optics}, {self.ver_lhc_run}.\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.assert_tune_chroma_coupling","title":"<code>assert_tune_chroma_coupling(collider)</code>","text":"<p>Asserts that the tune, chromaticity, and linear coupling of the collider match the expected values specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be checked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the tune, chromaticity, or linear coupling values do not match the expected values within the specified tolerances.</p> Notes <p>The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"): - Horizontal tune (qx) - Vertical tune (qy) - Horizontal chromaticity (dqx) - Vertical chromaticity (dqy) - Linear coupling (c_minus)</p> <p>The expected values are retrieved from the <code>self.config_knobs_and_tuning</code> dictionary.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def assert_tune_chroma_coupling(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Asserts that the tune, chromaticity, and linear coupling of the collider\n    match the expected values specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be checked.\n\n    Returns:\n        None\n\n    Raises:\n        AssertionError: If any of the tune, chromaticity, or linear coupling values do not match\n            the expected values within the specified tolerances.\n\n    Notes:\n        The function checks the following parameters for each line (\"lhcb1\" and \"lhcb2\"):\n        - Horizontal tune (qx)\n        - Vertical tune (qy)\n        - Horizontal chromaticity (dqx)\n        - Vertical chromaticity (dqy)\n        - Linear coupling (c_minus)\n\n    The expected values are retrieved from the `self.config_knobs_and_tuning` dictionary.\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        tw = collider[line_name].twiss()\n        assert np.isclose(tw.qx, self.config_knobs_and_tuning[\"qx\"][line_name], atol=1e-4), (\n            f\"tune_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qx'][line_name]}, got {tw.qx}\"\n        )\n        assert np.isclose(tw.qy, self.config_knobs_and_tuning[\"qy\"][line_name], atol=1e-4), (\n            f\"tune_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['qy'][line_name]}, got {tw.qy}\"\n        )\n        assert np.isclose(\n            tw.dqx,\n            self.config_knobs_and_tuning[\"dqx\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_x is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqx'][line_name]}, got {tw.dqx}\"\n        )\n        assert np.isclose(\n            tw.dqy,\n            self.config_knobs_and_tuning[\"dqy\"][line_name],\n            rtol=1e-2,\n        ), (\n            f\"chromaticity_y is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['dqy'][line_name]}, got {tw.dqy}\"\n        )\n\n        assert np.isclose(\n            tw.c_minus,\n            self.config_knobs_and_tuning[\"delta_cmr\"],\n            atol=5e-3,\n        ), (\n            f\"linear coupling is not correct for {line_name}. Expected\"\n            f\" {self.config_knobs_and_tuning['delta_cmr']}, got {tw.c_minus}\"\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.compute_collision_from_scheme","title":"<code>compute_collision_from_scheme()</code>","text":"<p>This method reads a filling scheme from a JSON file specified in the configuration, converts the filling scheme into boolean arrays for two beams, and calculates the number of collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.</p> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and IP8 respectively.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the filling scheme file is not in JSON format.</p> <code>AssertionError</code> <p>If the length of the beam arrays is not 3564.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def compute_collision_from_scheme(self) -&gt; tuple[int, int, int]:\n    \"\"\"\n    This method reads a filling scheme from a JSON file specified in the configuration, converts\n    the filling scheme into boolean arrays for two beams, and calculates the number of\n    collisions at IP1 &amp; IP5, IP2, and IP8 by performing convolutions on the arrays.\n\n    Returns:\n        tuple[int, int, int]: A tuple containing the number of collisions at IP1 &amp; IP5, IP2, and\n            IP8 respectively.\n\n    Raises:\n        ValueError: If the filling scheme file is not in JSON format.\n        AssertionError: If the length of the beam arrays is not 3564.\n    \"\"\"\n    # Get the filling scheme path (in json or csv format)\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Load the filling scheme\n    if not filling_scheme_path.endswith(\".json\"):\n        raise ValueError(\n            f\"Unknown filling scheme file format: {filling_scheme_path}. It you provided a csv\"\n            \" file, it should have been automatically convert when running the script\"\n            \" 001_make_folders.py. Something went wrong.\"\n        )\n\n    with open(filling_scheme_path, \"r\") as fid:\n        filling_scheme = json.load(fid)\n\n    # Extract booleans beam arrays\n    array_b1 = np.array(filling_scheme[\"beam1\"])\n    array_b2 = np.array(filling_scheme[\"beam2\"])\n\n    # Assert that the arrays have the required length, and do the convolution\n    assert len(array_b1) == len(array_b2) == 3564\n    n_collisions_ip1_and_5 = array_b1 @ array_b2\n    n_collisions_ip2 = np.roll(array_b1, 891) @ array_b2\n    n_collisions_ip8 = np.roll(array_b1, 2670) @ array_b2\n\n    return int(n_collisions_ip1_and_5), int(n_collisions_ip2), int(n_collisions_ip8)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.configure_beam_beam","title":"<code>configure_beam_beam(collider)</code>","text":"<p>Configures the beam-beam interactions for the collider.</p> <p>This method sets up the beam-beam interactions by configuring the number of particles per bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on the provided configuration. Additionally, it configures the filling scheme mask and bunch numbers if a filling pattern is specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to configure.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def configure_beam_beam(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Configures the beam-beam interactions for the collider.\n\n    This method sets up the beam-beam interactions by configuring the number of particles per\n    bunch, the horizontal emittance (nemitt_x), and the vertical emittance (nemitt_y) based on\n    the provided configuration. Additionally, it configures the filling scheme mask and bunch\n    numbers if a filling pattern is specified in the configuration.\n\n    Args:\n        collider (xt.Multiline): The collider object to configure.\n\n    Returns:\n        None\n    \"\"\"\n    collider.configure_beambeam_interactions(\n        num_particles=self.config_beambeam[\"num_particles_per_bunch\"],\n        nemitt_x=self.config_beambeam[\"nemitt_x\"],\n        nemitt_y=self.config_beambeam[\"nemitt_y\"],\n    )\n\n    # Configure filling scheme mask and bunch numbers\n    if \"mask_with_filling_pattern\" in self.config_beambeam and (\n        \"pattern_fname\" in self.config_beambeam[\"mask_with_filling_pattern\"]\n        and self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] is not None\n    ):\n        fname = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n        with open(fname, \"r\") as fid:\n            filling = json.load(fid)\n        filling_pattern_cw = filling[\"beam1\"]\n        filling_pattern_acw = filling[\"beam2\"]\n\n        # Initialize bunch numbers with empty values\n        i_bunch_cw = None\n        i_bunch_acw = None\n\n        # Only track bunch number if a filling pattern has been provided\n        if \"i_bunch_b1\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_cw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"]\n        if \"i_bunch_b2\" in self.config_beambeam[\"mask_with_filling_pattern\"]:\n            i_bunch_acw = self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"]\n\n        # Note that a bunch number must be provided if a filling pattern is provided\n        # Apply filling pattern\n        collider.apply_filling_pattern(\n            filling_pattern_cw=filling_pattern_cw,\n            filling_pattern_acw=filling_pattern_acw,\n            i_bunch_cw=i_bunch_cw,\n            i_bunch_acw=i_bunch_acw,\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.install_beam_beam_wrapper","title":"<code>install_beam_beam_wrapper(collider)</code>","text":"<p>This method installs beam-beam interactions in the collider with the specified parameters. The beam-beam lenses are initially inactive and not configured.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object where the beam-beam interactions will be installed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def install_beam_beam_wrapper(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    This method installs beam-beam interactions in the collider with the specified\n    parameters. The beam-beam lenses are initially inactive and not configured.\n\n    Args:\n        collider (xt.Multiline): The collider object where the beam-beam interactions\n            will be installed.\n\n    Returns:\n        None\n    \"\"\"\n    # Install beam-beam lenses (inactive and not configured)\n    collider.install_beambeam_interactions(\n        clockwise_line=\"lhcb1\",\n        anticlockwise_line=\"lhcb2\",\n        ip_names=[\"ip1\", \"ip2\", \"ip5\", \"ip8\"],\n        delay_at_ips_slots=[0, 891, 0, 2670],\n        num_long_range_encounters_per_side=self.config_beambeam[\n            \"num_long_range_encounters_per_side\"\n        ],\n        num_slices_head_on=self.config_beambeam[\"num_slices_head_on\"],\n        harmonic_number=35640,\n        bunch_spacing_buckets=self.config_beambeam[\"bunch_spacing_buckets\"],\n        sigmaz=self.config_beambeam[\"sigma_z\"],\n    )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_all_by_separation","title":"<code>level_all_by_separation(n_collisions_ip2, n_collisions_ip8, collider, n_collisions_ip1_and_5)</code>","text":"<p>This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the configuration file and performs luminosity leveling using the provided collider object. It also updates the separation knobs for the collider based on the new configuration.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>Number of collisions at interaction point 2.</p> required <code>n_collisions_ip8</code> <code>int</code> <p>Number of collisions at interaction point 8.</p> required <code>collider</code> <code>Multiline</code> <p>The collider object to be used for luminosity leveling.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>Number of collisions at interaction points 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_all_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP1, IP2, IP5, and IP8 in the\n    configuration file and performs luminosity leveling using the provided collider object.\n    It also updates the separation knobs for the collider based on the new configuration.\n\n    Args:\n        n_collisions_ip2 (int): Number of collisions at interaction point 2.\n        n_collisions_ip8 (int): Number of collisions at interaction point 8.\n        collider (xt.Multiline): The collider object to be used for luminosity leveling.\n        n_collisions_ip1_and_5 (int): Number of collisions at interaction points 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    l_n_collisions = [\n        n_collisions_ip1_and_5,\n        n_collisions_ip2,\n        n_collisions_ip1_and_5,\n        n_collisions_ip8,\n    ]\n    for ip, n_collisions in zip([\"ip1\", \"ip2\", \"ip5\", \"ip8\"], l_n_collisions):\n        if ip in self.config_lumi_leveling:\n            self.config_lumi_leveling[ip][\"num_colliding_bunches\"] = n_collisions\n        else:\n            logging.warning(f\"IP {ip} is not in the configuration\")\n\n    # ! Crabs are not handled in the following function\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip1\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip1\"], \"on_sep1\")\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip5\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip5\"], \"on_sep5\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_ip1_5_by_bunch_intensity","title":"<code>level_ip1_5_by_bunch_intensity(collider, n_collisions_ip1_and_5)</code>","text":"<p>This method modifies the bunch intensity to achieve the desired luminosity levels in IP 1 and 5. It updates the configuration with the new intensity values.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and lattice configuration.</p> required <code>n_collisions_ip1_and_5</code> <code>int</code> <p>The number of collisions in IP 1 and 5.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip1_5_by_bunch_intensity(\n    self,\n    collider: xt.Multiline,\n    n_collisions_ip1_and_5: int,\n) -&gt; None:\n    \"\"\"\n    This method modifies the bunch intensity to achieve the desired luminosity\n    levels in IP 1 and 5. It updates the configuration with the new intensity values.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and lattice\n            configuration.\n        n_collisions_ip1_and_5 (int):\n            The number of collisions in IP 1 and 5.\n\n    Returns:\n        None\n    \"\"\"\n    # Initial intensity\n    bunch_intensity = self.config_beambeam[\"num_particles_per_bunch\"]\n\n    # First level luminosity in IP 1/5 changing the intensity\n    if (\n        self.config_lumi_leveling_ip1_5 is not None\n        and not self.config_lumi_leveling_ip1_5[\"skip_leveling\"]\n    ):\n        logging.info(\"Leveling luminosity in IP 1/5 varying the intensity\")\n        # Update the number of bunches in the configuration file\n        self.config_lumi_leveling_ip1_5[\"num_colliding_bunches\"] = n_collisions_ip1_and_5\n\n        # Do the levelling\n        bunch_intensity = luminosity_leveling_ip1_5(\n            collider,\n            self.config_lumi_leveling_ip1_5,\n            self.config_beambeam,\n            crab=self.crab,\n            cross_section=self.config_beambeam[\"cross_section\"],\n        )\n\n    # Update the configuration\n    self.config_beambeam[\"final_num_particles_per_bunch\"] = float(bunch_intensity)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.level_ip2_8_by_separation","title":"<code>level_ip2_8_by_separation(n_collisions_ip2, n_collisions_ip8, collider)</code>","text":"<p>This method updates the number of colliding bunches for IP2 and IP8 in the configuration file, performs luminosity leveling for the specified collider, and updates the separation knobs for both interaction points.</p> <p>Parameters:</p> Name Type Description Default <code>n_collisions_ip2</code> <code>int</code> <p>The number of collisions at interaction point 2 (IP2).</p> required <code>n_collisions_ip8</code> <code>int</code> <p>The number of collisions at interaction point 8 (IP8).</p> required <code>collider</code> <code>Multiline</code> <p>The collider object for which the luminosity leveling is to be performed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def level_ip2_8_by_separation(\n    self,\n    n_collisions_ip2: int,\n    n_collisions_ip8: int,\n    collider: xt.Multiline,\n) -&gt; None:\n    \"\"\"\n    This method updates the number of colliding bunches for IP2 and IP8 in the configuration\n    file, performs luminosity leveling for the specified collider, and updates the separation\n    knobs for both interaction points.\n\n    Args:\n        n_collisions_ip2 (int): The number of collisions at interaction point 2 (IP2).\n        n_collisions_ip8 (int): The number of collisions at interaction point 8 (IP8).\n        collider (xt.Multiline): The collider object for which the luminosity leveling is to be\n            performed.\n\n    Returns:\n        None\n    \"\"\"\n    # Update the number of bunches in the configuration file\n    if \"ip2\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip2\"][\"num_colliding_bunches\"] = n_collisions_ip2\n    if \"ip8\" in self.config_lumi_leveling:\n        self.config_lumi_leveling[\"ip8\"][\"num_colliding_bunches\"] = n_collisions_ip8\n\n    # Do levelling in IP2 and IP8\n    xm.lhc.luminosity_leveling(  # type: ignore\n        collider,\n        config_lumi_leveling=self.config_lumi_leveling,\n        config_beambeam=self.config_beambeam,\n    )\n\n    # Update configuration\n    if \"ip2\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip2\"], \"on_sep2v\")\n    if \"ip8\" in self.config_lumi_leveling:\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8h\")\n        self.update_configuration_knob(collider, self.config_lumi_leveling[\"ip8\"], \"on_sep8v\")\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.load_collider","title":"<code>load_collider()</code>","text":"<p>Load a collider configuration from a file.</p> <p>If the file path ends with \".zip\", the file is uncompressed locally and the collider configuration is loaded from the uncompressed file. Otherwise, the collider configuration is loaded directly from the file.</p> <p>Returns:</p> Type Description <code>Multiline</code> <p>xt.Multiline: The loaded collider configuration.</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def load_collider(self) -&gt; xt.Multiline:\n    \"\"\"\n    Load a collider configuration from a file.\n\n    If the file path ends with \".zip\", the file is uncompressed locally\n    and the collider configuration is loaded from the uncompressed file.\n    Otherwise, the collider configuration is loaded directly from the file.\n\n    Returns:\n        xt.Multiline: The loaded collider configuration.\n    \"\"\"\n    return self._load_collider(self.path_collider_file_for_configuration_as_input)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.match_tune_and_chroma","title":"<code>match_tune_and_chroma(collider, match_linear_coupling_to_zero=True)</code>","text":"<p>This method adjusts the tune and chromaticity of the specified collider lines (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also optionally matches the linear coupling to zero.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the lines to be tuned.</p> required <code>match_linear_coupling_to_zero</code> <code>bool</code> <p>If True, linear coupling will be matched to zero. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def match_tune_and_chroma(\n    self, collider: xt.Multiline, match_linear_coupling_to_zero: bool = True\n) -&gt; None:\n    \"\"\"\n    This method adjusts the tune and chromaticity of the specified collider lines\n    (\"lhcb1\" and \"lhcb2\") to the target values defined in the configuration. It also\n    optionally matches the linear coupling to zero.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the lines to be tuned.\n        match_linear_coupling_to_zero (bool, optional): If True, linear coupling will be\n            matched to zero. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    for line_name in [\"lhcb1\", \"lhcb2\"]:\n        knob_names = self.config_knobs_and_tuning[\"knob_names\"][line_name]\n\n        targets = {\n            \"qx\": self.config_knobs_and_tuning[\"qx\"][line_name],\n            \"qy\": self.config_knobs_and_tuning[\"qy\"][line_name],\n            \"dqx\": self.config_knobs_and_tuning[\"dqx\"][line_name],\n            \"dqy\": self.config_knobs_and_tuning[\"dqy\"][line_name],\n        }\n\n        xm.machine_tuning(\n            line=collider[line_name],\n            enable_closed_orbit_correction=True,\n            enable_linear_coupling_correction=match_linear_coupling_to_zero,\n            enable_tune_correction=True,\n            enable_chromaticity_correction=True,\n            knob_names=knob_names,\n            targets=targets,\n            line_co_ref=collider[f\"{line_name}_co_ref\"],\n            co_corr_config=self.dict_orbit_correction[line_name],\n        )\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.record_final_luminosity","title":"<code>record_final_luminosity(collider, l_n_collisions)</code>","text":"<p>Records the final luminosity and pile-up for specified interaction points (IPs) in the collider, both with and without beam-beam effects.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <p>(xt.Multiline): The collider object configured.</p> required <code>l_n_collisions</code> <code>list[int]</code> <p>A list containing the number of colliding bunches for each IP.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def record_final_luminosity(self, collider: xt.Multiline, l_n_collisions: list[int]) -&gt; None:\n    \"\"\"\n    Records the final luminosity and pile-up for specified interaction points (IPs)\n    in the collider, both with and without beam-beam effects.\n\n    Args:\n        collider : (xt.Multiline): The collider object configured.\n        l_n_collisions (list[int]): A list containing the number of colliding bunches for each\n            IP.\n\n    Returns:\n        None\n    \"\"\"\n    # Define IPs in which the luminosity will be computed\n    l_ip = [\"ip1\", \"ip2\", \"ip5\", \"ip8\"]\n\n    # Ensure that the final number of particles per bunch is defined, even\n    # if the leveling has been done by separation\n    if \"final_num_particles_per_bunch\" not in self.config_beambeam:\n        self.config_beambeam[\"final_num_particles_per_bunch\"] = self.config_beambeam[\n            \"num_particles_per_bunch\"\n        ]\n\n    def _twiss_and_compute_lumi(collider, l_n_collisions):\n        # Loop over each IP and record the luminosity\n        twiss_b1 = collider[\"lhcb1\"].twiss()\n        twiss_b2 = collider[\"lhcb2\"].twiss()\n        l_lumi = []\n        l_PU = []\n        for n_col, ip in zip(l_n_collisions, l_ip):\n            L = xt.lumi.luminosity_from_twiss(  # type: ignore\n                n_colliding_bunches=n_col,\n                num_particles_per_bunch=self.config_beambeam[\"final_num_particles_per_bunch\"],\n                ip_name=ip,\n                nemitt_x=self.config_beambeam[\"nemitt_x\"],\n                nemitt_y=self.config_beambeam[\"nemitt_y\"],\n                sigma_z=self.config_beambeam[\"sigma_z\"],\n                twiss_b1=twiss_b1,\n                twiss_b2=twiss_b2,\n                crab=self.crab,\n            )\n            PU = compute_PU(\n                L,\n                n_col,\n                twiss_b1[\"T_rev0\"],\n                cross_section=self.config_beambeam[\"cross_section\"],\n            )\n\n            l_lumi.append(L)\n            l_PU.append(PU)\n\n        return l_lumi, l_PU\n\n    # Get the final luminosity in all IPs, without beam-beam\n    collider.vars[\"beambeam_scale\"] = 0\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_without_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_without_beam_beam\"] = float(PU)\n\n    # Get the final luminosity in all IPs, with beam-beam\n    collider.vars[\"beambeam_scale\"] = 1\n    l_lumi, l_PU = _twiss_and_compute_lumi(collider, l_n_collisions)\n\n    # Update configuration\n    for ip, L, PU in zip(l_ip, l_lumi, l_PU):\n        self.config_beambeam[f\"luminosity_{ip}_with_beam_beam\"] = float(L)\n        self.config_beambeam[f\"Pile-up_{ip}_with_beam_beam\"] = float(PU)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.return_fingerprint","title":"<code>return_fingerprint(collider, line_name='lhcb1')</code>  <code>staticmethod</code>","text":"<p>Generate a detailed fingerprint of the specified collider line. Useful to compare two colliders.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the line data.</p> required <code>line_name</code> <code>str</code> <p>The name of the line to analyze within the collider. Default to \"lhcb1\".</p> <code>'lhcb1'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing detailed information about the collider line, including: - Installed element types - Tunes and chromaticity - Synchrotron tune and slip factor - Twiss parameters and phases at interaction points (IPs) - Dispersion and crab dispersion at IPs - Amplitude detuning coefficients - Non-linear chromaticity - Tunes and momentum compaction vs delta</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef return_fingerprint(collider, line_name=\"lhcb1\") -&gt; str:\n    \"\"\"\n    Generate a detailed fingerprint of the specified collider line. Useful to compare two\n    colliders.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the line data.\n        line_name (str): The name of the line to analyze within the collider. Default to \"lhcb1\".\n\n    Returns:\n        str:\n            A formatted string containing detailed information about the collider line, including:\n            - Installed element types\n            - Tunes and chromaticity\n            - Synchrotron tune and slip factor\n            - Twiss parameters and phases at interaction points (IPs)\n            - Dispersion and crab dispersion at IPs\n            - Amplitude detuning coefficients\n            - Non-linear chromaticity\n            - Tunes and momentum compaction vs delta\n    \"\"\"\n    line = collider[line_name]\n\n    tw = line.twiss()\n    tt = line.get_table()\n\n    det = line.get_amplitude_detuning_coefficients(a0_sigmas=0.1, a1_sigmas=0.2, a2_sigmas=0.3)\n\n    det_table = xt.Table(\n        {\n            \"name\": np.array(list(det.keys())),\n            \"value\": np.array(list(det.values())),\n        }\n    )\n\n    nl_chrom = line.get_non_linear_chromaticity(\n        delta0_range=(-2e-4, 2e-4), num_delta=5, fit_order=3\n    )\n\n    out = \"\"\n\n    out += f\"Line: {line_name}\\n\"\n    out += \"\\n\"\n\n    out += \"Installed element types:\\n\"\n    out += repr([nn for nn in sorted(list(set(tt.element_type))) if len(nn) &gt; 0]) + \"\\n\"\n    out += \"\\n\"\n\n    out += f'Tunes:        Qx  = {tw[\"qx\"]:.5f}       Qy = {tw[\"qy\"]:.5f}\\n'\n    out += f\"\"\"Chromaticity: Q'x = {tw[\"dqx\"]:.2f}     Q'y = \"\"\" + f'{tw[\"dqy\"]:.2f}\\n'\n    out += f'c_minus:      {tw[\"c_minus\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += f'Synchrotron tune: {tw[\"qs\"]:5e}\\n'\n    out += f'Slip factor:      {tw[\"slip_factor\"]:.5e}\\n'\n    out += \"\\n\"\n\n    out += \"Twiss parameters and phases at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s betx bety alfx alfy mux muy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx dy dpx dpy\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Crab dispersion at IPs:\\n\"\n    out += (\n        tw.rows[\"ip.*\"]\n        .cols[\"name s dx_zeta dy_zeta dpx_zeta dpy_zeta\"]\n        .show(output=str, max_col_width=int(1e6), digits=8)\n    )\n    out += \"\\n\\n\"\n\n    out += \"Amplitude detuning coefficients:\\n\"\n    out += det_table.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    out += \"Non-linear chromaticity:\\n\"\n    out += f'dnqx = {list(nl_chrom[\"dnqx\"])}\\n'\n    out += f'dnqy = {list(nl_chrom[\"dnqy\"])}\\n'\n    out += \"\\n\\n\"\n\n    out += \"Tunes and momentum compaction vs delta:\\n\"\n    out += nl_chrom.show(output=str, max_col_width=int(1e6), digits=6)\n    out += \"\\n\\n\"\n\n    return out\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.set_filling_and_bunch_tracked","title":"<code>set_filling_and_bunch_tracked(ask_worst_bunch=False)</code>","text":"<p>Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.</p> <p>This method performs the following steps: 1. Retrieves the filling scheme path from the configuration. 2. Checks if the filling scheme path needs to be obtained from the template schemes. 3. Loads and verifies the filling scheme, potentially converting it if necessary. 4. Updates the configuration with the correct filling scheme path. 5. Determines the number of long-range encounters to consider. 6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest number of long-range interactions.    - If <code>ask_worst_bunch</code> is True, prompts the user to confirm or provide a bunch number.    - Otherwise, automatically selects the worst bunch. 7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.</p> <p>Parameters:</p> Name Type Description Default <code>ask_worst_bunch</code> <code>bool</code> <p>If True, prompts the user to confirm or provide the bunch number for beam 1. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_filling_and_bunch_tracked(self, ask_worst_bunch: bool = False) -&gt; None:\n    \"\"\"\n    Sets the filling scheme and determines the bunch to be tracked for beam-beam interactions.\n\n    This method performs the following steps:\n    1. Retrieves the filling scheme path from the configuration.\n    2. Checks if the filling scheme path needs to be obtained from the template schemes.\n    3. Loads and verifies the filling scheme, potentially converting it if necessary.\n    4. Updates the configuration with the correct filling scheme path.\n    5. Determines the number of long-range encounters to consider.\n    6. If the bunch number for beam 1 is not provided, it identifies the bunch with the largest\n    number of long-range interactions.\n       - If `ask_worst_bunch` is True, prompts the user to confirm or provide a bunch number.\n       - Otherwise, automatically selects the worst bunch.\n    7. If the bunch number for beam 2 is not provided, it automatically selects the worst bunch.\n\n    Args:\n        ask_worst_bunch (bool): If True, prompts the user to confirm or provide the bunch number\n            for beam 1. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Get the filling scheme path\n    filling_scheme_path = self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"]\n\n    # Check if the filling scheme path must be obtained from the template schemes\n    scheme_folder = pathlib.Path(__file__).parent.parent.resolve().joinpath(\"filling_schemes\")\n    if filling_scheme_path in os.listdir(scheme_folder):\n        filling_scheme_path = str(scheme_folder.joinpath(filling_scheme_path))\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Load and check filling scheme, potentially convert it\n    filling_scheme_path = load_and_check_filling_scheme(filling_scheme_path)\n\n    # Correct filling scheme in config, as it might have been converted\n    self.config_beambeam[\"mask_with_filling_pattern\"][\"pattern_fname\"] = filling_scheme_path\n\n    # Get number of LR to consider\n    n_LR = self.config_beambeam[\"num_long_range_encounters_per_side\"][\"ip1\"]\n\n    # If the bunch number is None, the bunch with the largest number of long-range interactions is used\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n        # Case the bunch number has not been provided\n        worst_bunch_b1 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_1\"\n        )\n        if ask_worst_bunch:\n            while self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] is None:\n                bool_inp = input(\n                    \"The bunch number for beam 1 has not been provided. Do you want to use the\"\n                    \" bunch with the largest number of long-range interactions? It is the bunch\"\n                    \" number \" + str(worst_bunch_b1) + \" (y/n): \"\n                )\n                if bool_inp == \"y\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = (\n                        worst_bunch_b1\n                    )\n                elif bool_inp == \"n\":\n                    self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = int(\n                        input(\"Please enter the bunch number for beam 1: \")\n                    )\n        else:\n            self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b1\"] = worst_bunch_b1\n\n    if self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] is None:\n        worst_bunch_b2 = get_worst_bunch(\n            filling_scheme_path, number_of_LR_to_consider=n_LR, beam=\"beam_2\"\n        )\n        # For beam 2, just select the worst bunch by default\n        self.config_beambeam[\"mask_with_filling_pattern\"][\"i_bunch_b2\"] = worst_bunch_b2\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.set_knobs","title":"<code>set_knobs(collider)</code>","text":"<p>Set all knobs for the collider, including crossing angles, dispersion correction, RF, crab cavities, experimental magnets, etc.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object to which the knob settings will be applied.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def set_knobs(self, collider: xt.Multiline) -&gt; None:\n    \"\"\"\n    Set all knobs for the collider, including crossing angles, dispersion correction,\n    RF, crab cavities, experimental magnets, etc.\n\n    Args:\n        collider (xt.Multiline): The collider object to which the knob settings will be applied.\n\n    Returns:\n        None\n    \"\"\"\n    # Set all knobs (crossing angles, dispersion correction, rf, crab cavities,\n    # experimental magnets, etc.)\n    for kk, vv in self.config_knobs_and_tuning[\"knob_settings\"].items():\n        collider.vars[kk] = vv\n\n    # Crab fix (if needed)\n    if self.ver_hllhc_optics is not None and self.ver_hllhc_optics == 1.3:\n        apply_crab_fix(collider, self.config_knobs_and_tuning)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.update_configuration_knob","title":"<code>update_configuration_knob(collider, dictionnary, knob_name)</code>  <code>staticmethod</code>","text":"<p>Updates the given dictionary with the final value of a specified knob from the collider.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing various variables.</p> required <code>dictionnary</code> <code>dict</code> <p>The dictionary to be updated with the knob's final value.</p> required <code>knob_name</code> <code>str</code> <p>The name of the knob whose value is to be retrieved and stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>@staticmethod\ndef update_configuration_knob(\n    collider: xt.Multiline, dictionnary: dict, knob_name: str\n) -&gt; None:\n    \"\"\"\n    Updates the given dictionary with the final value of a specified knob from the collider.\n\n    Args:\n        collider (xt.Multiline): The collider object containing various variables.\n        dictionnary (dict): The dictionary to be updated with the knob's final value.\n        knob_name (str): The name of the knob whose value is to be retrieved and stored.\n\n    Returns:\n        None\n    \"\"\"\n    if knob_name in collider.vars.keys():\n        dictionnary[f\"final_{knob_name}\"] = float(collider.vars[knob_name]._value)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_collider.html#study_da.generate.master_classes.xsuite_collider.XsuiteCollider.write_collider_to_disk","title":"<code>write_collider_to_disk(collider, full_configuration)</code>","text":"<p>Writes the collider object to disk in JSON format if the save_output_collider flag is set.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Collider</code> <p>The collider object to be saved.</p> required <code>full_configuration</code> <code>dict</code> <p>The full configuration dictionary to be deep-copied into the collider's metadata.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/master_classes/xsuite_collider.py</code> <pre><code>def write_collider_to_disk(self, collider, full_configuration) -&gt; None:\n    \"\"\"\n    Writes the collider object to disk in JSON format if the save_output_collider flag is set.\n\n    Args:\n        collider (Collider): The collider object to be saved.\n        full_configuration (dict): The full configuration dictionary to be deep-copied into the\n            collider's metadata.\n\n    Returns:\n        None\n    \"\"\"\n    if self.save_output_collider:\n        logging.info(\"Saving collider as json\")\n        if (\n            hasattr(collider, \"metadata\")\n            and collider.metadata is not None\n            and isinstance(collider.metadata, dict)\n        ):\n            collider.metadata.update(copy.deepcopy(full_configuration))\n        else:\n            collider.metadata = copy.deepcopy(full_configuration)\n        collider.to_json(self.path_collider_file_for_tracking_as_output)\n\n        # Compress the collider file to zip to ease the load on afs\n        if self.compress:\n            compress_and_write(self.path_collider_file_for_tracking_as_output)\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html","title":"xsuite_leveling","text":"<p>This modules contains functions used for luminosity leveling.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html#study_da.generate.master_classes.xsuite_leveling.compute_PU","title":"<code>compute_PU(luminosity, num_colliding_bunches, T_rev0, cross_section)</code>","text":"<p>Compute the Pile-Up (PU) value.</p> <p>Parameters:</p> Name Type Description Default <code>luminosity</code> <code>float</code> <p>The luminosity of the collider.</p> required <code>num_colliding_bunches</code> <code>int</code> <p>The number of colliding bunches.</p> required <code>T_rev0</code> <code>float</code> <p>The revolution time of the collider.</p> required <code>cross_section</code> <code>float</code> <p>The cross-section value.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The computed Pile-Up (PU) value.</p> Source code in <code>study_da/generate/master_classes/xsuite_leveling.py</code> <pre><code>def compute_PU(\n    luminosity: float, num_colliding_bunches: int, T_rev0: float, cross_section: float\n) -&gt; float:\n    \"\"\"\n    Compute the Pile-Up (PU) value.\n\n    Args:\n        luminosity (float): The luminosity of the collider.\n        num_colliding_bunches (int): The number of colliding bunches.\n        T_rev0 (float): The revolution time of the collider.\n        cross_section (float): The cross-section value.\n\n    Returns:\n        float: The computed Pile-Up (PU) value.\n    \"\"\"\n    return luminosity / num_colliding_bunches * cross_section * T_rev0\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_leveling.html#study_da.generate.master_classes.xsuite_leveling.luminosity_leveling_ip1_5","title":"<code>luminosity_leveling_ip1_5(collider, config_lumi_leveling_ip1_5, config_beambeam, crab=False, cross_section=8.1e-26)</code>","text":"<p>Perform luminosity leveling for interaction points IP1 and IP5.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>dict</code> <p>Dictionary containing collider objects for beams 'lhcb1' and 'lhcb2'.</p> required <code>config_lumi_leveling_ip1_5</code> <code>dict</code> <p>Configuration dictionary for luminosity leveling at IP1 and IP5. Must contain 'num_colliding_bunches' and 'constraints' with 'max_intensity' and 'max_PU'.</p> required <code>config_beambeam</code> <code>dict</code> <p>Configuration dictionary for beam-beam parameters. Must contain 'nemitt_x', 'nemitt_y', and 'sigma_z'.</p> required <code>crab</code> <code>bool</code> <p>Flag to indicate if crab cavities are used. Default to False.</p> <code>False</code> <code>cross_section</code> <code>float</code> <p>Cross-section value in square meters. Default to 81e-27.</p> <code>8.1e-26</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Optimized bunch intensity for leveling in IP1 and IP5.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If the optimization for leveling in IP1/5 fails, a warning is logged.</p> Source code in <code>study_da/generate/master_classes/xsuite_leveling.py</code> <pre><code>def luminosity_leveling_ip1_5(\n    collider: xt.Multiline,\n    config_lumi_leveling_ip1_5: dict[str, Any],\n    config_beambeam: dict[str, Any],\n    crab: bool = False,\n    cross_section: float = 81e-27,\n) -&gt; float:\n    \"\"\"\n    Perform luminosity leveling for interaction points IP1 and IP5.\n\n    Args:\n        collider (dict): Dictionary containing collider objects for beams 'lhcb1' and 'lhcb2'.\n        config_lumi_leveling_ip1_5 (dict): Configuration dictionary for luminosity leveling at IP1\n            and IP5. Must contain 'num_colliding_bunches' and 'constraints' with 'max_intensity'\n            and 'max_PU'.\n        config_beambeam (dict): Configuration dictionary for beam-beam parameters. Must contain\n            'nemitt_x', 'nemitt_y', and 'sigma_z'.\n        crab (bool): Flag to indicate if crab cavities are used. Default to False.\n        cross_section (float): Cross-section value in square meters. Default to 81e-27.\n\n    Returns:\n        float: Optimized bunch intensity for leveling in IP1 and IP5.\n\n    Raises:\n        Warning: If the optimization for leveling in IP1/5 fails, a warning is logged.\n    \"\"\"\n    # Get Twiss\n    twiss_b1 = collider[\"lhcb1\"].twiss()\n    twiss_b2 = collider[\"lhcb2\"].twiss()\n\n    # Get the number of colliding bunches in IP1/5\n    n_colliding_IP1_5 = config_lumi_leveling_ip1_5[\"num_colliding_bunches\"]\n\n    # Get max intensity in IP1/5\n    max_intensity_IP1_5 = float(config_lumi_leveling_ip1_5[\"constraints\"][\"max_intensity\"])\n\n    def _compute_lumi(bunch_intensity):\n        luminosity = xt.lumi.luminosity_from_twiss(  # type: ignore\n            n_colliding_bunches=n_colliding_IP1_5,\n            num_particles_per_bunch=bunch_intensity,\n            ip_name=\"ip1\",\n            nemitt_x=config_beambeam[\"nemitt_x\"],\n            nemitt_y=config_beambeam[\"nemitt_y\"],\n            sigma_z=config_beambeam[\"sigma_z\"],\n            twiss_b1=twiss_b1,\n            twiss_b2=twiss_b2,\n            crab=crab,\n        )\n        return luminosity\n\n    def f(bunch_intensity):\n        luminosity = _compute_lumi(bunch_intensity)\n\n        max_PU_IP_1_5 = config_lumi_leveling_ip1_5[\"constraints\"][\"max_PU\"]\n\n        target_luminosity_IP_1_5 = config_lumi_leveling_ip1_5[\"luminosity\"]\n        PU = compute_PU(\n            luminosity,\n            n_colliding_IP1_5,\n            twiss_b1[\"T_rev0\"],\n            cross_section,\n        )\n\n        penalty_PU = max(0, (PU - max_PU_IP_1_5) * 1e35)  # in units of 1e-35\n        penalty_excess_lumi = max(\n            0, (luminosity - target_luminosity_IP_1_5) * 10\n        )  # in units of 1e-35 if luminosity is in units of 1e34\n\n        return abs(luminosity - target_luminosity_IP_1_5) + penalty_PU + penalty_excess_lumi\n\n    # Do the optimization\n    res = minimize_scalar(\n        f,\n        bounds=(\n            1e10,\n            max_intensity_IP1_5,\n        ),\n        method=\"bounded\",\n        options={\"xatol\": 1e7},\n    )\n    if not res.success:  # type: ignore\n        logging.warning(\"Optimization for leveling in IP 1/5 failed. Please check the constraints.\")\n    else:\n        logging.info(\n            f\"Optimization for leveling in IP 1/5 succeeded with I={res.x:.2e} particles per bunch\"  # type: ignore\n        )\n    return res.x  # type: ignore\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html","title":"xsuite_tracking","text":"<p>This class is used to build a Xsuite collider from a madx sequence and optics.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking","title":"<code>XsuiteTracking</code>","text":"<p>XsuiteTracking class for managing particle tracking simulations.</p> <p>Attributes:</p> Name Type Description <code>context_str</code> <code>str</code> <p>The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").</p> <code>device_number</code> <code>int</code> <p>The device number for GPU contexts.</p> <code>_context</code> <code>Context</code> <p>The context object for the simulation.</p> <code>beam</code> <code>str</code> <p>The beam configuration.</p> <code>distribution_file</code> <code>str</code> <p>The file path to the particle data.</p> <code>delta_max</code> <code>float</code> <p>The maximum delta value for particles.</p> <code>n_turns</code> <code>int</code> <p>The number of turns for the simulation.</p> <code>nemitt_x</code> <code>float</code> <p>The normalized emittance in the x direction.</p> <code>nemitt_y</code> <code>float</code> <p>The normalized emittance in the y direction.</p> <p>Methods:</p> Name Description <code>context</code> <p>Get the context object for the simulation.</p> <code>prepare_particle_distribution_for_tracking</code> <p>Prepare the particle distribution for tracking.</p> <code>track</code> <p>Track the particles in the collider.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>class XsuiteTracking:\n    \"\"\"\n    XsuiteTracking class for managing particle tracking simulations.\n\n    Attributes:\n        context_str (str): The context for the simulation (e.g., \"cupy\", \"opencl\", \"cpu\").\n        device_number (int): The device number for GPU contexts.\n        _context (xo.Context): The context object for the simulation.\n        beam (str): The beam configuration.\n        distribution_file (str): The file path to the particle data.\n        delta_max (float): The maximum delta value for particles.\n        n_turns (int): The number of turns for the simulation.\n        nemitt_x (float): The normalized emittance in the x direction.\n        nemitt_y (float): The normalized emittance in the y direction.\n\n    Methods:\n        context: Get the context object for the simulation.\n        prepare_particle_distribution_for_tracking: Prepare the particle distribution for tracking.\n        track: Track the particles in the collider.\n    \"\"\"\n\n    def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n        \"\"\"\n        Initialize the tracking configuration.\n\n        Args:\n            configuration (dict): A dictionary containing the configuration parameters.\n                Expected keys:\n                - \"context\": str, context string for the simulation.\n                - \"device_number\": int, device number for the simulation.\n                - \"beam\": str, beam type for the simulation.\n                - \"distribution_file\": str, path to the particle file.\n                - \"delta_max\": float, maximum delta value for the simulation.\n                - \"n_turns\": int, number of turns for the simulation.\n            nemitt_x (float): Normalized emittance in the x-plane.\n            nemitt_y (float): Normalized emittance in the y-plane.\n        \"\"\"\n        # Context parameters\n        self.context_str: str = configuration[\"context\"]\n        self.device_number: int = configuration[\"device_number\"]\n        self._context = None\n\n        # Simulation parameters\n        self.beam: str = configuration[\"beam\"]\n        self.distribution_file: str = configuration[\"distribution_file\"]\n        self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n        self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n        self.delta_max: float = configuration[\"delta_max\"]\n        self.n_turns: int = configuration[\"n_turns\"]\n\n        # Beambeam parameters\n        self.nemitt_x: float = nemitt_x\n        self.nemitt_y: float = nemitt_y\n\n    @property\n    def context(self) -&gt; Any:\n        \"\"\"\n        Returns the context for the current instance. If the context is not already set,\n        it initializes the context based on the `context_str` attribute. The context can\n        be one of the following:\n\n        - \"cupy\": Uses `xo.ContextCupy`. If `device_number` is specified, it initializes\n            the context with the given device number.\n        - \"opencl\": Uses `xo.ContextPyopencl`.\n        - \"cpu\": Uses `xo.ContextCpu`.\n        - Any other value: Logs a warning and defaults to `xo.ContextCpu`.\n\n        If `device_number` is specified but the context is not \"cupy\", a warning is logged\n        indicating that the device number will be ignored.\n\n        Returns:\n            Any: The initialized context.\n        \"\"\"\n        if self._context is None:\n            if self.device_number is not None and self.context_str not in [\"cupy\"]:\n                logging.warning(\"Device number will be ignored since context is not cupy\")\n            match self.context_str:\n                case \"cupy\":\n                    if self.device_number is not None:\n                        self._context = xo.ContextCupy(device=self.device_number)\n                    else:\n                        self._context = xo.ContextCupy()\n                case \"opencl\":\n                    self._context = xo.ContextPyopencl()\n                case \"cpu\":\n                    self._context = xo.ContextCpu()\n                case _:\n                    logging.warning(\"Context not recognized, using cpu\")\n                    self._context = xo.ContextCpu()\n        return self._context\n\n    # ? I removed type hints for the output as I get an unclear linting error\n    # TODO: Check the proper type hints for the output\n    def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n        \"\"\"\n        Prepare a particle distribution for tracking in the collider.\n\n        This method reads particle data from a parquet file, processes the data to\n        generate normalized amplitudes and angles, and then builds particles for\n        tracking in the collider. If the context is set to use GPU, the collider\n        trackers are reset and rebuilt accordingly.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beam and\n                tracking information.\n\n        Returns:\n            tuple: A tuple containing:\n                - xp.Particles: The particles ready for tracking.\n                - np.ndarray: Array of particle IDs.\n                - np.ndarray: Array of normalized amplitudes in the xy-plane.\n                - np.ndarray: Array of angles in the xy-plane in radians.\n        \"\"\"\n        # Reset the tracker to go to GPU if needed\n        if self.context_str in [\"cupy\", \"opencl\"]:\n            collider.discard_trackers()\n            collider.build_trackers(_context=self.context)\n\n        particle_df = pd.read_parquet(self.particle_path)\n\n        r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n        theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n        A1_in_sigma = r_vect * np.cos(theta_vect)\n        A2_in_sigma = r_vect * np.sin(theta_vect)\n\n        particles = collider[self.beam].build_particles(\n            x_norm=A1_in_sigma,\n            y_norm=A2_in_sigma,\n            delta=self.delta_max,\n            scale_with_transverse_norm_emitt=(\n                self.nemitt_x,\n                self.nemitt_y,\n            ),\n            _context=self.context,\n        )\n\n        particle_id = particle_df.particle_id.values\n        return particles, particle_id, r_vect, theta_vect\n\n    def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n        \"\"\"\n        Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n        Args:\n            collider (xt.Multiline): The collider object containing the beamline to be tracked.\n            particles (xp.Particles): The particles to be tracked.\n\n        Returns:\n            dict: A dictionary representation of the tracked particles.\n        \"\"\"\n        # Optimize line for tracking\n        collider[self.beam].optimize_for_tracking()\n\n        # Track\n        num_turns = self.n_turns\n        a = time.time()\n        collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n        b = time.time()\n\n        logging.info(f\"Elapsed time: {b-a} s\")\n        logging.info(\n            f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n        )\n\n        return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.context","title":"<code>context: Any</code>  <code>property</code>","text":"<p>Returns the context for the current instance. If the context is not already set, it initializes the context based on the <code>context_str</code> attribute. The context can be one of the following:</p> <ul> <li>\"cupy\": Uses <code>xo.ContextCupy</code>. If <code>device_number</code> is specified, it initializes     the context with the given device number.</li> <li>\"opencl\": Uses <code>xo.ContextPyopencl</code>.</li> <li>\"cpu\": Uses <code>xo.ContextCpu</code>.</li> <li>Any other value: Logs a warning and defaults to <code>xo.ContextCpu</code>.</li> </ul> <p>If <code>device_number</code> is specified but the context is not \"cupy\", a warning is logged indicating that the device number will be ignored.</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The initialized context.</p>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.__init__","title":"<code>__init__(configuration, nemitt_x, nemitt_y)</code>","text":"<p>Initialize the tracking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>dict</code> <p>A dictionary containing the configuration parameters. Expected keys: - \"context\": str, context string for the simulation. - \"device_number\": int, device number for the simulation. - \"beam\": str, beam type for the simulation. - \"distribution_file\": str, path to the particle file. - \"delta_max\": float, maximum delta value for the simulation. - \"n_turns\": int, number of turns for the simulation.</p> required <code>nemitt_x</code> <code>float</code> <p>Normalized emittance in the x-plane.</p> required <code>nemitt_y</code> <code>float</code> <p>Normalized emittance in the y-plane.</p> required Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def __init__(self, configuration: dict, nemitt_x: float, nemitt_y: float) -&gt; None:\n    \"\"\"\n    Initialize the tracking configuration.\n\n    Args:\n        configuration (dict): A dictionary containing the configuration parameters.\n            Expected keys:\n            - \"context\": str, context string for the simulation.\n            - \"device_number\": int, device number for the simulation.\n            - \"beam\": str, beam type for the simulation.\n            - \"distribution_file\": str, path to the particle file.\n            - \"delta_max\": float, maximum delta value for the simulation.\n            - \"n_turns\": int, number of turns for the simulation.\n        nemitt_x (float): Normalized emittance in the x-plane.\n        nemitt_y (float): Normalized emittance in the y-plane.\n    \"\"\"\n    # Context parameters\n    self.context_str: str = configuration[\"context\"]\n    self.device_number: int = configuration[\"device_number\"]\n    self._context = None\n\n    # Simulation parameters\n    self.beam: str = configuration[\"beam\"]\n    self.distribution_file: str = configuration[\"distribution_file\"]\n    self.path_distribution_folder_input: str = configuration[\"path_distribution_folder_input\"]\n    self.particle_path: str = f\"{self.path_distribution_folder_input}/{self.distribution_file}\"\n    self.delta_max: float = configuration[\"delta_max\"]\n    self.n_turns: int = configuration[\"n_turns\"]\n\n    # Beambeam parameters\n    self.nemitt_x: float = nemitt_x\n    self.nemitt_y: float = nemitt_y\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.prepare_particle_distribution_for_tracking","title":"<code>prepare_particle_distribution_for_tracking(collider)</code>","text":"<p>Prepare a particle distribution for tracking in the collider.</p> <p>This method reads particle data from a parquet file, processes the data to generate normalized amplitudes and angles, and then builds particles for tracking in the collider. If the context is set to use GPU, the collider trackers are reset and rebuilt accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beam and tracking information.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing: - xp.Particles: The particles ready for tracking. - np.ndarray: Array of particle IDs. - np.ndarray: Array of normalized amplitudes in the xy-plane. - np.ndarray: Array of angles in the xy-plane in radians.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def prepare_particle_distribution_for_tracking(self, collider: xt.Multiline) -&gt; tuple:\n    \"\"\"\n    Prepare a particle distribution for tracking in the collider.\n\n    This method reads particle data from a parquet file, processes the data to\n    generate normalized amplitudes and angles, and then builds particles for\n    tracking in the collider. If the context is set to use GPU, the collider\n    trackers are reset and rebuilt accordingly.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beam and\n            tracking information.\n\n    Returns:\n        tuple: A tuple containing:\n            - xp.Particles: The particles ready for tracking.\n            - np.ndarray: Array of particle IDs.\n            - np.ndarray: Array of normalized amplitudes in the xy-plane.\n            - np.ndarray: Array of angles in the xy-plane in radians.\n    \"\"\"\n    # Reset the tracker to go to GPU if needed\n    if self.context_str in [\"cupy\", \"opencl\"]:\n        collider.discard_trackers()\n        collider.build_trackers(_context=self.context)\n\n    particle_df = pd.read_parquet(self.particle_path)\n\n    r_vect = particle_df[\"normalized amplitude in xy-plane\"].values\n    theta_vect = particle_df[\"angle in xy-plane [deg]\"].values * np.pi / 180  # type: ignore # [rad]\n\n    A1_in_sigma = r_vect * np.cos(theta_vect)\n    A2_in_sigma = r_vect * np.sin(theta_vect)\n\n    particles = collider[self.beam].build_particles(\n        x_norm=A1_in_sigma,\n        y_norm=A2_in_sigma,\n        delta=self.delta_max,\n        scale_with_transverse_norm_emitt=(\n            self.nemitt_x,\n            self.nemitt_y,\n        ),\n        _context=self.context,\n    )\n\n    particle_id = particle_df.particle_id.values\n    return particles, particle_id, r_vect, theta_vect\n</code></pre>"},{"location":"reference/study_da/generate/master_classes/xsuite_tracking.html#study_da.generate.master_classes.xsuite_tracking.XsuiteTracking.track","title":"<code>track(collider, particles)</code>","text":"<p>Tracks particles through a collider for a specified number of turns and logs the elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The collider object containing the beamline to be tracked.</p> required <code>particles</code> <code>Particles</code> <p>The particles to be tracked.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary representation of the tracked particles.</p> Source code in <code>study_da/generate/master_classes/xsuite_tracking.py</code> <pre><code>def track(self, collider: xt.Multiline, particles: xp.Particles) -&gt; dict:\n    \"\"\"\n    Tracks particles through a collider for a specified number of turns and logs the elapsed time.\n\n    Args:\n        collider (xt.Multiline): The collider object containing the beamline to be tracked.\n        particles (xp.Particles): The particles to be tracked.\n\n    Returns:\n        dict: A dictionary representation of the tracked particles.\n    \"\"\"\n    # Optimize line for tracking\n    collider[self.beam].optimize_for_tracking()\n\n    # Track\n    num_turns = self.n_turns\n    a = time.time()\n    collider[self.beam].track(particles, turn_by_turn_monitor=False, num_turns=num_turns)\n    b = time.time()\n\n    logging.info(f\"Elapsed time: {b-a} s\")\n    logging.info(\n        f\"Elapsed time per particle per turn: {(b-a)/particles._capacity/num_turns*1e6} us\"\n    )\n\n    return particles.to_dict()\n</code></pre>"},{"location":"reference/study_da/generate/template_scripts/index.html","title":"template_scripts","text":""},{"location":"reference/study_da/generate/template_scripts/generation_1.html","title":"generation_1","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/template_scripts/generation_1_dummy.html","title":"generation_1_dummy","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/template_scripts/generation_2_dummy.html","title":"generation_2_dummy","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/template_scripts/generation_2_level_by_nb.html","title":"generation_2_level_by_nb","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/template_scripts/generation_2_level_by_sep.html","title":"generation_2_level_by_sep","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/template_scripts/generation_3_dummy.html","title":"generation_3_dummy","text":"<p>This is a template script for generation 1 of simulation study, in which ones generates a particle distribution and a collider from a MAD-X model.</p>"},{"location":"reference/study_da/generate/version_specific_files/index.html","title":"version_specific_files","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html","title":"hllhc13","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html#study_da.generate.version_specific_files.hllhc13.apply_crab_fix","title":"<code>apply_crab_fix(collider, config_knobs_and_tuning)</code>","text":"<p>Apply crab fix beam 2 crabs for HLLHC13</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The Xtrack collider object</p> required <code>config_knobs_and_tuning</code> <code>dict</code> <p>The configuration of the knobs and tuning from the configuration file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/crab_fix.py</code> <pre><code>def apply_crab_fix(collider: xt.Multiline, config_knobs_and_tuning: dict) -&gt; None:\n    \"\"\"Apply crab fix beam 2 crabs for HLLHC13\n\n    Args:\n        collider (xt.Multiline): The Xtrack collider object\n        config_knobs_and_tuning (dict): The configuration of the knobs and tuning from the\n            configuration file.\n\n    Returns:\n        None\n    \"\"\"\n    if \"on_crab5\" in config_knobs_and_tuning[\"knob_settings\"]:\n        collider.vars[\"avcrab_r5b2\"] = -collider.vars[\"avcrab_r5b2\"]._get_value()\n        collider.vars[\"ahcrab_r5b2\"] = -collider.vars[\"ahcrab_r5b2\"]._get_value()\n        collider.vars[\"avcrab_l5b2\"] = -collider.vars[\"avcrab_l5b2\"]._get_value()\n        collider.vars[\"ahcrab_l5b2\"] = -collider.vars[\"ahcrab_l5b2\"]._get_value()\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/index.html#study_da.generate.version_specific_files.hllhc13.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbyvs5.l1b1\",\n                    \"corr_co_acbyhs5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbyvs5.l5b1\",\n                    \"corr_co_acbyhs5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbyvs5.r1b2\",\n                    \"corr_co_acbyhs5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbyvs5.r5b2\",\n                    \"corr_co_acbyhs5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/crab_fix.html","title":"crab_fix","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/crab_fix.html#study_da.generate.version_specific_files.hllhc13.crab_fix.apply_crab_fix","title":"<code>apply_crab_fix(collider, config_knobs_and_tuning)</code>","text":"<p>Apply crab fix beam 2 crabs for HLLHC13</p> <p>Parameters:</p> Name Type Description Default <code>collider</code> <code>Multiline</code> <p>The Xtrack collider object</p> required <code>config_knobs_and_tuning</code> <code>dict</code> <p>The configuration of the knobs and tuning from the configuration file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/crab_fix.py</code> <pre><code>def apply_crab_fix(collider: xt.Multiline, config_knobs_and_tuning: dict) -&gt; None:\n    \"\"\"Apply crab fix beam 2 crabs for HLLHC13\n\n    Args:\n        collider (xt.Multiline): The Xtrack collider object\n        config_knobs_and_tuning (dict): The configuration of the knobs and tuning from the\n            configuration file.\n\n    Returns:\n        None\n    \"\"\"\n    if \"on_crab5\" in config_knobs_and_tuning[\"knob_settings\"]:\n        collider.vars[\"avcrab_r5b2\"] = -collider.vars[\"avcrab_r5b2\"]._get_value()\n        collider.vars[\"ahcrab_r5b2\"] = -collider.vars[\"ahcrab_r5b2\"]._get_value()\n        collider.vars[\"avcrab_l5b2\"] = -collider.vars[\"avcrab_l5b2\"]._get_value()\n        collider.vars[\"ahcrab_l5b2\"] = -collider.vars[\"ahcrab_l5b2\"]._get_value()\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.apply_optics","title":"<code>apply_optics(*args, **kwargs)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.apply_optics.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.apply_optics.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def apply_optics(*args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        *args (Any): See hllhc16.optics_specific_tools.apply_optics.\n        **kwargs (Any): See hllhc16.optics_specific_tools.apply_optics.\n\n    Returns:\n        None\n    \"\"\"\n    apply_optics_hllhc16(*args, **kwargs)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=None, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to None.</p> <code>None</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],  # Not used but important for consistency with other optics\n    ignore_cycling: bool = False,\n    slice_factor: int | None = None,  # Not used but important for consistency with other optics\n    BFPP: bool = False,  # Not used but important for consistency with other optics\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to None.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    # Build sequence\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhcb4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc.seq\";\n      };\n      !Install HL-LHC\n      call, file=\n        \"acc-models-lhc/hllhc_sequence.madx\";\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      option, -echo, warn,-info;\n      \"\"\")\n\n    mad.input(\"\"\"\n      ! Slice nominal sequence\n      exec, myslice;\n      \"\"\")\n\n    mad.input(\"\"\"exec,mk_beam(7000);\"\"\")\n\n    install_errors_placeholders_hllhc(mad)\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    # Incorporate crab-cavities\n    mad.input(\"\"\"\n    ! Install crab cavities (they are off)\n    call, file='acc-models-lhc/toolkit/enable_crabcavities.madx';\n    on_crab1 = 0;\n    on_crab5 = 0;\n    \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc13.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(*args, **kwargs)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.check_madx_lattices.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>See hllhc16.optics_specific_tools.check_madx_lattices.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(*args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        *args (Any): See hllhc16.optics_specific_tools.check_madx_lattices.\n        **kwargs (Any): See hllhc16.optics_specific_tools.check_madx_lattices.\n\n    Returns:\n        None\n    \"\"\"\n    check_madx_lattices_hllhc16(*args, **kwargs)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc13/orbit_correction.html","title":"orbit_correction","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc13/orbit_correction.html#study_da.generate.version_specific_files.hllhc13.orbit_correction.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc13/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbyvs5.l1b1\",\n                    \"corr_co_acbyhs5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbyvs5.l5b1\",\n                    \"corr_co_acbyhs5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbyvs5.r1b2\",\n                    \"corr_co_acbyhs5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbyvs5.r5b2\",\n                    \"corr_co_acbyhs5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/index.html","title":"hllhc16","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/index.html#study_da.generate.version_specific_files.hllhc16.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    # A knob redefinition\n    mad.input(\"on_alice := on_alice_normalized * 7000./nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000./nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=None, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to None.</p> <code>None</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],  # Not used but important for consistency with other optics\n    ignore_cycling: bool = False,\n    slice_factor: int | None = None,  # Not used but important for consistency with other optics\n    BFPP: bool = False,  # Not used but important for consistency with other optics\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to None.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    # Build sequence\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhcb4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc.seq\";\n      };\n      !Install HL-LHC\n      call, file=\n        \"acc-models-lhc/hllhc_sequence.madx\";\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      option, -echo, warn,-info;\n      \"\"\")\n\n    # Fix for hllhc16\n    mad.input(\"\"\"\n    l.mbh = 0.001000;\n    ACSCA, HARMON := HRF400;\n\n    ACSCA.D5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.C5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.B5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.A5L4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.A5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.B5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.C5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.D5R4.B1, VOLT := VRF400/8, LAG := LAGRF400.B1, HARMON := HRF400;\n    ACSCA.D5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.C5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.B5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.A5L4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.A5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.B5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.C5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    ACSCA.D5R4.B2, VOLT := VRF400/8, LAG := LAGRF400.B2, HARMON := HRF400;\n    \"\"\")\n\n    mad.input(\"\"\"\n      ! Slice nominal sequence\n      exec, myslice;\n      \"\"\")\n\n    if mylhcbeam &lt; 3:\n        mad.input(\"\"\"\n      nrj=7000;\n      beam,particle=proton,sequence=lhcb1,energy=nrj,npart=1.15E11,sige=4.5e-4;\n      beam,particle=proton,sequence=lhcb2,energy=nrj,bv = -1,npart=1.15E11,sige=4.5e-4;\n      \"\"\")\n\n    install_errors_placeholders_hllhc(mad)\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    # Incorporate crab-cavities\n    mad.input(\"\"\"\n    ! Install crab cavities (they are off)\n    call, file='acc-models-lhc/toolkit/enable_crabcavities.madx';\n    on_crab1 = 0;\n    on_crab5 = 0;\n    \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/optics_specific_tools.html#study_da.generate.version_specific_files.hllhc16.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:  # sourcery skip: extract-method\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-02)\n    assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-02)\n\n    try:\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=1e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=1e-01)\n\n        df = mad.table.twiss.dframe()\n        for my_ip in [1, 2, 5, 8]:\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-02)\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-02)\n\n        assert df[\"x\"].std() &lt; 1e-6\n        assert df[\"y\"].std() &lt; 1e-6\n    except AssertionError:\n        logging.warning(\"WARNING: Some sanity checks have failed during the madx lattice check\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/hllhc16/orbit_correction.html","title":"orbit_correction","text":""},{"location":"reference/study_da/generate/version_specific_files/hllhc16/orbit_correction.html#study_da.generate.version_specific_files.hllhc16.orbit_correction.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/index.html","title":"runIII","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII/index.html#study_da.generate.version_specific_files.runIII.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    # A knob redefinition\n    mad.input(\"on_alice := on_alice_normalized * 7000./nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000./nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=4, BFPP=False)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to 4.</p> <code>4</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],\n    ignore_cycling: bool = False,\n    slice_factor: int | None = 4,\n    BFPP: bool = False,\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to 4.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"  # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n\n    mad.input(\"\"\"\n      ! Get the toolkit\n      call,file=\n        \"acc-models-lhc/toolkit/macro.madx\";\n      \"\"\")\n\n    mad.input(\"\"\"\n      ! Build sequence\n      option, -echo,-warn,-info;\n      if (mylhcbeam==4){\n        call,file=\"acc-models-lhc/lhc_acc-models-lhc_b4.seq\";\n      } else {\n        call,file=\"acc-models-lhc/lhc_acc-models-lhc.seq\";\n      };\n      option, -echo, warn,-info;\n      \"\"\")\n\n    # Redefine macro for myslice\n    if slice_factor is not None:\n        my_slice(mad, slice_factor=slice_factor)\n\n    # Slice nominal sequence\n    mad.input(\"exec, myslice;\")\n\n    mad.input(\"\"\"\n    nrj=6800;\n    beam,particle=proton,sequence=lhcb1,energy=nrj,npart=1.15E11,sige=4.5e-4;\n    beam,particle=proton,sequence=lhcb2,energy=nrj,bv = -1,npart=1.15E11,sige=4.5e-4;\n    \"\"\")\n\n    if not ignore_cycling:\n        mad.input(\"\"\"\n        !Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n        if (mylhcbeam&lt;3){\n        seqedit, sequence=lhcb1; flatten; cycle, start=IP3; flatten; endedit;\n        };\n        seqedit, sequence=lhcb2; flatten; cycle, start=IP3; flatten; endedit;\n        \"\"\")\n\n    mad.input(\"\"\"\n        ! Set twiss formats for MAD-X parts (macro from opt. toolkit)\n        exec, twiss_opt;\n        \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:  # sourcery skip: extract-method\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-05)\n    assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-05)\n\n    try:\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=1e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=1e-01)\n\n        df = mad.table.twiss.dframe()\n        for my_ip in [1, 2, 5, 8]:\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-03)\n            assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-03)\n\n        assert df[\"x\"].std() &lt; 1e-8\n        assert df[\"y\"].std() &lt; 1e-8\n    except AssertionError:\n        logging.warning(\"WARNING: Some sanity checks have failed during the madx lattice check\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII/optics_specific_tools.html#study_da.generate.version_specific_files.runIII.optics_specific_tools.my_slice","title":"<code>my_slice(mad, slice_factor=2)</code>","text":"<p>Redefine the macro myslice for the LHC, to make a sequence thin.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>slice_factor</code> <code>int</code> <p>The slice factor. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII/optics_specific_tools.py</code> <pre><code>def my_slice(mad: Madx, slice_factor: int = 2) -&gt; None:\n    \"\"\"Redefine the macro myslice for the LHC, to make a sequence thin.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        slice_factor (int, optional): The slice factor. Defaults to 2.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(f\"slicefactor = {slice_factor};\")\n    mad.input(\"\"\"\n        myslice: macro = {\n        if (MBX.4L2-&gt;l&gt;0) {\n          select, flag=makethin, clear;\n          select, flag=makethin, class=mb, slice=2;\n          select, flag=makethin, class=mq, slice=2 * slicefactor;\n          select, flag=makethin, class=mqxa,  slice=16 * slicefactor;  !old triplet\n          select, flag=makethin, class=mqxb,  slice=16 * slicefactor;  !old triplet\n          select, flag=makethin, class=mqxc,  slice=16 * slicefactor;  !new mqxa (q1,q3)\n          select, flag=makethin, class=mqxd,  slice=16 * slicefactor;  !new mqxb (q2a,q2b)\n          select, flag=makethin, class=mqxfa, slice=16 * slicefactor;  !new (q1,q3 v1.1)\n          select, flag=makethin, class=mqxfb, slice=16 * slicefactor;  !new (q2a,q2b v1.1)\n          select, flag=makethin, class=mbxa,  slice=4;   !new d1\n          select, flag=makethin, class=mbxf,  slice=4;   !new d1 (v1.1)\n          select, flag=makethin, class=mbrd,  slice=4;   !new d2 (if needed)\n          select, flag=makethin, class=mqyy,  slice=4 * slicefactor;;   !new q4\n          select, flag=makethin, class=mqyl,  slice=4 * slicefactor;;   !new q5\n          select, flag=makethin, class=mbh,   slice=4;   !11T dipoles\n          select, flag=makethin, pattern=mbx\\.,    slice=4;\n          select, flag=makethin, pattern=mbrb\\.,   slice=4;\n          select, flag=makethin, pattern=mbrc\\.,   slice=4;\n          select, flag=makethin, pattern=mbrs\\.,   slice=4;\n          select, flag=makethin, pattern=mbh\\.,    slice=4;\n          select, flag=makethin, pattern=mqwa\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqwb\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqy\\.,    slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqm\\.,    slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqmc\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqml\\.,   slice=4 * slicefactor;\n          select, flag=makethin, pattern=mqtlh\\.,  slice=2 * slicefactor;\n          select, flag=makethin, pattern=mqtli\\.,  slice=2 * slicefactor;\n          select, flag=makethin, pattern=mqt\\.  ,  slice=2 * slicefactor;\n          !thin lens\n          option rbarc=false; beam;\n          use,sequence=lhcb1; makethin,sequence=lhcb1,makedipedge=true,style=teapot;\n          use,sequence=lhcb2; makethin,sequence=lhcb2,makedipedge=true,style=teapot;\n          option rbarc=true;\n        } else {\n          print, text=\"Sequence is already thin\";\n        };\n          is_thin=1;\n        };\n    \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/index.html","title":"runIII_ions","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/index.html#study_da.generate.version_specific_files.runIII_ions.generate_orbit_correction_setup","title":"<code>generate_orbit_correction_setup()</code>","text":"<p>Return a dictionary with the setup for the orbit correction.</p> Source code in <code>study_da/generate/version_specific_files/hllhc16/orbit_correction.py</code> <pre><code>def generate_orbit_correction_setup() -&gt; dict:\n    \"\"\"Return a dictionary with the setup for the orbit correction.\"\"\"\n    return {\n        \"lhcb1\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r8.b1\",\n                end=\"e.ds.l1.b1\",\n                vary=(\n                    \"corr_co_acbh14.l1b1\",\n                    \"corr_co_acbh12.l1b1\",\n                    \"corr_co_acbv15.l1b1\",\n                    \"corr_co_acbv13.l1b1\",\n                ),\n                targets=(\"e.ds.l1.b1\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b1\",\n                end=\"s.ds.l2.b1\",\n                vary=(\n                    \"corr_co_acbh13.r1b1\",\n                    \"corr_co_acbh15.r1b1\",\n                    \"corr_co_acbv12.r1b1\",\n                    \"corr_co_acbv14.r1b1\",\n                ),\n                targets=(\"s.ds.l2.b1\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.r4.b1\",\n                end=\"e.ds.l5.b1\",\n                vary=(\n                    \"corr_co_acbh14.l5b1\",\n                    \"corr_co_acbh12.l5b1\",\n                    \"corr_co_acbv15.l5b1\",\n                    \"corr_co_acbv13.l5b1\",\n                ),\n                targets=(\"e.ds.l5.b1\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b1\",\n                end=\"s.ds.l6.b1\",\n                vary=(\n                    \"corr_co_acbh13.r5b1\",\n                    \"corr_co_acbh15.r5b1\",\n                    \"corr_co_acbv12.r5b1\",\n                    \"corr_co_acbv14.r5b1\",\n                ),\n                targets=(\"s.ds.l6.b1\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b1\",\n                end=\"s.ds.r1.b1\",\n                vary=(\n                    \"corr_co_acbch6.l1b1\",\n                    \"corr_co_acbcv5.l1b1\",\n                    \"corr_co_acbch5.r1b1\",\n                    \"corr_co_acbcv6.r1b1\",\n                    \"corr_co_acbyhs4.l1b1\",\n                    \"corr_co_acbyhs4.r1b1\",\n                    \"corr_co_acbyvs4.l1b1\",\n                    \"corr_co_acbyvs4.r1b1\",\n                ),\n                targets=(\"ip1\", \"s.ds.r1.b1\"),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l2.b1\",\n                end=\"s.ds.r2.b1\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b1\",\n                    \"corr_co_acbchs5.r2b1\",\n                    \"corr_co_acbyvs5.l2b1\",\n                    \"corr_co_acbcvs5.r2b1\",\n                    \"corr_co_acbyhs4.l2b1\",\n                    \"corr_co_acbyhs4.r2b1\",\n                    \"corr_co_acbyvs4.l2b1\",\n                    \"corr_co_acbyvs4.r2b1\",\n                ),\n                targets=(\"ip2\", \"s.ds.r2.b1\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b1\",\n                end=\"s.ds.r5.b1\",\n                vary=(\n                    \"corr_co_acbch6.l5b1\",\n                    \"corr_co_acbcv5.l5b1\",\n                    \"corr_co_acbch5.r5b1\",\n                    \"corr_co_acbcv6.r5b1\",\n                    \"corr_co_acbyhs4.l5b1\",\n                    \"corr_co_acbyhs4.r5b1\",\n                    \"corr_co_acbyvs4.l5b1\",\n                    \"corr_co_acbyvs4.r5b1\",\n                ),\n                targets=(\"ip5\", \"s.ds.r5.b1\"),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l8.b1\",\n                end=\"s.ds.r8.b1\",\n                vary=(\n                    \"corr_co_acbch5.l8b1\",\n                    \"corr_co_acbyhs4.l8b1\",\n                    \"corr_co_acbyhs4.r8b1\",\n                    \"corr_co_acbyhs5.r8b1\",\n                    \"corr_co_acbcvs5.l8b1\",\n                    \"corr_co_acbyvs4.l8b1\",\n                    \"corr_co_acbyvs4.r8b1\",\n                    \"corr_co_acbyvs5.r8b1\",\n                ),\n                targets=(\"ip8\", \"s.ds.r8.b1\"),\n            ),\n        },\n        \"lhcb2\": {\n            \"IR1 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l1.b2\",\n                end=\"e.ds.r8.b2\",\n                vary=(\n                    \"corr_co_acbh13.l1b2\",\n                    \"corr_co_acbh15.l1b2\",\n                    \"corr_co_acbv12.l1b2\",\n                    \"corr_co_acbv14.l1b2\",\n                ),\n                targets=(\"e.ds.r8.b2\",),\n            ),\n            \"IR1 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l2.b2\",\n                end=\"s.ds.r1.b2\",\n                vary=(\n                    \"corr_co_acbh12.r1b2\",\n                    \"corr_co_acbh14.r1b2\",\n                    \"corr_co_acbv13.r1b2\",\n                    \"corr_co_acbv15.r1b2\",\n                ),\n                targets=(\"s.ds.r1.b2\",),\n            ),\n            \"IR5 left\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"e.ds.l5.b2\",\n                end=\"e.ds.r4.b2\",\n                vary=(\n                    \"corr_co_acbh13.l5b2\",\n                    \"corr_co_acbh15.l5b2\",\n                    \"corr_co_acbv12.l5b2\",\n                    \"corr_co_acbv14.l5b2\",\n                ),\n                targets=(\"e.ds.r4.b2\",),\n            ),\n            \"IR5 right\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.l6.b2\",\n                end=\"s.ds.r5.b2\",\n                vary=(\n                    \"corr_co_acbh12.r5b2\",\n                    \"corr_co_acbh14.r5b2\",\n                    \"corr_co_acbv13.r5b2\",\n                    \"corr_co_acbv15.r5b2\",\n                ),\n                targets=(\"s.ds.r5.b2\",),\n            ),\n            \"IP1\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r1.b2\",\n                end=\"e.ds.l1.b2\",\n                vary=(\n                    \"corr_co_acbch6.r1b2\",\n                    \"corr_co_acbcv5.r1b2\",\n                    \"corr_co_acbch5.l1b2\",\n                    \"corr_co_acbcv6.l1b2\",\n                    \"corr_co_acbyhs4.l1b2\",\n                    \"corr_co_acbyhs4.r1b2\",\n                    \"corr_co_acbyvs4.l1b2\",\n                    \"corr_co_acbyvs4.r1b2\",\n                ),\n                targets=(\n                    \"ip1\",\n                    \"e.ds.l1.b2\",\n                ),\n            ),\n            \"IP2\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r2.b2\",\n                end=\"e.ds.l2.b2\",\n                vary=(\n                    \"corr_co_acbyhs5.l2b2\",\n                    \"corr_co_acbchs5.r2b2\",\n                    \"corr_co_acbyvs5.l2b2\",\n                    \"corr_co_acbcvs5.r2b2\",\n                    \"corr_co_acbyhs4.l2b2\",\n                    \"corr_co_acbyhs4.r2b2\",\n                    \"corr_co_acbyvs4.l2b2\",\n                    \"corr_co_acbyvs4.r2b2\",\n                ),\n                targets=(\"ip2\", \"e.ds.l2.b2\"),\n            ),\n            \"IP5\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r5.b2\",\n                end=\"e.ds.l5.b2\",\n                vary=(\n                    \"corr_co_acbch6.r5b2\",\n                    \"corr_co_acbcv5.r5b2\",\n                    \"corr_co_acbch5.l5b2\",\n                    \"corr_co_acbcv6.l5b2\",\n                    \"corr_co_acbyhs4.l5b2\",\n                    \"corr_co_acbyhs4.r5b2\",\n                    \"corr_co_acbyvs4.l5b2\",\n                    \"corr_co_acbyvs4.r5b2\",\n                ),\n                targets=(\n                    \"ip5\",\n                    \"e.ds.l5.b2\",\n                ),\n            ),\n            \"IP8\": dict(\n                ref_with_knobs={\"on_corr_co\": 0, \"on_disp\": 0},\n                start=\"s.ds.r8.b2\",\n                end=\"e.ds.l8.b2\",\n                vary=(\n                    \"corr_co_acbchs5.l8b2\",\n                    \"corr_co_acbyhs5.r8b2\",\n                    \"corr_co_acbcvs5.l8b2\",\n                    \"corr_co_acbyvs5.r8b2\",\n                    \"corr_co_acbyhs4.l8b2\",\n                    \"corr_co_acbyhs4.r8b2\",\n                    \"corr_co_acbyvs4.l8b2\",\n                    \"corr_co_acbyvs4.r8b2\",\n                ),\n                targets=(\n                    \"ip8\",\n                    \"e.ds.l8.b2\",\n                ),\n            ),\n        },\n    }\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html","title":"optics_specific_tools","text":""},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_BFPP","title":"<code>apply_BFPP(mad)</code>","text":"<p>Apply the BFPP knob to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_BFPP(mad: Madx) -&gt; None:\n    \"\"\"Apply the BFPP knob to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(\"\"\"acbch8.r2b1        :=   6.336517325e-05 * ON_BFPP.R2 / 7.8;\n                acbch10.r2b1       :=   2.102863759e-05 * ON_BFPP.R2 / 7.8;\n                acbh12.r2b1        :=   4.404997133e-05 * ON_BFPP.R2 / 7.8;\n\n                acbch7.r1b1        :=   4.259479019e-06 * ON_BFPP.R1 / 2.5;\n                acbch9.r1b1        :=   1.794045373e-05 * ON_BFPP.R1 / 2.5;\n                acbh13.r1b1        :=   1.371178403e-05 * ON_BFPP.R1 / 2.5;\n\n                acbch7.r5b1        :=   2.153161387e-06 * ON_BFPP.R5 / 1.3;\n                acbch9.r5b1        :=   9.314782805e-06 * ON_BFPP.R5 / 1.3;\n                acbh13.r5b1        :=   7.12996247e-06 * ON_BFPP.R5 / 1.3;\n\n                acbch8.r8b1        :=   3.521812667e-05 * ON_BFPP.R8 / 4.6;\n                acbch10.r8b1       :=   1.064966564e-05 * ON_BFPP.R8 / 4.6;\n                acbh12.r8b1        :=   2.786990521e-05 * ON_BFPP.R8 / 4.6;\n            \"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_ir7_strengths","title":"<code>apply_ir7_strengths(mad)</code>","text":"<p>Apply the IR7 strengths fix to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_ir7_strengths(mad: Madx) -&gt; None:\n    \"\"\"Apply the IR7 strengths fix to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    mad.input(\"\"\"!***IR7 Optics***\n            KQ4.LR7     :=    0.131382724100E-02 ;\n            KQT4.L7     :=    0.331689344000E-03 ;\n            KQT4.R7     :=    0.331689344000E-03 ;\n            KQ5.LR7     :=   -0.133553657300E-02 ;\n            KQT5.L7     :=    0.000000000000E+00 ;\n            KQT5.R7     :=    0.000000000000E+00 ;\n\n            !Beam1\n            KQ6.L7B1    :=    0.332380383100E-02 ;\n            KQ6.R7B1    :=   -0.281821059300E-02 ;\n            KQTL7.L7B1  :=    0.307231360100E-03 ;\n            KQTL7.R7B1  :=    0.411775382800E-02 ;\n            KQTL8.L7B1  :=    0.535631538200E-03 ;\n            KQTL8.R7B1  :=    0.180061251400E-02 ;\n            KQTL9.L7B1  :=    0.104649831600E-03 ;\n            KQTL9.R7B1  :=    0.316515736800E-02 ;\n            KQTL10.L7B1 :=    0.469149843300E-02 ;\n            KQTL10.R7B1 :=    0.234006504200E-03 ;\n            KQTL11.L7B1 :=    0.109300381500E-02 ;\n            KQTL11.R7B1 :=   -0.129517571700E-03 ;\n            KQT12.L7B1  :=    0.203869506000E-02 ;\n            KQT12.R7B1  :=    0.414855502900E-03 ;\n            KQT13.L7B1  :=   -0.647047560500E-03 ;\n            KQT13.R7B1  :=    0.163470209700E-03 ;\n\n            !Beam2\n            KQ6.L7B2    :=   -0.278052285800E-02 ;\n            KQ6.R7B2    :=    0.330261896100E-02 ;\n            KQTL7.L7B2  :=    0.391109869200E-02 ;\n            KQTL7.R7B2  :=    0.307913213400E-03 ;\n            KQTL8.L7B2  :=    0.141328062600E-02 ;\n            KQTL8.R7B2  :=    0.139274871000E-02 ;\n            KQTL9.L7B2  :=    0.363516060400E-02 ;\n            KQTL9.R7B2  :=    0.692028108000E-04 ;\n            KQTL10.L7B2 :=    0.156243369200E-03 ;\n            KQTL10.R7B2 :=    0.451207010600E-02 ;\n            KQTL11.L7B2 :=    0.360602594900E-03 ;\n            KQTL11.R7B2 :=    0.131920025500E-02 ;\n            KQT12.L7B2  :=   -0.705199531300E-03 ;\n            KQT12.R7B2  :=   -0.138620184600E-02 ;\n            KQT13.L7B2  :=   -0.606647736700E-03 ;\n            KQT13.R7B2  :=   -0.585571959400E-03 ;\"\"\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.apply_optics","title":"<code>apply_optics(mad, optics_file)</code>","text":"<p>Apply the optics to the MAD-X model.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>optics_file</code> <code>str</code> <p>The path to the optics file to apply.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def apply_optics(mad: Madx, optics_file: str) -&gt; None:\n    \"\"\"Apply the optics to the MAD-X model.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        optics_file (str): The path to the optics file to apply.\n\n    Returns:\n        None\n    \"\"\"\n    mad.call(optics_file)\n    apply_ir7_strengths(mad)\n    mad.input(\"on_alice := on_alice_normalized * 7000. / nrj;\")\n    mad.input(\"on_lhcb := on_lhcb_normalized * 7000. / nrj;\")\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.build_sequence","title":"<code>build_sequence(mad, mylhcbeam, beam_config, ignore_cycling=False, slice_factor=8, BFPP=True)</code>","text":"<p>Build the sequence for the (HL-)LHC, for a given beam.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <code>mylhcbeam</code> <code>int</code> <p>The beam number (1, 2 or 4).</p> required <code>beam_config</code> <code>dict[str, Any]</code> <p>The configuration of the beam from the configuration file.</p> required <code>ignore_cycling</code> <code>bool</code> <p>Whether to ignore cycling to have IP3 at position s=0. Defaults to False.</p> <code>False</code> <code>slice_factor</code> <code>int | None</code> <p>The slice factor if optic is not thin. Defaults to 8.</p> <code>8</code> <code>BFPP</code> <code>bool</code> <p>Whether to use the BFPP knob. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def build_sequence(\n    mad: Madx,\n    mylhcbeam: int,\n    beam_config: dict[str, Any],\n    ignore_cycling: bool = False,\n    slice_factor: int | None = 8,\n    BFPP: bool = True,\n) -&gt; None:\n    \"\"\"Build the sequence for the (HL-)LHC, for a given beam.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n        mylhcbeam (int): The beam number (1, 2 or 4).\n        beam_config (dict[str, Any]): The configuration of the beam from the configuration file.\n        ignore_cycling (bool, optional): Whether to ignore cycling to have IP3 at position s=0.\n            Defaults to False.\n        slice_factor (int | None, optional): The slice factor if optic is not thin. Defaults to 8.\n        BFPP (bool, optional): Whether to use the BFPP knob. Defaults to True.\n\n    Returns:\n        None\n    \"\"\"\n    # Select beam\n    mad.input(f\"mylhcbeam = {mylhcbeam}\")\n    mad.input(\"option, -echo,warn, -info;\")\n\n    # optics dependent macros (for splitting)\n    mad.call(\"acc-models-lhc/runII/2018/toolkit/macro.madx\")\n\n    assert mylhcbeam in {1, 2, 4}, \"Invalid mylhcbeam (it should be in [1, 2, 4])\"\n\n    if mylhcbeam in {1, 2}:\n        mad.call(\"acc-models-lhc/runII/2018/lhc_as-built.seq\")\n    else:\n        mad.call(\"acc-models-lhc/runII/2018/lhcb4_as-built.seq\")\n\n    # New IR7 MQW layout and cabling\n    mad.call(\"acc-models-lhc/runIII/RunIII_dev/IR7-Run3seqedit.madx\")\n\n    # Makethin part\n    if slice_factor is not None and slice_factor &gt; 0:\n        # the variable in the macro is slice_factor\n        mad.input(f\"slicefactor={slice_factor};\")\n        mad.call(\"acc-models-lhc/runII/2018/toolkit/myslice.madx\")\n        if mylhcbeam == 1:\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb1\"], 1, beam_config[\"lhcb1\"])\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb2\"], 2, beam_config[\"lhcb2\"])\n        elif mylhcbeam == 4:\n            xm.attach_beam_to_sequence(mad.sequence[\"lhcb2\"], 4, beam_config[\"lhcb2\"])\n        else:\n            raise ValueError(\"Invalid mylhcbeam\")\n        # mad.beam()\n        for my_sequence in [\"lhcb1\", \"lhcb2\"]:\n            if my_sequence in list(mad.sequence):\n                mad.input(\n                    f\"use, sequence={my_sequence}; makethin,\"\n                    f\"sequence={my_sequence}, style=teapot, makedipedge=true;\"\n                )\n    else:\n        logging.warning(\"WARNING: The sequences are not thin!\")\n\n    # Cycling w.r.t. to IP3 (mandatory to find closed orbit in collision in the presence of errors)\n    if not ignore_cycling:\n        for my_sequence in [\"lhcb1\", \"lhcb2\"]:\n            if my_sequence in list(mad.sequence):\n                mad.input(\n                    f\"seqedit, sequence={my_sequence}; flatten;\"\n                    \"cycle, start=IP3; flatten; endedit;\"\n                )\n\n    # BFPP\n    if mylhcbeam == 1 and BFPP:\n        apply_BFPP(mad)\n</code></pre>"},{"location":"reference/study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.html#study_da.generate.version_specific_files.runIII_ions.optics_specific_tools.check_madx_lattices","title":"<code>check_madx_lattices(mad)</code>","text":"<p>Check the consistency of the MAD-X lattice for the (HL-)LHC.</p> <p>Parameters:</p> Name Type Description Default <code>mad</code> <code>Madx</code> <p>The MAD-X object used to build the sequence.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/generate/version_specific_files/runIII_ions/optics_specific_tools.py</code> <pre><code>def check_madx_lattices(mad: Madx) -&gt; None:\n    \"\"\"Check the consistency of the MAD-X lattice for the (HL-)LHC.\n\n    Args:\n        mad (Madx): The MAD-X object used to build the sequence.\n\n    Returns:\n        None\n    \"\"\"\n    assert mad.globals[\"qxb1\"] == mad.globals[\"qxb2\"]\n    assert mad.globals[\"qyb1\"] == mad.globals[\"qyb2\"]\n    assert mad.globals[\"qpxb1\"] == mad.globals[\"qpxb2\"]\n    assert mad.globals[\"qpyb1\"] == mad.globals[\"qpyb2\"]\n\n    try:\n        assert np.isclose(mad.table.summ.q1, mad.globals[\"qxb1\"], atol=1e-02)\n        assert np.isclose(mad.table.summ.q2, mad.globals[\"qyb1\"], atol=1e-02)\n        assert np.isclose(mad.table.summ.dq1, mad.globals[\"qpxb1\"], atol=5e-01)\n        assert np.isclose(mad.table.summ.dq2, mad.globals[\"qpyb1\"], atol=5e-01)\n    except AssertionError:\n        logging.warning(\n            \"Warning: some of the Qx, Qy, DQx, DQy values are not close to the expected ones\"\n        )\n\n    df = mad.table.twiss.dframe()\n    for my_ip in [1, 2, 5, 8]:\n        # assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betx_IP{my_ip}\"], rtol=1e-02)\n        # assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"bety_IP{my_ip}\"], rtol=1e-02)\n        assert np.isclose(df.loc[f\"ip{my_ip}\"].betx, mad.globals[f\"betxIP{my_ip}b1\"], rtol=1e-02)\n        assert np.isclose(df.loc[f\"ip{my_ip}\"].bety, mad.globals[f\"betyIP{my_ip}b1\"], rtol=1e-02)\n\n    mad.twiss()\n    df = mad.table.twiss.dframe()\n\n    try:\n        assert df[\"x\"].std() &lt; 1e-6\n        assert df[\"y\"].std() &lt; 1e-6\n    except AssertionError:\n        logging.warning(\"Warning: the standard deviation of x and y are not close to zero\")\n</code></pre>"},{"location":"reference/study_da/plot/index.html","title":"plot","text":""},{"location":"reference/study_da/plot/index.html#study_da.plot.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, betx_value=np.nan, bety_value=np.nan, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>betx_value</code> <code>float</code> <p>The value of the horizontal beta function. Defaults to np.nan.</p> <code>nan</code> <code>bety_value</code> <code>float</code> <p>The value of the vertical beta function. Defaults to np.nan.</p> <code>nan</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing. Defaults to \"flathv\".</p> <code>None</code> <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    betx_value: float = np.nan,\n    bety_value: float = np.nan,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        betx_value (float, optional): The value of the horizontal beta function. Defaults to np.nan.\n        bety_value (float, optional): The value of the vertical beta function. Defaults to np.nan.\n        crossing_type (str, optional): The type of crossing. Defaults to \"flathv\".\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n    # Find out what is the crossing type\n    crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data)\n    energy_str = get_energy_str(dataframe_data)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(betx_value, bety_value)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n\n    return title\n</code></pre>"},{"location":"reference/study_da/plot/index.html#study_da.plot.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', output_path='output.png', output_path_html='output.html', display_plot=True)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/index.html#study_da.plot.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, symmetric_missing=True, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.pdf', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.</p> <code>True</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.pdf'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    symmetric_missing: bool = True,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.pdf\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values with. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            raise NotImplementedError(\"Interpolation of missing values is not implemented yet\")\n\n    # Mask the lower or upper triangle\n    if mask_lower_triangle or mask_upper_triangle:\n        data_array_masked = _mask(mask_lower_triangle, mask_upper_triangle, data_array, k_masking)\n    else:\n        data_array_masked = data_array\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        plt.savefig(output_path, bbox_inches=\"tight\")\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html","title":"build_title","text":"<p>This module provides functions to compute LaTeX strings for plot titles based on data from a pandas DataFrame. The functions extract various parameters such as crossing type, LHC version, energy, bunch index, crab cavity crossing angle, bunch intensity, beta functions, crossing angles at different interaction points (IPs), bunch length, polarity, normalized emittance, chromaticity, octupole intensity, linear coupling, filling scheme, tune, luminosity, and pile-up.</p> <p>Functions:</p> Name Description <code>latex_float</code> <p>float, precision: int = 3) -&gt; str:</p> <code>get_crossing_type</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_LHC_version_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_energy_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_bunch_index_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_CC_crossing_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_bunch_intensity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_beta_str</code> <p>float, bety_value: float) -&gt; str:</p> <code>_get_plane_crossing_IP_1_5_str</code> <p>pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:</p> <code>_get_crossing_value_IP_1_5</code> <p>pd.DataFrame, ip: int) -&gt; float:</p> <code>get_crossing_IP_1_5_str</code> <p>pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:</p> <code>get_crossing_IP_2_8_str</code> <p>pd.DataFrame) -&gt; list[str]:</p> <code>get_bunch_length_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_polarity_IP_2_8_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_normalized_emittance_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_chromaticity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_octupole_intensity_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_linear_coupling_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_filling_scheme_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_tune_str</code> <p>pd.DataFrame) -&gt; str:</p> <code>get_luminosity_at_ip_str</code> <p>pd.DataFrame, ip: int, beam_beam=True) -&gt; str:</p> <code>get_PU_at_IP_str</code> <p>pd.DataFrame, ip: int, beam_beam=True) -&gt; str:</p> <code>get_title_from_configuration</code>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_CC_crossing_str","title":"<code>get_CC_crossing_str(dataframe_data)</code>","text":"<p>Retrieves the crab cavity crossing angle from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crab cavity crossing angle information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The crab cavity crossing angle string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_CC_crossing_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the crab cavity crossing angle from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crab cavity crossing angle\n            information.\n\n    Returns:\n        str: The crab cavity crossing angle string.\n    \"\"\"\n    if \"on_crab1\" in dataframe_data.columns:\n        CC_crossing_value = dataframe_data[\"on_crab1\"].unique()[0]\n        return f\"$CC = {{{CC_crossing_value:.1f}}}$ $\\mu rad$\"\n    else:\n        logging.warning(\"CC crossing not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_LHC_version_str","title":"<code>get_LHC_version_str(dataframe_data)</code>","text":"<p>Retrieves the LHC version from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing LHC version information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The LHC version string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_LHC_version_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the LHC version from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing LHC version information.\n\n    Returns:\n        str: The LHC version string.\n    \"\"\"\n    string_HL_LHC = None\n    string_LHC = None\n    if \"ver_hllhc_optics\" in dataframe_data.columns:\n        ver_hllhc_optics = dataframe_data[\"ver_hllhc_optics\"].unique()[0]\n        if ver_hllhc_optics is not None and not np.isnan(ver_hllhc_optics):\n            string_HL_LHC = f\"HL-LHC v{ver_hllhc_optics}\"\n    if \"ver_lhc_run\" in dataframe_data.columns:\n        ver_lhc_run = dataframe_data[\"ver_lhc_run\"].unique()[0]\n        if ver_lhc_run is not None and not np.isnan(ver_lhc_run):\n            string_LHC = f\"LHC Run {int(ver_lhc_run)}\"\n\n    if string_HL_LHC is not None and string_LHC is not None:\n        raise ValueError(\"Both HL-LHC and LHC Run versions found in the dataframe. Please check.\")\n    elif string_HL_LHC is not None:\n        return string_HL_LHC\n    elif string_LHC is not None:\n        return string_LHC\n    logging.warning(\"LHC version not found in the dataframe\")\n    return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_PU_at_IP_str","title":"<code>get_PU_at_IP_str(dataframe_data, ip, beam_beam=True)</code>","text":"<p>Retrieves the pile-up at a given IP from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing pile-up information.</p> required <code>ip</code> <code>int</code> <p>The IP number.</p> required <code>beam_beam</code> <code>bool</code> <p>Whether to consider beam-beam pile-up. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The pile-up string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_PU_at_IP_str(dataframe_data: pd.DataFrame, ip: int, beam_beam=True) -&gt; str:\n    \"\"\"\n    Retrieves the pile-up at a given IP from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing pile-up information.\n        ip (int): The IP number.\n        beam_beam (bool, optional): Whether to consider beam-beam pile-up. Defaults to True.\n\n    Returns:\n        str: The pile-up string.\n    \"\"\"\n    # sourcery skip: merge-else-if-into-elif, simplify-fstring-formatting\n    if beam_beam:\n        if f\"Pile-up_ip{ip}_with_beam_beam\" in dataframe_data.columns:\n            PU_value = dataframe_data[f\"Pile-up_ip{ip}_with_beam_beam\"].unique()[0]\n            return f\"$PU_{{{ip}}} \\simeq ${latex_float(float(PU_value))}\"\n        else:\n            logging.warning(f\"Pile-up at IP{ip} with beam-beam not found in the dataframe\")\n            return \"\"\n    else:\n        if f\"Pile-up_ip{ip}_without_beam_beam\" in dataframe_data.columns:\n            PU_value = dataframe_data[f\"Pile-up_ip{ip}_without_beam_beam\"].unique()[0]\n            return f\"$PU_{{{ip}}} \\simeq ${latex_float(float(PU_value))}\"\n        else:\n            logging.warning(f\"Pile-up at IP{ip} without beam-beam not found in the dataframe\")\n            return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_beta_str","title":"<code>get_beta_str(betx_value, bety_value)</code>","text":"<p>Retrieves the beta functions from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>betx_value</code> <code>float</code> <p>The value of the horizontal beta function.</p> required <code>bety_value</code> <code>float</code> <p>The value of the vertical beta function.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The beta function string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_beta_str(betx_value: float, bety_value: float) -&gt; str:\n    \"\"\"\n    Retrieves the beta functions from the dataframe.\n\n    Args:\n        betx_value (float): The value of the horizontal beta function.\n        bety_value (float): The value of the vertical beta function.\n\n    Returns:\n        str: The beta function string.\n    \"\"\"\n\n    betx_str = r\"$\\beta^{*}_{x,1}$\"\n    bety_str = r\"$\\beta^{*}_{y,1}$\"\n    return f\"{betx_str}$= {{{betx_value}}}$ m, {bety_str}\" + f\"$= {{{bety_value}}}$\" + \" m\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_index_str","title":"<code>get_bunch_index_str(dataframe_data)</code>","text":"<p>Retrieves the bunch index from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch index information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch index string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_index_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch index from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch index information.\n\n    Returns:\n        str: The bunch index string.\n    \"\"\"\n    if \"i_bunch_b1\" in dataframe_data.columns:\n        bunch_index_value = dataframe_data[\"i_bunch_b1\"].unique()[0]\n        return f\"Bunch {bunch_index_value}\"\n    else:\n        logging.warning(\"Bunch index not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_intensity_str","title":"<code>get_bunch_intensity_str(dataframe_data)</code>","text":"<p>Retrieves the bunch intensity string from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch intensity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch intensity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_intensity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch intensity string from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch intensity information.\n\n    Returns:\n        str: The bunch intensity string.\n    \"\"\"\n\n    if \"final_num_particles_per_bunch\" in dataframe_data.columns:\n        bunch_intensity_value = dataframe_data[\"final_num_particles_per_bunch\"].unique()[0]\n        return f\"$N_b \\simeq ${latex_float(float(bunch_intensity_value))} ppb\"\n    elif \"num_particles_per_bunch\" in dataframe_data.columns:\n        logging.warning(\n            \"final_num_particles_per_bunch not found in the dataframe.\"\n            \"Using num_particles_per_bunch instead.\"\n        )\n        bunch_intensity_value = dataframe_data[\"num_particles_per_bunch\"].unique()[0]\n        return f\"$N_b \\simeq ${latex_float(float(bunch_intensity_value))} ppb\"\n    else:\n        logging.warning(\"Bunch intensity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_bunch_length_str","title":"<code>get_bunch_length_str(dataframe_data)</code>","text":"<p>Retrieves the bunch length from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing bunch length information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The bunch length string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_bunch_length_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the bunch length from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing bunch length information.\n\n    Returns:\n        str: The bunch length string.\n    \"\"\"\n    if \"sigma_z\" in dataframe_data.columns:\n        bunch_length_value = dataframe_data[\"sigma_z\"].unique()[0] * 100\n        return f\"$\\sigma_{{z}} = {{{bunch_length_value}}}$ $cm$\"\n    else:\n        logging.warning(\"Bunch length not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_chromaticity_str","title":"<code>get_chromaticity_str(dataframe_data)</code>","text":"<p>Retrieves the chromaticity from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing chromaticity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The chromaticity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_chromaticity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the chromaticity from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing chromaticity information.\n\n    Returns:\n        str: The chromaticity string.\n    \"\"\"\n    if \"dqx_b1\" in dataframe_data.columns:\n        chroma_value = dataframe_data[\"dqx_b1\"].unique()[0]\n        return f\"$Q' = {{{chroma_value}}}$\"\n    else:\n        logging.warning(\"Chromaticity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_IP_1_5_str","title":"<code>get_crossing_IP_1_5_str(dataframe_data, type_crossing)</code>","text":"<p>Retrieves the crossing angle strings for IP1 and IP5.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing angle information.</p> required <code>type_crossing</code> <code>str</code> <p>The type of crossing. Either \"flathv\" or \"flatvh\".</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: The crossing angle strings for IP1 and IP5.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_IP_1_5_str(dataframe_data: pd.DataFrame, type_crossing: str) -&gt; tuple[str, str]:\n    \"\"\"\n    Retrieves the crossing angle strings for IP1 and IP5.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing angle information.\n        type_crossing (str): The type of crossing. Either \"flathv\" or \"flatvh\".\n\n    Returns:\n        tuple[str, str]: The crossing angle strings for IP1 and IP5.\n    \"\"\"\n    # Get crossing plane at IP1/5\n    phi_1_str, phi_5_str = _get_plane_crossing_IP_1_5_str(dataframe_data, type_crossing)\n\n    # Get crossing angle values at IP1 and IP5\n    xing_value_IP1 = _get_crossing_value_IP_1_5(dataframe_data, ip=1)\n    xing_value_IP5 = _get_crossing_value_IP_1_5(dataframe_data, ip=5)\n\n    # Get corresponding strings\n    xing_IP1_str = f\"{phi_1_str}$= {{{xing_value_IP1:.0f}}}$\" + \" $\\mu rad$\"\n    xing_IP5_str = f\"{phi_5_str}$= {{{xing_value_IP5:.0f}}}$\" + \" $\\mu rad$\"\n\n    return xing_IP1_str, xing_IP5_str\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_IP_2_8_str","title":"<code>get_crossing_IP_2_8_str(dataframe_data)</code>","text":"<p>Retrieves the crossing angle strings for IP2 and IP8.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing angle information.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>tuple[str, str]: The crossing angle strings for IP2 and IP8.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_IP_2_8_str(dataframe_data: pd.DataFrame) -&gt; list[str]:\n    \"\"\"\n    Retrieves the crossing angle strings for IP2 and IP8.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing angle information.\n\n    Returns:\n        tuple[str, str]: The crossing angle strings for IP2 and IP8.\n    \"\"\"\n    # First collect crossing angle values\n    dic_xing_values = {}\n    for ip in [2, 8]:\n        dic_xing_values[ip] = {}\n        for type_angle in [\"\", \"h\", \"v\"]:\n            if f\"on_x{ip}{type_angle}_final\" in dataframe_data.columns:\n                xing_value = dataframe_data[f\"on_x{ip}{type_angle}_final\"].unique()[0]\n            elif f\"on_x{ip}{type_angle}\" in dataframe_data.columns:\n                logging.warning(\n                    f\"on_x{ip}{type_angle}_final not found in the dataframe. \"\n                    f\"Using on_x{ip}{type_angle} instead.\"\n                )\n                xing_value = dataframe_data[f\"on_x{ip}{type_angle}\"].unique()[0]\n            else:\n                xing_value = 0\n            dic_xing_values[ip][type_angle] = xing_value\n\n    # Then create the strings\n    l_xing_IP_str = []\n    for ip in [2, 8]:\n        if dic_xing_values[ip][\"h\"] != 0 and dic_xing_values[ip][\"v\"] == 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},H\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['h']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"h\"] == 0 and dic_xing_values[ip][\"v\"] != 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},V\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['v']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"h\"] != 0 and dic_xing_values[ip][\"v\"] != 0:\n            logging.warning(\n                f\"It seems that the crossing angles at IP{ip} are not orthogonal... \"\n                f\"Only keeping the plane with the maximum crossing angle, but you might want to \"\n                f\"double-check this.\"\n            )\n            xing_IP_str = (\n                r\"$\\Phi/2_{\"\n                + f\"{ip},H\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['h']:.0f}}}$ $\\mu rad$\"\n                if dic_xing_values[ip][\"h\"] &gt; dic_xing_values[ip][\"v\"]\n                else r\"$\\Phi/2_{\"\n                + f\"{ip},V\"\n                + r\"}$\"\n                + f\"$= {{{dic_xing_values[ip]['v']:.0f}}}$ $\\mu rad$\"\n            )\n        elif dic_xing_values[ip][\"\"] != 0:\n            xing_IP_str = (\n                r\"$\\Phi/2_{\" + f\"{ip}\" + r\"}$\" + f\"$= {{{dic_xing_values[ip]['']:.0f}}}$ $\\mu rad$\"\n            )\n        else:\n            logging.warning(f\"Crossing angle at IP{ip} seems to be 0. Maybe double-check.\")\n            xing_IP_str = r\"$\\Phi/2_{\" + f\"{ip}\" + r\"}$\" + f\"$= 0$ $\\mu rad$\"\n        l_xing_IP_str.append(xing_IP_str)\n\n    return l_xing_IP_str\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_crossing_type","title":"<code>get_crossing_type(dataframe_data)</code>","text":"<p>Retrieves the crossing type from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing crossing type information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The crossing type string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_crossing_type(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the crossing type from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing crossing type information.\n\n    Returns:\n        str: The crossing type string.\n    \"\"\"\n    if \"optics_file\" in dataframe_data.columns:\n        optics_file = dataframe_data[\"optics_file\"].unique()[0]\n        if \"flatvh\" in optics_file:\n            return \"flatvh\"\n        elif \"flathv\" in optics_file:\n            return \"flathv\"\n\n    logging.warning(\"Crossing type not found in the dataframe. Falling back to flathv.\")\n    return \"flathv\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_energy_str","title":"<code>get_energy_str(dataframe_data)</code>","text":"<p>Retrieves the energy from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing energy information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The energy string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_energy_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the energy from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing energy information.\n\n    Returns:\n        str: The energy string.\n    \"\"\"\n    if \"beam_energy_tot_b1\" in dataframe_data.columns:\n        energy_value = dataframe_data[\"beam_energy_tot_b1\"].unique()[0] / 1000\n        return f\"$E = {{{energy_value:.1f}}}$ $TeV$\"\n    else:\n        logging.warning(\"Energy not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_filling_scheme_str","title":"<code>get_filling_scheme_str(dataframe_data)</code>","text":"<p>Retrieves the filling scheme from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing filling scheme information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The filling scheme string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_filling_scheme_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the filling scheme from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing filling scheme information.\n\n    Returns:\n        str: The filling scheme string.\n    \"\"\"\n    if \"pattern_fname\" in dataframe_data.columns:\n        filling_scheme_value = dataframe_data[\"pattern_fname\"].unique()[0]\n        # Only keep the last part of the path, which is the filling scheme\n        filling_scheme_value = filling_scheme_value.split(\"/\")[-1]\n        # Clean\n        if \"12inj\" in filling_scheme_value:\n            filling_scheme_value = filling_scheme_value.split(\"12inj\")[0] + \"12inj\"\n        return f\"{filling_scheme_value}\"\n    else:\n        logging.warning(\"Filling scheme not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_linear_coupling_str","title":"<code>get_linear_coupling_str(dataframe_data)</code>","text":"<p>Retrieves the linear coupling from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing linear coupling information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The linear coupling string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_linear_coupling_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the linear coupling from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing linear coupling information.\n\n    Returns:\n        str: The linear coupling string.\n    \"\"\"\n    if \"delta_cmr\" in dataframe_data.columns:\n        coupling_value = dataframe_data[\"delta_cmr\"].unique()[0]\n        return f\"$C^- = {{{coupling_value}}}$\"\n    else:\n        logging.warning(\"Linear coupling not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_luminosity_at_ip_str","title":"<code>get_luminosity_at_ip_str(dataframe_data, ip, beam_beam=True)</code>","text":"<p>Retrieves the luminosity at a given IP from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing luminosity information.</p> required <code>ip</code> <code>int</code> <p>The IP number.</p> required <code>beam_beam</code> <code>bool</code> <p>Whether to consider beam-beam luminosity. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The luminosity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_luminosity_at_ip_str(dataframe_data: pd.DataFrame, ip: int, beam_beam=True) -&gt; str:\n    \"\"\"\n    Retrieves the luminosity at a given IP from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing luminosity information.\n        ip (int): The IP number.\n        beam_beam (bool, optional): Whether to consider beam-beam luminosity. Defaults to True.\n\n    Returns:\n        str: The luminosity string.\n    \"\"\"\n    # sourcery skip: merge-else-if-into-elif, simplify-fstring-formatting\n    unit_luminosity = \"cm$^{-2}$s$^{-1}$\"\n    if beam_beam:\n        if f\"luminosity_ip{ip}_with_beam_beam\" in dataframe_data.columns:\n            luminosity_value = dataframe_data[f\"luminosity_ip{ip}_with_beam_beam\"].unique()[0]\n            return f\"$L_{{{ip}}} \\simeq ${latex_float(float(luminosity_value))} {unit_luminosity}\"\n        else:\n            logging.warning(f\"Luminosity at IP{ip} with beam-beam not found in the dataframe\")\n            return \"\"\n    else:\n        if f\"luminosity_ip{ip}_without_beam_beam\" in dataframe_data.columns:\n            luminosity_value = dataframe_data[f\"luminosity_ip{ip}_without_beam_beam\"].unique()[0]\n            return f\"$L_{{{ip}}} \\simeq ${latex_float(float(luminosity_value))} {unit_luminosity}\"\n        else:\n            logging.warning(f\"Luminosity at IP{ip} without beam-beam not found in the dataframe\")\n            return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_normalized_emittance_str","title":"<code>get_normalized_emittance_str(dataframe_data)</code>","text":"<p>Retrieves the normalized emittance from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing normalized emittance information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized emittance string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_normalized_emittance_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the normalized emittance from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing normalized emittance information.\n\n    Returns:\n        str: The normalized emittance string.\n    \"\"\"\n    if \"nemitt_x\" in dataframe_data.columns:\n        emittance_value = dataframe_data[\"nemitt_x\"].unique()[0] / 1e-6\n        # Round to 5 digits\n        emittance_value = round(emittance_value, 5)\n        return f\"$\\epsilon_{{n}} = {{{emittance_value}}}$ $\\mu m$\"\n    else:\n        logging.warning(\"Emittance not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_octupole_intensity_str","title":"<code>get_octupole_intensity_str(dataframe_data)</code>","text":"<p>Retrieves the octupole intensity from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing octupole intensity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The octupole intensity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_octupole_intensity_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the octupole intensity from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing octupole intensity information.\n\n    Returns:\n        str: The octupole intensity string.\n    \"\"\"\n    if \"i_oct_b1\" in dataframe_data.columns:\n        octupole_intensity_value = dataframe_data[\"i_oct_b1\"].unique()[0]\n        return f\"$I_{{OCT}} = {{{octupole_intensity_value}}}$ $A$\"\n    else:\n        logging.warning(\"Octupole intensity not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_polarity_IP_2_8_str","title":"<code>get_polarity_IP_2_8_str(dataframe_data)</code>","text":"<p>Retrieves the polarity at IP2 and IP8 from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing polarity information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The polarity string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_polarity_IP_2_8_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the polarity at IP2 and IP8 from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing polarity information.\n\n    Returns:\n        str: The polarity string.\n    \"\"\"\n    if (\n        \"on_alice_normalized\" in dataframe_data.columns\n        and \"on_lhcb_normalized\" in dataframe_data.columns\n    ):\n        polarity_value_IP2 = dataframe_data[\"on_alice_normalized\"].unique()[0]\n        polarity_value_IP8 = dataframe_data[\"on_lhcb_normalized\"].unique()[0]\n        return f\"$polarity$ $IP_{{2/8}} = {{{polarity_value_IP2}}}/{{{polarity_value_IP8}}}$\"\n    else:\n        logging.warning(\"Polarity at IP2 and IP8 not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_title_from_configuration","title":"<code>get_title_from_configuration(dataframe_data, betx_value=np.nan, bety_value=np.nan, crossing_type=None, display_LHC_version=True, display_energy=True, display_bunch_index=True, display_CC_crossing=True, display_bunch_intensity=True, display_beta=True, display_crossing_IP_1=True, display_crossing_IP_2=True, display_crossing_IP_5=True, display_crossing_IP_8=True, display_bunch_length=True, display_polarity_IP_2_8=True, display_emittance=True, display_chromaticity=True, display_octupole_intensity=True, display_coupling=True, display_filling_scheme=True, display_tune=True, display_luminosity_1=True, display_luminosity_2=True, display_luminosity_5=True, display_luminosity_8=True, display_PU_1=True, display_PU_2=True, display_PU_5=True, display_PU_8=True)</code>","text":"<p>Generates a title string from the configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing configuration data.</p> required <code>betx_value</code> <code>float</code> <p>The value of the horizontal beta function. Defaults to np.nan.</p> <code>nan</code> <code>bety_value</code> <code>float</code> <p>The value of the vertical beta function. Defaults to np.nan.</p> <code>nan</code> <code>crossing_type</code> <code>str</code> <p>The type of crossing. Defaults to \"flathv\".</p> <code>None</code> <code>display_LHC_version</code> <code>bool</code> <p>Whether to display the LHC version. Defaults to True.</p> <code>True</code> <code>display_energy</code> <code>bool</code> <p>Whether to display the energy. Defaults to True.</p> <code>True</code> <code>display_bunch_index</code> <code>bool</code> <p>Whether to display the bunch index. Defaults to True.</p> <code>True</code> <code>display_CC_crossing</code> <code>bool</code> <p>Whether to display the CC crossing. Defaults to True.</p> <code>True</code> <code>display_bunch_intensity</code> <code>bool</code> <p>Whether to display the bunch intensity. Defaults to True.</p> <code>True</code> <code>display_beta</code> <code>bool</code> <p>Whether to display the beta function. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_1</code> <code>bool</code> <p>Whether to display the crossing at IP1. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_2</code> <code>bool</code> <p>Whether to display the crossing at IP2. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_5</code> <code>bool</code> <p>Whether to display the crossing at IP5. Defaults to True.</p> <code>True</code> <code>display_crossing_IP_8</code> <code>bool</code> <p>Whether to display the crossing at IP8. Defaults to True.</p> <code>True</code> <code>display_bunch_length</code> <code>bool</code> <p>Whether to display the bunch length. Defaults to True.</p> <code>True</code> <code>display_polarity_IP_2_8</code> <code>bool</code> <p>Whether to display the polarity at IP2 and IP8. Defaults to True.</p> <code>True</code> <code>display_emittance</code> <code>bool</code> <p>Whether to display the emittance. Defaults to True.</p> <code>True</code> <code>display_chromaticity</code> <code>bool</code> <p>Whether to display the chromaticity. Defaults to True.</p> <code>True</code> <code>display_octupole_intensity</code> <code>bool</code> <p>Whether to display the octupole intensity. Defaults to True.</p> <code>True</code> <code>display_coupling</code> <code>bool</code> <p>Whether to display the coupling. Defaults to True.</p> <code>True</code> <code>display_filling_scheme</code> <code>bool</code> <p>Whether to display the filling scheme. Defaults to True.</p> <code>True</code> <code>display_tune</code> <code>bool</code> <p>Whether to display the tune. Defaults to True.</p> <code>True</code> <code>display_luminosity_1</code> <code>bool</code> <p>Whether to display the luminosity at IP1. Defaults to True.</p> <code>True</code> <code>display_luminosity_2</code> <code>bool</code> <p>Whether to display the luminosity at IP2. Defaults to True.</p> <code>True</code> <code>display_luminosity_5</code> <code>bool</code> <p>Whether to display the luminosity at IP5. Defaults to True.</p> <code>True</code> <code>display_luminosity_8</code> <code>bool</code> <p>Whether to display the luminosity at IP8. Defaults to True.</p> <code>True</code> <code>display_PU_1</code> <code>bool</code> <p>Whether to display the PU at IP1. Defaults to True.</p> <code>True</code> <code>display_PU_2</code> <code>bool</code> <p>Whether to display the PU at IP2. Defaults to True.</p> <code>True</code> <code>display_PU_5</code> <code>bool</code> <p>Whether to display the PU at IP5. Defaults to True.</p> <code>True</code> <code>display_PU_8</code> <code>bool</code> <p>Whether to display the PU at IP8. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated title string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_title_from_configuration(\n    dataframe_data: pd.DataFrame,\n    betx_value: float = np.nan,\n    bety_value: float = np.nan,\n    crossing_type: Optional[str] = None,\n    display_LHC_version: bool = True,\n    display_energy: bool = True,\n    display_bunch_index: bool = True,\n    display_CC_crossing: bool = True,\n    display_bunch_intensity: bool = True,\n    display_beta: bool = True,\n    display_crossing_IP_1: bool = True,\n    display_crossing_IP_2: bool = True,\n    display_crossing_IP_5: bool = True,\n    display_crossing_IP_8: bool = True,\n    display_bunch_length: bool = True,\n    display_polarity_IP_2_8: bool = True,\n    display_emittance: bool = True,\n    display_chromaticity: bool = True,\n    display_octupole_intensity: bool = True,\n    display_coupling: bool = True,\n    display_filling_scheme: bool = True,\n    display_tune: bool = True,\n    display_luminosity_1: bool = True,\n    display_luminosity_2: bool = True,\n    display_luminosity_5: bool = True,\n    display_luminosity_8: bool = True,\n    display_PU_1: bool = True,\n    display_PU_2: bool = True,\n    display_PU_5: bool = True,\n    display_PU_8: bool = True,\n) -&gt; str:\n    \"\"\"\n    Generates a title string from the configuration data.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing configuration data.\n        betx_value (float, optional): The value of the horizontal beta function. Defaults to np.nan.\n        bety_value (float, optional): The value of the vertical beta function. Defaults to np.nan.\n        crossing_type (str, optional): The type of crossing. Defaults to \"flathv\".\n        display_LHC_version (bool, optional): Whether to display the LHC version. Defaults to True.\n        display_energy (bool, optional): Whether to display the energy. Defaults to True.\n        display_bunch_index (bool, optional): Whether to display the bunch index. Defaults to True.\n        display_CC_crossing (bool, optional): Whether to display the CC crossing. Defaults to True.\n        display_bunch_intensity (bool, optional): Whether to display the bunch intensity. Defaults\n            to True.\n        display_beta (bool, optional): Whether to display the beta function. Defaults to True.\n        display_crossing_IP_1 (bool, optional): Whether to display the crossing at IP1. Defaults to\n            True.\n        display_crossing_IP_2 (bool, optional): Whether to display the crossing at IP2. Defaults to\n            True.\n        display_crossing_IP_5 (bool, optional): Whether to display the crossing at IP5. Defaults to\n            True.\n        display_crossing_IP_8 (bool, optional): Whether to display the crossing at IP8. Defaults to\n            True.\n        display_bunch_length (bool, optional): Whether to display the bunch length. Defaults to\n            True.\n        display_polarity_IP_2_8 (bool, optional): Whether to display the polarity at IP2 and IP8.\n            Defaults to True.\n        display_emittance (bool, optional): Whether to display the emittance. Defaults to True.\n        display_chromaticity (bool, optional): Whether to display the chromaticity.\n            Defaults to True.\n        display_octupole_intensity (bool, optional): Whether to display the octupole intensity.\n            Defaults to True.\n        display_coupling (bool, optional): Whether to display the coupling. Defaults to True.\n        display_filling_scheme (bool, optional): Whether to display the filling scheme. Defaults to\n            True.\n        display_tune (bool, optional): Whether to display the tune. Defaults to True.\n        display_luminosity_1 (bool, optional): Whether to display the luminosity at IP1. Defaults to\n            True.\n        display_luminosity_2 (bool, optional): Whether to display the luminosity at IP2. Defaults to\n            True.\n        display_luminosity_5 (bool, optional): Whether to display the luminosity at IP5. Defaults to\n            True.\n        display_luminosity_8 (bool, optional): Whether to display the luminosity at IP8. Defaults to\n            True.\n        display_PU_1 (bool, optional): Whether to display the PU at IP1. Defaults to True.\n        display_PU_2 (bool, optional): Whether to display the PU at IP2. Defaults to True.\n        display_PU_5 (bool, optional): Whether to display the PU at IP5. Defaults to True.\n        display_PU_8 (bool, optional): Whether to display the PU at IP8. Defaults to True.\n\n    Returns:\n        str: The generated title string.\n    \"\"\"\n    # Find out what is the crossing type\n    crossing_type = get_crossing_type(dataframe_data)\n\n    # Collect all the information to display\n    LHC_version_str = get_LHC_version_str(dataframe_data)\n    energy_str = get_energy_str(dataframe_data)\n    bunch_index_str = get_bunch_index_str(dataframe_data)\n    CC_crossing_str = get_CC_crossing_str(dataframe_data)\n    bunch_intensity_str = get_bunch_intensity_str(dataframe_data)\n    beta_str = get_beta_str(betx_value, bety_value)\n    xing_IP1_str, xing_IP5_str = get_crossing_IP_1_5_str(dataframe_data, crossing_type)\n    xing_IP2_str, xing_IP8_str = get_crossing_IP_2_8_str(dataframe_data)\n    bunch_length_str = get_bunch_length_str(dataframe_data)\n    polarity_str = get_polarity_IP_2_8_str(dataframe_data)\n    emittance_str = get_normalized_emittance_str(dataframe_data)\n    chromaticity_str = get_chromaticity_str(dataframe_data)\n    octupole_intensity_str = get_octupole_intensity_str(dataframe_data)\n    coupling_str = get_linear_coupling_str(dataframe_data)\n    filling_scheme_str = get_filling_scheme_str(dataframe_data)\n    tune_str = get_tune_str(dataframe_data)\n\n    # Collect luminosity and PU strings at each IP\n    dic_lumi_PU_str = {\n        \"with_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n        \"without_beam_beam\": {\"lumi\": {}, \"PU\": {}},\n    }\n    for beam_beam in [\"with_beam_beam\", \"without_beam_beam\"]:\n        for ip in [1, 2, 5, 8]:\n            dic_lumi_PU_str[beam_beam][\"lumi\"][ip] = get_luminosity_at_ip_str(\n                dataframe_data, ip, beam_beam=True\n            )\n            dic_lumi_PU_str[beam_beam][\"PU\"][ip] = get_PU_at_IP_str(\n                dataframe_data, ip, beam_beam=True\n            )\n\n    def test_if_empty_and_add_period(string: str) -&gt; str:\n        \"\"\"\n        Test if a string is empty and add a period if not.\n\n        Args:\n            string (str): The string to test.\n\n        Returns:\n            str: The string with a period if not empty.\n        \"\"\"\n        return f\"{string}. \" if string != \"\" else \"\"\n\n    # Make the final title (order is the same as in the past)\n    title = \"\"\n    if display_LHC_version:\n        title += test_if_empty_and_add_period(LHC_version_str)\n    if display_energy:\n        title += test_if_empty_and_add_period(energy_str)\n    if display_CC_crossing:\n        title += test_if_empty_and_add_period(CC_crossing_str)\n    if display_bunch_intensity:\n        title += test_if_empty_and_add_period(bunch_intensity_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][1])\n    if display_PU_1:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][1])\n    if display_luminosity_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][5])\n    if display_PU_5:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][5])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_luminosity_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][2])\n    if display_PU_2:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][2])\n    if display_luminosity_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"lumi\"][8])\n    if display_PU_8:\n        title += test_if_empty_and_add_period(dic_lumi_PU_str[\"with_beam_beam\"][\"PU\"][8])\n    # Jump to the next line\n    title += \"\\n\"\n    if display_beta:\n        title += test_if_empty_and_add_period(beta_str)\n    if display_polarity_IP_2_8:\n        title += test_if_empty_and_add_period(polarity_str)\n    if display_bunch_length:\n        title += test_if_empty_and_add_period(bunch_length_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_crossing_IP_1:\n        title += test_if_empty_and_add_period(xing_IP1_str)\n    if display_crossing_IP_5:\n        title += test_if_empty_and_add_period(xing_IP5_str)\n    if display_crossing_IP_2:\n        title += test_if_empty_and_add_period(xing_IP2_str)\n    if display_crossing_IP_8:\n        title += test_if_empty_and_add_period(xing_IP8_str)\n\n    # Jump to the next line\n    title += \"\\n\"\n    if display_emittance:\n        title += test_if_empty_and_add_period(emittance_str)\n    if display_chromaticity:\n        title += test_if_empty_and_add_period(chromaticity_str)\n    if display_octupole_intensity:\n        title += test_if_empty_and_add_period(octupole_intensity_str)\n    if display_coupling:\n        title += test_if_empty_and_add_period(coupling_str)\n    if display_tune:\n        title += test_if_empty_and_add_period(tune_str)\n    # Jump to the next line\n    title += \"\\n\"\n    if display_filling_scheme:\n        title += test_if_empty_and_add_period(filling_scheme_str)\n    if display_bunch_index:\n        title += test_if_empty_and_add_period(bunch_index_str)\n\n    return title\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.get_tune_str","title":"<code>get_tune_str(dataframe_data)</code>","text":"<p>Retrieves the tune from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing tune information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The tune string.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def get_tune_str(dataframe_data: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Retrieves the tune from the dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing tune information.\n\n    Returns:\n        str: The tune string.\n    \"\"\"\n    if \"qx_b1\" in dataframe_data.columns and \"qy_b1\" in dataframe_data.columns:\n        tune_h_value = dataframe_data[\"qx_b1\"].unique()[0]\n        tune_v_value = dataframe_data[\"qy_b1\"].unique()[0]\n        return f\"$Q_x = {tune_h_value}$, $Q_y = {tune_v_value}$\"\n    else:\n        logging.warning(\"Tune not found in the dataframe\")\n        return \"\"\n</code></pre>"},{"location":"reference/study_da/plot/build_title.html#study_da.plot.build_title.latex_float","title":"<code>latex_float(f, precision=3)</code>","text":"<p>Converts a float to a scientific LaTeX format string.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>float</code> <p>The float to convert.</p> required <code>precision</code> <code>int</code> <p>The precision of the float. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The float in scientific LaTeX format.</p> Source code in <code>study_da/plot/build_title.py</code> <pre><code>def latex_float(f: float, precision: int = 3) -&gt; str:\n    \"\"\"\n    Converts a float to a scientific LaTeX format string.\n\n    Args:\n        f (float): The float to convert.\n        precision (int, optional): The precision of the float. Defaults to 3.\n\n    Returns:\n        str: The float in scientific LaTeX format.\n    \"\"\"\n    float_str = \"{0:.{1}g}\".format(f, precision)\n\n    # In case the float is an integer, don't use scientific notation\n    if \"e\" not in float_str:\n        return float_str\n\n    # Otherwise, split the float into base and exponent\n    base, exponent = float_str.split(\"e\")\n    return r\"${0} \\times 10^{{{1}}}$\".format(base, int(exponent))\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html","title":"plot_study","text":"<p>This module provides functions to create and customize study plots, including heatmaps and 3D volume renderings.</p> <p>Functions:</p> Name Description <code>_set_style</code> <code>_add_text_annotation</code> <code>_smooth</code> <code>_mask</code> <code>_add_contours</code> <code>_add_diagonal_lines</code> <code>add_QR_code</code> <code>_set_labels</code> <code>plot_heatmap</code> <code>plot_3D</code>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.add_QR_code","title":"<code>add_QR_code(fig, link, position_qr='top-right')</code>","text":"<p>Adds a QR code pointing to the given link to the figure.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to add the QR code to.</p> required <code>link</code> <code>str</code> <p>The link to encode in the QR code.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The figure with the QR code.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def add_QR_code(fig: plt.Figure, link: str, position_qr=\"top-right\") -&gt; plt.Figure:\n    \"\"\"\n    Adds a QR code pointing to the given link to the figure.\n\n    Args:\n        fig (plt.Figure): The figure to add the QR code to.\n        link (str): The link to encode in the QR code.\n\n    Returns:\n        plt.Figure: The figure with the QR code.\n    \"\"\"\n    # Add QR code pointing to the github repository\n    qr = qrcode.QRCode(\n        # version=None,\n        box_size=10,\n        border=1,\n    )\n    qr.add_data(link)\n    qr.make(fit=False)\n    im = qr.make_image(fill_color=\"black\", back_color=\"transparent\")\n    if position_qr == \"top-right\":\n        newax = fig.add_axes([0.9, 0.9, 0.05, 0.05], anchor=\"NE\", zorder=1)\n    elif position_qr == \"bottom-right\":\n        newax = fig.add_axes([0.9, 0.1, 0.05, 0.05], anchor=\"SE\", zorder=1)\n    elif position_qr == \"bottom-left\":\n        newax = fig.add_axes([0.1, 0.1, 0.05, 0.05], anchor=\"SW\", zorder=1)\n    elif position_qr == \"top-left\":\n        newax = fig.add_axes([0.1, 0.9, 0.05, 0.05], anchor=\"NW\", zorder=1)\n    else:\n        raise ValueError(f\"Position {position_qr} not recognized\")\n    newax.imshow(im, resample=False, interpolation=\"none\", filternorm=False)\n    # Add link below qrcode\n    newax.plot([0, 0], [0, 0], color=\"white\", label=\"link\")\n    _ = newax.annotate(\n        \"lin\",\n        xy=(0, 300),\n        xytext=(0, 300),\n        fontsize=30,\n        url=link,\n        bbox=dict(color=\"white\", alpha=1e-6, url=link),\n        alpha=0,\n    )\n    # Hide X and Y axes label marks\n    newax.xaxis.set_tick_params(labelbottom=False)\n    newax.yaxis.set_tick_params(labelleft=False)\n    # Hide X and Y axes tick marks\n    newax.set_xticks([])\n    newax.set_yticks([])\n    newax.set_axis_off()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.plot_3D","title":"<code>plot_3D(dataframe_data, x_variable, y_variable, z_variable, color_variable, xlabel=None, ylabel=None, z_label=None, title='', vmin=4.5, vmax=7.5, surface_count=30, opacity=0.2, figsize=(1000, 1000), colormap='RdBu', output_path='output.png', output_path_html='output.html', display_plot=True)</code>","text":"<p>Plots a 3D volume rendering from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>x_variable</code> <code>str</code> <p>The variable to plot on the x-axis.</p> required <code>y_variable</code> <code>str</code> <p>The variable to plot on the y-axis.</p> required <code>z_variable</code> <code>str</code> <p>The variable to plot on the z-axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>z_label</code> <code>Optional[str]</code> <p>The label for the z-axis. Defaults to None.</p> <code>None</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>surface_count</code> <code>int</code> <p>The number of surfaces for volume rendering. Defaults to 30.</p> <code>30</code> <code>opacity</code> <code>float</code> <p>The opacity of the volume rendering. Defaults to 0.2.</p> <code>0.2</code> <code>figsize</code> <code>tuple[float, float]</code> <p>The size of the figure. Defaults to (1000, 1000).</p> <code>(1000, 1000)</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"RdBu\".</p> <code>'RdBu'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot image. Defaults to \"output.png\".</p> <code>'output.png'</code> <code>output_path_html</code> <code>str</code> <p>The path to save the plot HTML. Defaults to \"output.html\".</p> <code>'output.html'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>go.Figure: The plotly figure object.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_3D(\n    dataframe_data: pd.DataFrame,\n    x_variable: str,\n    y_variable: str,\n    z_variable: str,\n    color_variable: str,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    z_label: Optional[str] = None,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    surface_count: int = 30,\n    opacity: float = 0.2,\n    figsize: tuple[float, float] = (1000, 1000),\n    colormap: str = \"RdBu\",\n    output_path: str = \"output.png\",\n    output_path_html: str = \"output.html\",\n    display_plot: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Plots a 3D volume rendering from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        x_variable (str): The variable to plot on the x-axis.\n        y_variable (str): The variable to plot on the y-axis.\n        z_variable (str): The variable to plot on the z-axis.\n        color_variable (str): The variable to use for the color scale.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        z_label (Optional[str], optional): The label for the z-axis. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        surface_count (int, optional): The number of surfaces for volume rendering. Defaults to 30.\n        opacity (float, optional): The opacity of the volume rendering. Defaults to 0.2.\n        figsize (tuple[float, float], optional): The size of the figure. Defaults to (1000, 1000).\n        colormap (str, optional): The colormap to use. Defaults to \"RdBu\".\n        output_path (str, optional): The path to save the plot image. Defaults to \"output.png\".\n        output_path_html (str, optional): The path to save the plot HTML. Defaults to \"output.html\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n\n    Returns:\n        go.Figure: The plotly figure object.\n    \"\"\"\n    # Check if plotly is installed\n    try:\n        import plotly.graph_objects as go\n    except ImportError as e:\n        raise ImportError(\"Please install plotly to use this function\") from e\n\n    X = np.array(dataframe_data[x_variable])\n    Y = np.array(dataframe_data[y_variable])\n    Z = np.array(dataframe_data[z_variable])\n    values = np.array(dataframe_data[color_variable])\n    fig = go.Figure(\n        data=go.Volume(\n            x=X.flatten(),\n            y=Y.flatten(),\n            z=Z.flatten(),\n            value=values.flatten(),\n            isomin=vmin,\n            isomax=vmax,\n            opacity=opacity,  # needs to be small to see through all surfaces\n            surface_count=surface_count,  # needs to be a large number for good volume rendering\n            colorscale=colormap,\n        )\n    )\n\n    fig.update_layout(\n        scene_xaxis_title_text=xlabel,\n        scene_yaxis_title_text=ylabel,\n        scene_zaxis_title_text=z_label,\n        title=title,\n    )\n\n    # Center the title\n    fig.update_layout(title_x=0.5, title_y=0.9, title_xanchor=\"center\", title_yanchor=\"top\")\n\n    # Specify the width and height of the figure\n    fig.update_layout(width=figsize[0], height=figsize[1])\n\n    # Display/save/return the figure\n    if output_path is not None:\n        fig.write_image(output_path)\n\n    if output_path_html is not None:\n        fig.write_html(output_path_html)\n\n    if display_plot:\n        fig.show()\n\n    return fig\n</code></pre>"},{"location":"reference/study_da/plot/plot_study.html#study_da.plot.plot_study.plot_heatmap","title":"<code>plot_heatmap(dataframe_data, horizontal_variable, vertical_variable, color_variable, link=None, position_qr='top-right', plot_contours=True, xlabel=None, ylabel=None, symmetric_missing=True, mask_lower_triangle=False, mask_upper_triangle=False, plot_diagonal_lines=True, shift_diagonal_lines=1, xaxis_ticks_on_top=True, title='', vmin=4.5, vmax=7.5, k_masking=-1, green_contour=6.0, min_level_contours=1, max_level_contours=15, delta_levels_contours=0.5, figsize=None, label_cbar='Minimum DA (' + '$\\\\sigma$' + ')', colormap='coolwarm_r', style='ggplot', output_path='output.pdf', display_plot=True, latex_fonts=True, vectorize=False, fill_missing_value_with=None)</code>","text":"<p>Plots a heatmap from the given dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe_data</code> <code>DataFrame</code> <p>The dataframe containing the data to plot.</p> required <code>horizontal_variable</code> <code>str</code> <p>The variable to plot on the horizontal axis.</p> required <code>vertical_variable</code> <code>str</code> <p>The variable to plot on the vertical axis.</p> required <code>color_variable</code> <code>str</code> <p>The variable to use for the color scale.</p> required <code>link</code> <code>Optional[str]</code> <p>A link to encode in a QR code. Defaults to None.</p> <code>None</code> <code>plot_contours</code> <code>bool</code> <p>Whether to plot contours. Defaults to True.</p> <code>True</code> <code>xlabel</code> <code>Optional[str]</code> <p>The label for the x-axis. Defaults to None.</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>The label for the y-axis. Defaults to None.</p> <code>None</code> <code>symmetric_missing</code> <code>bool</code> <p>Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.</p> <code>True</code> <code>mask_lower_triangle</code> <code>bool</code> <p>Whether to mask the lower triangle. Defaults to False.</p> <code>False</code> <code>mask_upper_triangle</code> <code>bool</code> <p>Whether to mask the upper triangle. Defaults to False.</p> <code>False</code> <code>plot_diagonal_lines</code> <code>bool</code> <p>Whether to plot diagonal lines. Defaults to True.</p> <code>True</code> <code>shift_diagonal_lines</code> <code>int</code> <p>The shift for the diagonal lines. Defaults to 1.</p> <code>1</code> <code>xaxis_ticks_on_top</code> <code>bool</code> <p>Whether to place the x-axis ticks on top. Defaults to True.</p> <code>True</code> <code>title</code> <code>str</code> <p>The title of the plot. Defaults to \"\".</p> <code>''</code> <code>vmin</code> <code>float</code> <p>The minimum value for the color scale. Defaults to 4.5.</p> <code>4.5</code> <code>vmax</code> <code>float</code> <p>The maximum value for the color scale. Defaults to 7.5.</p> <code>7.5</code> <code>k_masking</code> <code>int</code> <p>The k parameter for masking. Defaults to -1.</p> <code>-1</code> <code>green_contour</code> <code>Optional[float]</code> <p>The value for the green contour line. Defaults to 6.0.</p> <code>6.0</code> <code>min_level_contours</code> <code>float</code> <p>The minimum level for the contours. Defaults to 1.</p> <code>1</code> <code>max_level_contours</code> <code>float</code> <p>The maximum level for the contours. Defaults to 15.</p> <code>15</code> <code>delta_levels_contours</code> <code>float</code> <p>The delta between contour levels. Defaults to 0.5.</p> <code>0.5</code> <code>figsize</code> <code>Optional[tuple[float, float]]</code> <p>The size of the figure. Defaults to None.</p> <code>None</code> <code>label_cbar</code> <code>str</code> <p>The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".</p> <code>'Minimum DA (' + '$\\\\sigma$' + ')'</code> <code>colormap</code> <code>str</code> <p>The colormap to use. Defaults to \"coolwarm_r\".</p> <code>'coolwarm_r'</code> <code>style</code> <code>str</code> <p>The style to use for the plot. Defaults to \"ggplot\".</p> <code>'ggplot'</code> <code>output_path</code> <code>str</code> <p>The path to save the plot. Defaults to \"output.pdf\".</p> <code>'output.pdf'</code> <code>display_plot</code> <code>bool</code> <p>Whether to display the plot. Defaults to True.</p> <code>True</code> <code>latex_fonts</code> <code>bool</code> <p>Whether to use LaTeX fonts. Defaults to True.</p> <code>True</code> <code>vectorize</code> <code>bool</code> <p>Whether to vectorize the plot. Defaults to False.</p> <code>False</code> <code>fill_missing_value_with</code> <code>Optional[str | float]</code> <p>The value to fill missing values with. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> Source code in <code>study_da/plot/plot_study.py</code> <pre><code>def plot_heatmap(\n    dataframe_data: pd.DataFrame,\n    horizontal_variable: str,\n    vertical_variable: str,\n    color_variable: str,\n    link: Optional[str] = None,\n    position_qr: Optional[str] = \"top-right\",\n    plot_contours: bool = True,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    symmetric_missing: bool = True,\n    mask_lower_triangle: bool = False,\n    mask_upper_triangle: bool = False,\n    plot_diagonal_lines: bool = True,\n    shift_diagonal_lines: int = 1,\n    xaxis_ticks_on_top: bool = True,\n    title: str = \"\",\n    vmin: float = 4.5,\n    vmax: float = 7.5,\n    k_masking: int = -1,\n    green_contour: Optional[float] = 6.0,\n    min_level_contours: float = 1,\n    max_level_contours: float = 15,\n    delta_levels_contours: float = 0.5,\n    figsize: Optional[tuple[float, float]] = None,\n    label_cbar: str = \"Minimum DA (\" + r\"$\\sigma$\" + \")\",\n    colormap: str = \"coolwarm_r\",\n    style: str = \"ggplot\",\n    output_path: str = \"output.pdf\",\n    display_plot: bool = True,\n    latex_fonts: bool = True,\n    vectorize: bool = False,\n    fill_missing_value_with: Optional[str | float] = None,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"\n    Plots a heatmap from the given dataframe.\n\n    Args:\n        dataframe_data (pd.DataFrame): The dataframe containing the data to plot.\n        horizontal_variable (str): The variable to plot on the horizontal axis.\n        vertical_variable (str): The variable to plot on the vertical axis.\n        color_variable (str): The variable to use for the color scale.\n        link (Optional[str], optional): A link to encode in a QR code. Defaults to None.\n        plot_contours (bool, optional): Whether to plot contours. Defaults to True.\n        xlabel (Optional[str], optional): The label for the x-axis. Defaults to None.\n        ylabel (Optional[str], optional): The label for the y-axis. Defaults to None.\n        symmetric_missing (bool, optional): Whether to make the matrix symmetric by replacing the lower triangle with the upper triangle. Defaults to True.\n        mask_lower_triangle (bool, optional): Whether to mask the lower triangle. Defaults to False.\n        mask_upper_triangle (bool, optional): Whether to mask the upper triangle. Defaults to False.\n        plot_diagonal_lines (bool, optional): Whether to plot diagonal lines. Defaults to True.\n        shift_diagonal_lines (int, optional): The shift for the diagonal lines. Defaults to 1.\n        xaxis_ticks_on_top (bool, optional): Whether to place the x-axis ticks on top. Defaults to True.\n        title (str, optional): The title of the plot. Defaults to \"\".\n        vmin (float, optional): The minimum value for the color scale. Defaults to 4.5.\n        vmax (float, optional): The maximum value for the color scale. Defaults to 7.5.\n        k_masking (int, optional): The k parameter for masking. Defaults to -1.\n        green_contour (Optional[float], optional): The value for the green contour line. Defaults to 6.0.\n        min_level_contours (float, optional): The minimum level for the contours. Defaults to 1.\n        max_level_contours (float, optional): The maximum level for the contours. Defaults to 15.\n        delta_levels_contours (float, optional): The delta between contour levels. Defaults to 0.5.\n        figsize (Optional[tuple[float, float]], optional): The size of the figure. Defaults to None.\n        label_cbar (str, optional): The label for the colorbar. Defaults to \"Minimum DA ($\\sigma$)\".\n        colormap (str, optional): The colormap to use. Defaults to \"coolwarm_r\".\n        style (str, optional): The style to use for the plot. Defaults to \"ggplot\".\n        output_path (str, optional): The path to save the plot. Defaults to \"output.pdf\".\n        display_plot (bool, optional): Whether to display the plot. Defaults to True.\n        latex_fonts (bool, optional): Whether to use LaTeX fonts. Defaults to True.\n        vectorize (bool, optional): Whether to vectorize the plot. Defaults to False.\n        fill_missing_value_with (Optional[str | float], optional): The value to fill missing values with. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n    \"\"\"\n    # Use the requested style\n    _set_style(style, latex_fonts, vectorize)\n\n    # Get the dataframe to plot\n    df_to_plot = dataframe_data.pivot(\n        index=vertical_variable, columns=horizontal_variable, values=color_variable\n    )\n\n    # Get numpy array from dataframe\n    data_array = df_to_plot.to_numpy(dtype=float)\n\n    # Replace NaNs with a value if requested\n    if fill_missing_value_with is not None:\n        if isinstance(fill_missing_value_with, (int, float)):\n            data_array[np.isnan(data_array)] = fill_missing_value_with\n        elif fill_missing_value_with == \"interpolate\":\n            raise NotImplementedError(\"Interpolation of missing values is not implemented yet\")\n\n    # Mask the lower or upper triangle\n    if mask_lower_triangle or mask_upper_triangle:\n        data_array_masked = _mask(mask_lower_triangle, mask_upper_triangle, data_array, k_masking)\n    else:\n        data_array_masked = data_array\n\n    # Define colormap and set NaNs to white\n    cmap = matplotlib.colormaps.get_cmap(colormap)\n    cmap.set_bad(\"w\")\n\n    # Build heatmap, with inverted y axis\n    fig, ax = plt.subplots()\n    if figsize is not None:\n        fig.set_size_inches(figsize)\n    im = ax.imshow(data_array_masked, cmap=cmap, vmin=vmin, vmax=vmax)\n    ax.invert_yaxis()\n\n    # Add text annotations\n    ax = _add_text_annotation(df_to_plot, data_array, ax, vmin, vmax)\n\n    # Smooth data for contours\n    mx = _smooth(data_array, symmetric_missing)\n\n    # Plot contours if requested\n    if plot_contours:\n        ax = _add_contours(\n            ax,\n            data_array,\n            mx,\n            green_contour,\n            min_level_contours,\n            max_level_contours,\n            delta_levels_contours,\n        )\n\n    if plot_diagonal_lines:\n        # Diagonal lines must be plotted after the contour lines, because of bug in matplotlib\n        # Shift might need to be adjusted\n        ax = _add_diagonal_lines(ax, shift=shift_diagonal_lines)\n\n    # Define title and axis labels\n    ax.set_title(\n        title,\n        fontsize=10,\n    )\n\n    # Set axis labels\n    ax = _set_labels(\n        ax,\n        df_to_plot,\n        data_array,\n        horizontal_variable,\n        vertical_variable,\n        xlabel,\n        ylabel,\n        xaxis_ticks_on_top,\n    )\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, fraction=0.026, pad=0.04)\n    cbar.ax.set_ylabel(label_cbar, rotation=90, va=\"bottom\", labelpad=15)\n\n    # Remove potential grid\n    plt.grid(visible=None)\n\n    # Add QR code with a link to the topright side (a bit experimental, might need adjustments)\n    if link is not None:\n        fig = add_QR_code(fig, link, position_qr)\n\n    # Save and potentially display the plot\n    if output_path is not None:\n        plt.savefig(output_path, bbox_inches=\"tight\")\n\n    if display_plot:\n        plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html","title":"utils","text":""},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_high_quality","title":"<code>apply_high_quality(vectorial=False)</code>","text":"<p>Sets the matplotlib output format to high quality or vectorial plots.</p> <p>Parameters:</p> Name Type Description Default <code>vectorial</code> <code>bool</code> <p>If True, sets the format to 'svg'. Otherwise, sets it to 'retina'. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_high_quality(vectorial: bool = False) -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to high quality or vectorial plots.\n\n    Args:\n        vectorial (bool, optional): If True, sets the format to 'svg'. Otherwise, sets it to\n            'retina'. Defaults to False.\n    \"\"\"\n    if vectorial:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n    else:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_nicer_style","title":"<code>apply_nicer_style(remove_right_upper_spines=True)</code>","text":"<p>Applies a nicer style to plots, using the whitegrid seaborn theme.</p> <p>Parameters:</p> Name Type Description Default <code>remove_right_upper_spines</code> <code>bool</code> <p>If True, removes the right and upper spines from the plots. Defaults to True.</p> <code>True</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_nicer_style(remove_right_upper_spines: bool = True) -&gt; None:\n    \"\"\"\n    Applies a nicer style to plots, using the whitegrid seaborn theme.\n\n    Args:\n        remove_right_upper_spines (bool, optional): If True, removes the right and upper spines\n            from the plots. Defaults to True.\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n\n    if remove_right_upper_spines:\n        custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n        sns.set_theme(style=\"ticks\", rc=custom_params)\n\n    # sns.set(font='Adobe Devanagari')\n    sns.set_context(\"paper\", font_scale=1, rc={\"lines.linewidth\": 0.5, \"grid.linewidth\": 0.3})\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.apply_standard_quality","title":"<code>apply_standard_quality()</code>","text":"<p>Sets the matplotlib output format to standard quality (\"png\" argument).</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_standard_quality() -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to standard quality (\"png\" argument).\n    \"\"\"\n    matplotlib_inline.backend_inline.set_matplotlib_formats(\"png\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.use_default_fonts","title":"<code>use_default_fonts()</code>","text":"<p>Configures matplotlib to use the default DejaVu Sans fonts.</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_default_fonts() -&gt; None:\n    \"\"\"\n    Configures matplotlib to use the default DejaVu Sans fonts.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"dejavusans\"\n    matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n    matplotlib.rcParams[\"mathtext.default\"] = \"it\"\n    matplotlib.rcParams[\"font.weight\"] = \"normal\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/index.html#study_da.plot.utils.use_latex_fonts","title":"<code>use_latex_fonts(italic=False)</code>","text":"<p>Configures matplotlib to use LaTeX fonts.</p> <p>Parameters:</p> Name Type Description Default <code>italic</code> <code>bool</code> <p>If True, sets the default mathtext to italic. Otherwise, sets it to regular. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_latex_fonts(italic: bool = False) -&gt; None:\n    \"\"\"\n    Configures matplotlib to use LaTeX fonts.\n\n    Args:\n        italic (bool, optional): If True, sets the default mathtext to italic. Otherwise, sets it\n            to regular. Defaults to False.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n    matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n    if not italic:\n        matplotlib.rcParams[\"mathtext.default\"] = \"regular\"\n        matplotlib.rcParams[\"font.weight\"] = \"light\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html","title":"maplotlib_utils","text":""},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_high_quality","title":"<code>apply_high_quality(vectorial=False)</code>","text":"<p>Sets the matplotlib output format to high quality or vectorial plots.</p> <p>Parameters:</p> Name Type Description Default <code>vectorial</code> <code>bool</code> <p>If True, sets the format to 'svg'. Otherwise, sets it to 'retina'. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_high_quality(vectorial: bool = False) -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to high quality or vectorial plots.\n\n    Args:\n        vectorial (bool, optional): If True, sets the format to 'svg'. Otherwise, sets it to\n            'retina'. Defaults to False.\n    \"\"\"\n    if vectorial:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n    else:\n        matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_nicer_style","title":"<code>apply_nicer_style(remove_right_upper_spines=True)</code>","text":"<p>Applies a nicer style to plots, using the whitegrid seaborn theme.</p> <p>Parameters:</p> Name Type Description Default <code>remove_right_upper_spines</code> <code>bool</code> <p>If True, removes the right and upper spines from the plots. Defaults to True.</p> <code>True</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_nicer_style(remove_right_upper_spines: bool = True) -&gt; None:\n    \"\"\"\n    Applies a nicer style to plots, using the whitegrid seaborn theme.\n\n    Args:\n        remove_right_upper_spines (bool, optional): If True, removes the right and upper spines\n            from the plots. Defaults to True.\n    \"\"\"\n    sns.set_theme(style=\"whitegrid\")\n\n    if remove_right_upper_spines:\n        custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n        sns.set_theme(style=\"ticks\", rc=custom_params)\n\n    # sns.set(font='Adobe Devanagari')\n    sns.set_context(\"paper\", font_scale=1, rc={\"lines.linewidth\": 0.5, \"grid.linewidth\": 0.3})\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.apply_standard_quality","title":"<code>apply_standard_quality()</code>","text":"<p>Sets the matplotlib output format to standard quality (\"png\" argument).</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def apply_standard_quality() -&gt; None:\n    \"\"\"\n    Sets the matplotlib output format to standard quality (\"png\" argument).\n    \"\"\"\n    matplotlib_inline.backend_inline.set_matplotlib_formats(\"png\")\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.use_default_fonts","title":"<code>use_default_fonts()</code>","text":"<p>Configures matplotlib to use the default DejaVu Sans fonts.</p> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_default_fonts() -&gt; None:\n    \"\"\"\n    Configures matplotlib to use the default DejaVu Sans fonts.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"dejavusans\"\n    matplotlib.rcParams[\"font.family\"] = \"DejaVu Sans\"\n    matplotlib.rcParams[\"mathtext.default\"] = \"it\"\n    matplotlib.rcParams[\"font.weight\"] = \"normal\"\n</code></pre>"},{"location":"reference/study_da/plot/utils/maplotlib_utils.html#study_da.plot.utils.maplotlib_utils.use_latex_fonts","title":"<code>use_latex_fonts(italic=False)</code>","text":"<p>Configures matplotlib to use LaTeX fonts.</p> <p>Parameters:</p> Name Type Description Default <code>italic</code> <code>bool</code> <p>If True, sets the default mathtext to italic. Otherwise, sets it to regular. Defaults to False.</p> <code>False</code> Source code in <code>study_da/plot/utils/maplotlib_utils.py</code> <pre><code>def use_latex_fonts(italic: bool = False) -&gt; None:\n    \"\"\"\n    Configures matplotlib to use LaTeX fonts.\n\n    Args:\n        italic (bool, optional): If True, sets the default mathtext to italic. Otherwise, sets it\n            to regular. Defaults to False.\n    \"\"\"\n    matplotlib.rcParams[\"mathtext.fontset\"] = \"cm\"\n    matplotlib.rcParams[\"font.family\"] = \"STIXGeneral\"\n    if not italic:\n        matplotlib.rcParams[\"mathtext.default\"] = \"regular\"\n        matplotlib.rcParams[\"font.weight\"] = \"light\"\n</code></pre>"},{"location":"reference/study_da/postprocess/index.html","title":"postprocess","text":""},{"location":"reference/study_da/postprocess/index.html#study_da.postprocess.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output, l_group_by_parameters, only_keep_lost_particles, l_parameters_to_keep\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html","title":"postprocess","text":"<p>This module provides functions to process and analyze simulation output data.</p> <p>Functions:</p> Name Description <code>get_particles_data</code> <code>add_parameters_from_config</code> <code>merge_and_group_by_parameters_of_interest</code> <code>aggregate_output_data</code> <code>fix_LHC_version</code>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.add_parameters_from_config","title":"<code>add_parameters_from_config(l_df_output, dic_parameters_of_interest, default_path_template_parameters=False)</code>","text":"<p>Adds parameters from the configuration to the output data.</p> <p>Parameters:</p> Name Type Description Default <code>l_df_output</code> <code>list</code> <p>List of DataFrames containing the output data.</p> required <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest.</p> required <code>default_path_template_parameters</code> <code>bool</code> <p>Flag to indicate if the default path template parameters are used. If True, less caution is applied in the checking of the parameters. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[DataFrame]</code> <p>A list of DataFrames with added parameters.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def add_parameters_from_config(\n    l_df_output: List[pd.DataFrame],\n    dic_parameters_of_interest: Dict[str, List[str]],\n    default_path_template_parameters: bool = False,\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Adds parameters from the configuration to the output data.\n\n    Args:\n        l_df_output (list): List of DataFrames containing the output data.\n        dic_parameters_of_interest (dict): Dictionary of parameters of interest.\n        default_path_template_parameters (bool, optional): Flag to indicate if the default path\n            template parameters are used. If True, less caution is applied in the checking of the\n            parameters. Defaults to False.\n\n    Returns:\n        list: A list of DataFrames with added parameters.\n    \"\"\"\n\n    for df_output in l_df_output:\n        # Get generation configurations as dictionnaries for parameter assignation\n        dic_configuration = df_output.attrs[\"configuration\"]\n\n        # Select simulations parameters of interest\n        for name_param, l_path_param in dic_parameters_of_interest.items():\n            try:\n                df_output[name_param] = nested_get(dic_configuration, l_path_param)\n            except KeyError:\n                # Only be verbose if the dic_parameters_of_interest has not been provided by the user\n                if not default_path_template_parameters:\n                    logging.warning(f\"Parameter {name_param} not found in the configuration file\")\n\n    return l_df_output\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.aggregate_output_data","title":"<code>aggregate_output_data(path_tree, l_group_by_parameters, generation_of_interest=2, name_output='output_particles.parquet', write_output=True, path_output=None, only_keep_lost_particles=True, dic_parameters_of_interest=None, l_parameters_to_keep=None, name_template_parameters='parameters_lhc.yaml', path_template_parameters=None, force_overwrite=False)</code>","text":"<p>Aggregates output data from simulation files.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree file.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <code>write_output</code> <code>bool</code> <p>Flag to indicate if the output should be written to a file. Defaults to True.</p> <code>True</code> <code>path_output</code> <code>str</code> <p>The path to the output file. If not provided, the default output file will be in the study folder as 'da.parquet'. Defaults to None.</p> <code>None</code> <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>dic_parameters_of_interest</code> <code>dict</code> <p>Dictionary of parameters of interest. Defaults to None.</p> <code>None</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <code>name_template_parameters</code> <code>str</code> <p>The name of the template parameters file associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which is already contained in the study-da package, and includes the main usual parameters.</p> <code>'parameters_lhc.yaml'</code> <code>path_template_parameters</code> <code>str</code> <p>The path to the template parameters file. Must be provided if a no template already contained in study-da is provided through the argument name_template_parameters. Defaults to None.</p> <code>None</code> <code>force_overwrite</code> <code>bool</code> <p>Flag to indicate if the output file should be overwritten if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The final aggregated DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def aggregate_output_data(\n    path_tree: str,\n    l_group_by_parameters: List[str],\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n    write_output: bool = True,\n    path_output: Optional[str] = None,\n    only_keep_lost_particles: bool = True,\n    dic_parameters_of_interest: Optional[Dict[str, List[str]]] = None,\n    l_parameters_to_keep: Optional[List[str]] = None,\n    name_template_parameters: str = \"parameters_lhc.yaml\",\n    path_template_parameters: Optional[str] = None,\n    force_overwrite: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates output data from simulation files.\n\n    Args:\n        path_tree (str): The path to the tree file.\n        l_group_by_parameters (list): List of parameters to group by.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file. Defaults to \"output_particles.parquet\".\n        write_output (bool, optional): Flag to indicate if the output should be written to a file.\n            Defaults to True.\n        path_output (str, optional): The path to the output file. If not provided, the default\n            output file will be in the study folder as 'da.parquet'. Defaults to None.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should be\n            kept. Defaults to True.\n        dic_parameters_of_interest (dict, optional): Dictionary of parameters of interest. Defaults\n            to None.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n        name_template_parameters (str, optional): The name of the template parameters file\n            associating each parameter to a list of keys. Defaults to \"parameters_lhc.yaml\", which\n            is already contained in the study-da package, and includes the main usual parameters.\n        path_template_parameters (str, optional): The path to the template parameters file. Must\n            be provided if a no template already contained in study-da is provided through the\n            argument name_template_parameters. Defaults to None.\n        force_overwrite (bool, optional): Flag to indicate if the output file should be overwritten\n            if it already exists. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The final aggregated DataFrame.\n    \"\"\"\n    # Check it the output doesn't already exist and ask for confirmation to overwrite\n    dic_tree, _ = load_dic_from_path(path_tree)\n    absolute_path_study = dic_tree[\"absolute_path\"]\n    if path_output is None:\n        path_output = os.path.join(absolute_path_study, \"da.parquet\")\n    if os.path.exists(path_output) and not force_overwrite:\n        input_user = input(\n            f\"The output file {path_output} already exists. Do you want to overwrite it? (y/n) \"\n        )\n        if input_user.lower() != \"y\":\n            logging.warning(\"Output file not overwritten\")\n            return pd.read_parquet(path_output)\n\n    logging.info(\"Analysis of output simulation files started\")\n\n    dic_all_jobs = ConfigJobs(dic_tree).find_all_jobs()\n\n    l_df_sim = get_particles_data(\n        dic_all_jobs, absolute_path_study, generation_of_interest, name_output\n    )\n\n    default_path_template_parameters = False\n    if dic_parameters_of_interest is None:\n        if path_template_parameters is not None:\n            logging.info(\"Loading parameters of interest from the provided configuration file\")\n        else:\n            if name_template_parameters is None:\n                raise ValueError(\n                    \"No template configuration file provided for the parameters of interest\"\n                )\n            logging.info(\"Loading parameters of interest from the template configuration file\")\n            path_template_parameters = os.path.join(\n                os.path.dirname(inspect.getfile(aggregate_output_data)),\n                \"configs\",\n                name_template_parameters,\n            )\n            default_path_template_parameters = True\n        dic_parameters_of_interest, _ = load_dic_from_path(path_template_parameters)\n\n    l_df_output = add_parameters_from_config(\n        l_df_sim, dic_parameters_of_interest, default_path_template_parameters\n    )\n\n    df_final = merge_and_group_by_parameters_of_interest(\n        l_df_output, l_group_by_parameters, only_keep_lost_particles, l_parameters_to_keep\n    )\n\n    # Fix the LHC version type\n    df_final = fix_LHC_version(df_final)\n\n    if write_output:\n        df_final.to_parquet(path_output)\n    elif path_output is not None:\n        logging.warning(\"Output path provided but write_output set to False, no output saved\")\n\n    logging.info(\"Final dataframe for current set of simulations: %s\", df_final)\n    return df_final\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.fix_LHC_version","title":"<code>fix_LHC_version(df)</code>","text":"<p>Fixes the LHC version type in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to fix.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The fixed DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def fix_LHC_version(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Fixes the LHC version type in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to fix.\n\n    Returns:\n        pd.DataFrame: The fixed DataFrame.\n    \"\"\"\n    # Fix the LHC version type\n    if \"ver_lhc_run\" in df.columns:\n        df[\"ver_lhc_run\"] = df[\"ver_lhc_run\"].astype(\"int32\")\n    if \"ver_hllhc_optics\" in df.columns:\n        df[\"ver_hllhc_optics\"] = df[\"ver_hllhc_optics\"].astype(\"float32\")\n\n    return df\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.get_particles_data","title":"<code>get_particles_data(dic_all_jobs, absolute_path_study, generation_of_interest=2, name_output='output_particles.parquet')</code>","text":"<p>Retrieves particle data from simulation output files.</p> <p>Parameters:</p> Name Type Description Default <code>dic_all_jobs</code> <code>dict</code> <p>Dictionary containing all jobs and their details.</p> required <code>absolute_path_study</code> <code>str</code> <p>The absolute path to the study directory.</p> required <code>generation_of_interest</code> <code>int</code> <p>The generation of interest. Defaults to 2.</p> <code>2</code> <code>name_output</code> <code>str</code> <p>The name of the output file. Defaults to \"output_particles.parquet\".</p> <code>'output_particles.parquet'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[DataFrame]</code> <p>A list of DataFrames containing the particle data.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def get_particles_data(\n    dic_all_jobs: Dict[str, Dict[str, List[str]]],\n    absolute_path_study: str,\n    generation_of_interest: int = 2,\n    name_output: str = \"output_particles.parquet\",\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Retrieves particle data from simulation output files.\n\n    Args:\n        dic_all_jobs (dict): Dictionary containing all jobs and their details.\n        absolute_path_study (str): The absolute path to the study directory.\n        generation_of_interest (int, optional): The generation of interest. Defaults to 2.\n        name_output (str, optional): The name of the output file.\n            Defaults to \"output_particles.parquet\".\n\n    Returns:\n        list: A list of DataFrames containing the particle data.\n    \"\"\"\n\n    # Loop over all jobs and extract the output data\n    l_df_output = []\n    for relative_path_job, dic_job in dic_all_jobs.items():\n        if dic_job[\"gen\"] != generation_of_interest:\n            continue\n        absolute_path_job = os.path.join(absolute_path_study, relative_path_job)\n        absolute_folder_job = os.path.dirname(absolute_path_job)\n        try:\n            df_output = pd.read_parquet(os.path.join(absolute_folder_job, name_output))\n        except FileNotFoundError as e:\n            logging.warning(f\"File not found: {e}\")\n            continue\n        except Exception as e:\n            logging.warning(f\"Error reading parquet file: {e}\")\n            continue\n\n        # Register path of the job\n        df_output[\"name base collider\"] = relative_path_job\n\n        # Add to the list\n        l_df_output.append(df_output)\n\n    return l_df_output\n</code></pre>"},{"location":"reference/study_da/postprocess/postprocess.html#study_da.postprocess.postprocess.merge_and_group_by_parameters_of_interest","title":"<code>merge_and_group_by_parameters_of_interest(l_df_output, l_group_by_parameters, only_keep_lost_particles=True, l_parameters_to_keep=None)</code>","text":"<p>Merges and groups the output data by parameters of interest.</p> <p>Parameters:</p> Name Type Description Default <code>l_df_output</code> <code>list</code> <p>List of DataFrames containing the output data.</p> required <code>l_group_by_parameters</code> <code>list</code> <p>List of parameters to group by.</p> required <code>only_keep_lost_particles</code> <code>bool</code> <p>Flag to indicate if only lost particles should be kept. Defaults to True.</p> <code>True</code> <code>l_parameters_to_keep</code> <code>list</code> <p>List of parameters to keep. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The merged and grouped DataFrame.</p> Source code in <code>study_da/postprocess/postprocess.py</code> <pre><code>def merge_and_group_by_parameters_of_interest(\n    l_df_output: List[pd.DataFrame],\n    l_group_by_parameters: List[str],\n    only_keep_lost_particles: bool = True,\n    l_parameters_to_keep: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Merges and groups the output data by parameters of interest.\n\n    Args:\n        l_df_output (list): List of DataFrames containing the output data.\n        l_group_by_parameters (list): List of parameters to group by.\n        only_keep_lost_particles (bool, optional): Flag to indicate if only lost particles should\n            be kept. Defaults to True.\n        l_parameters_to_keep (list, optional): List of parameters to keep. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The merged and grouped DataFrame.\n    \"\"\"\n    # Merge all dataframes\n    df_all_sim = pd.concat(l_df_output)\n\n    # Handle mutable default arguments\n    if l_parameters_to_keep is None:\n        logging.info(\"No list of parameters to keep provided, keeping all available parameters\")\n        l_parameters_to_keep = list(df_all_sim.columns)\n\n    if only_keep_lost_particles:\n        # Extract the particles that were lost for DA computation\n        df_all_sim = df_all_sim[df_all_sim[\"state\"] != 1]\n\n    # Check if the dataframe is empty\n    if df_all_sim.empty:\n        logging.warning(\"No unstable particles found, the output dataframe will be empty.\")\n\n    # Group by parameters of interest\n    df_grouped = df_all_sim.groupby(l_group_by_parameters)\n\n    # Return the grouped dataframe, keeping only the minimum values of the parameters of interest\n    # (should not have impact except for DA, which we want to be minimal)\n    return pd.DataFrame(\n        [df_grouped[parameter].min() for parameter in l_parameters_to_keep]\n    ).transpose()\n</code></pre>"},{"location":"reference/study_da/submit/index.html","title":"submit","text":""},{"location":"reference/study_da/submit/ask_user_config.html","title":"ask_user_config","text":"<p>This module contains functions to prompt the user for various job configuration settings.</p> <p>Functions:</p> Name Description <code>ask_and_set_context</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_and_set_htc_flavour</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_and_set_run_on</code> <p>dict[str, Any]) -&gt; None:</p> <code>ask_keep_setting</code> <p>str) -&gt; bool:</p> <code>ask_skip_configured_jobs</code>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_context","title":"<code>ask_and_set_context(dic_gen)</code>","text":"<p>Prompts the user to select a context for the job and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_context(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user to select a context for the job and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        try:\n            context = input(\n                f\"What type of context do you want to use for job {dic_gen['file']}?\"\n                \" 1: cpu, 2: cupy, 3: opencl. Default is cpu.\"\n            )\n            context = 1 if context == \"\" else int(context)\n            if context in range(1, 4):\n                break\n            else:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input. Please enter a number between 1 and 3.\")\n\n    dict_context = {\n        1: \"cpu\",\n        2: \"cupy\",\n        3: \"opencl\",\n    }\n    dic_gen[\"context\"] = dict_context[context]\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_htc_flavour","title":"<code>ask_and_set_htc_flavour(dic_gen)</code>","text":"<p>Prompts the user to select an HTCondor job flavor and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_htc_flavour(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user to select an HTCondor job flavor and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        try:\n            submission_type = input(\n                f\"What type of htc job flavour do you want to use for job {dic_gen['file']}?\"\n                f\" 1: espresso, 2: microcentury, 3: longlunch, 4: workday, 5: tomorrow,\"\n                f\" 6: testmatch, 7: nextweek. Default is espresso.\"\n            )\n            submission_type = 1 if submission_type == \"\" else int(submission_type)\n            if submission_type in range(1, 8):\n                break\n            else:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input. Please enter a number between 1 and 7.\")\n\n    dict_flavour_type = {\n        1: \"espresso\",\n        2: \"microcentury\",\n        3: \"longlunch\",\n        4: \"workday\",\n        5: \"tomorrow\",\n        6: \"testmatch\",\n        7: \"nextweek\",\n    }\n    dic_gen[\"htc_flavor\"] = dict_flavour_type[submission_type]\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_and_set_run_on","title":"<code>ask_and_set_run_on(dic_gen)</code>","text":"<p>Prompts the user to select a submission type and sets it in the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dic_gen</code> <code>dict[str, Any]</code> <p>The dictionary containing job configuration.</p> required Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_and_set_run_on(dic_gen: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Prompts the user to select a submission type and sets it in the provided dictionary.\n\n    Args:\n        dic_gen (dict[str, Any]): The dictionary containing job configuration.\n    \"\"\"\n    while True:\n        try:\n            submission_type = input(\n                f\"What type of submission do you want to use for job {dic_gen['file']}?\"\n                \" 1: local, 2: htc, 3: htc_docker, 4: slurm, 5: slurm_docker. Default is local.\"\n            )\n            submission_type = 1 if submission_type == \"\" else int(submission_type)\n            if submission_type in range(1, 6):\n                break\n            else:\n                raise ValueError\n        except ValueError:\n            print(\"Invalid input. Please enter a number between 1 and 5.\")\n\n    dict_submission_type = {\n        1: \"local\",\n        2: \"htc\",\n        3: \"htc_docker\",\n        4: \"slurm\",\n        5: \"slurm_docker\",\n    }\n    dic_gen[\"submission_type\"] = dict_submission_type[submission_type]\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_keep_setting","title":"<code>ask_keep_setting(job_name)</code>","text":"<p>Prompts the user to decide whether to keep the same settings for identical jobs.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the user wants to keep the same settings, False otherwise.</p> Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_keep_setting(job_name: str) -&gt; bool:\n    \"\"\"\n    Prompts the user to decide whether to keep the same settings for identical jobs.\n\n    Returns:\n        bool: True if the user wants to keep the same settings, False otherwise.\n    \"\"\"\n    keep_setting = input(\n        f\"Do you want to keep the same setting for all jobs of the type {job_name} ? (y/n).\"\n        f\"Default is y.\"\n    )\n    while keep_setting not in [\"\", \"y\", \"n\"]:\n        keep_setting = input(\"Invalid input. Please enter y, n or skip question.\")\n    if keep_setting == \"\":\n        keep_setting = \"y\"\n    return keep_setting == \"y\"\n</code></pre>"},{"location":"reference/study_da/submit/ask_user_config.html#study_da.submit.ask_user_config.ask_skip_configured_jobs","title":"<code>ask_skip_configured_jobs()</code>","text":"<p>Prompts the user to decide whether to skip already configured jobs.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the user wants to skip already configured jobs, False otherwise.</p> Source code in <code>study_da/submit/ask_user_config.py</code> <pre><code>def ask_skip_configured_jobs() -&gt; bool:\n    \"\"\"\n    Prompts the user to decide whether to skip already configured jobs.\n\n    Returns:\n        bool: True if the user wants to skip already configured jobs, False otherwise.\n    \"\"\"\n    skip_configured_jobs = input(\n        \"Some jobs to submit seem to be configured already. Do you want to skip them? (y/n). \"\n        \"Default is y.\"\n    )\n    while skip_configured_jobs not in [\"\", \"y\", \"n\"]:\n        skip_configured_jobs = input(\"Invalid input. Please enter y, n or skip question.\")\n    if skip_configured_jobs == \"\":\n        skip_configured_jobs = \"y\"\n    return skip_configured_jobs == \"y\"\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html","title":"config_jobs","text":"<p>This module contains the ConfigJobs class that allows to configure jobs in the tree file.</p>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs","title":"<code>ConfigJobs</code>","text":"<p>A class to configure jobs in the tree file.</p> <p>Attributes:</p> Name Type Description <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> <p>Methods:</p> Name Description <code>_find_and_configure_jobs_recursion</code> <p>Recursively finds and configures jobs.</p> <code>find_and_configure_jobs</code> <p>Finds and configures all jobs in the tree.</p> <code>find_all_jobs</code> <p>Finds all jobs in the tree.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>class ConfigJobs:\n    \"\"\"\n    A class to configure jobs in the tree file.\n\n    Attributes:\n        dic_tree (dict): The dictionary representing the job tree.\n\n    Methods:\n        _find_and_configure_jobs_recursion(dic_gen, depth=0, l_keys=None, find_only=False):\n            Recursively finds and configures jobs.\n        find_and_configure_jobs(): Finds and configures all jobs in the tree.\n        find_all_jobs(): Finds all jobs in the tree.\n    \"\"\"\n\n    def __init__(self, dic_tree: dict):\n        \"\"\"\n        Initializes the ConfigJobs class.\n\n        Args:\n            dic_tree (dict): The dictionary representing the job tree.\n\n        \"\"\"\n        self.dic_tree: dict = dic_tree\n\n        # Flag set to True if self.find_all_jobs() has been called at least once\n        self.all_jobs_found: bool = False\n\n        # Variables to store the jobs and their configuration\n        self.dic_all_jobs: dict[str, Any] = {}\n\n    def _find_and_configure_jobs_recursion(\n        self,\n        dic_gen: dict[str, Any],\n        depth: int = 0,\n        l_keys: list[str] | None = None,\n        find_only: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Recursively finds and configures jobs in the tree.\n\n        Args:\n            dic_gen (dict[str, Any]): The dictionary representing the current level of the job tree.\n            depth (int, optional): The current depth in the tree. Defaults to 0.\n            l_keys (list[str], optional): The list of keys representing the path in the tree.\n                Defaults to None.\n            find_only (bool, optional): If True, only finds jobs without configuring them.\n                Defaults to False.\n\n        Raises:\n            AttributeError: If required attributes are not set before calling this method.\n        \"\"\"\n        if l_keys is None:\n            l_keys = []\n\n        if not hasattr(self, \"dic_all_jobs\"):\n            raise AttributeError(\"dic_all_jobs should be set before calling this method\")\n\n        # Recursively look for job key in the tree, keeping track of the depth\n        # of the job in the tree\n        # Browse a list of keys rather than than the keys() generator not to create mutation errors\n        for key in list(dic_gen.keys()):\n            value = dic_gen[key]\n            if isinstance(value, dict):\n                self._find_and_configure_jobs_recursion(\n                    dic_gen=value, depth=depth + 1, l_keys=l_keys + [key], find_only=find_only\n                )\n            elif key == \"file\":\n                # Add job the the list of all jobs\n                # In theory, the list of keys can be obtained from the job path\n                # but it's safer to keep it in the dict\n                self.dic_all_jobs[value] = {\n                    \"gen\": depth,\n                    \"l_keys\": copy.copy(l_keys),\n                }\n\n                # Stop the browsing if we only want to find the jobs\n                if find_only:\n                    return\n\n                # Otherwise ensure that the job can be configured\n                if not hasattr(self, \"dic_config_jobs\") or not hasattr(\n                    self, \"skip_configured_jobs\"\n                ):\n                    raise AttributeError(\n                        \"dic_config_jobs and skip_configured_jobs should be set before calling\"\n                        \"this method\"\n                    )\n\n                # Put path_run to None if it exists\n                if \"path_run\" in dic_gen:\n                    logging.warning(\n                        f\"Job {value} has a path_run attribute. It will be set to None.\"\n                    )\n                    dic_gen[\"path_run\"] = None\n\n                # If all is fine so far, get job name and configure\n                job_name = value.split(\"/\")[-1]\n\n                # Ensure configuration is not already set\n                if \"submission_type\" in dic_gen:\n                    if self.skip_configured_jobs is None:\n                        self.skip_configured_jobs = ask_skip_configured_jobs()\n                    if self.skip_configured_jobs:\n                        return\n\n                # If it's the first time we find the job, ask for context and run_on\n                # Note that a job can be configured and not be in self.dic_config_jobs\n                # self.dic_config_jobs contains the archetypical main jobs (one per gen), not all jobs\n                if job_name not in self.dic_config_jobs:\n                    self._get_context_and_run_on(depth, value, dic_gen, job_name)\n                else:\n                    # Check that the config for the current job is ok\n                    self.check_config_jobs(job_name)\n\n                    # Merge the configuration of the job with the existing one\n                    dic_gen |= self.dic_config_jobs[job_name]\n\n    def check_config_jobs(self, job_name: str) -&gt; None:\n        \"\"\"\n        Check the configuration of a job and ensure that it is properly set.\n        Useful when the dic_config_jobs is provided externally.\n\n        Args:\n            job_name (str): The name of the job to be configured.\n\n        Returns:\n            dict: The updated job configuration.\n        \"\"\"\n\n        # Ensure flavour is set for htc jobs\n        if (\n            self.dic_config_jobs[job_name][\"submission_type\"] in [\"htc\", \"htc_docker\"]\n            and \"htc_flavor\" not in self.dic_config_jobs[job_name]\n        ):\n            raise ValueError(\n                f\"Job {job_name} is not properly configured. Please set the htc_flavor.\"\n            )\n\n        # Set status to to_submit if not already set\n        if \"status\" not in self.dic_config_jobs[job_name]:\n            self.dic_config_jobs[job_name][\"status\"] = \"to_submit\"\n\n    def _get_context_and_run_on(self, depth: int, value: str, dic_gen: dict, job_name: str) -&gt; None:\n        \"\"\"\n        Sets the context and run-on parameters for a job, updates the job configuration,\n        and stores it in the job dictionary if the user chooses to keep the settings.\n\n        Args:\n            depth (int): The depth level of the job in the hierarchy.\n            value (str): The value associated with the job.\n            dic_gen (dict): A dictionary containing general job configuration parameters.\n            job_name (str): The name of the job to be configured.\n\n        Returns:\n            None\n        \"\"\"\n        logging.info(f\"Found job at depth {depth}: {value}\")\n        # Set context and run_on\n        ask_and_set_context(dic_gen)\n        ask_and_set_run_on(dic_gen)\n        if dic_gen[\"submission_type\"] in [\"htc\", \"htc_docker\"]:\n            ask_and_set_htc_flavour(dic_gen)\n        else:\n            dic_gen[\"htc_flavor\"] = None\n        dic_gen[\"status\"] = \"to_submit\"\n\n        # Compute all jobs to see if there are at least two jobs in the current generation\n        self.find_all_jobs()\n        # Ensure there are more than one job of the same type to ask the user\n        # sourcery skip: merge-nested-ifs\n        if [x[\"gen\"] == depth for x in self.dic_all_jobs.values()].count(True) &gt; 1:\n            # Ask the user if they want to keep the settings for all jobs of the same type\n            if ask_keep_setting(job_name):\n                self.dic_config_jobs[job_name] = {\n                    \"context\": dic_gen[\"context\"],\n                    \"submission_type\": dic_gen[\"submission_type\"],\n                    \"status\": dic_gen[\"status\"],\n                    \"htc_flavor\": dic_gen[\"htc_flavor\"],\n                }\n\n    def find_and_configure_jobs(\n        self, dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Finds and configures all jobs in the tree.\n\n        Args:\n            dic_config_jobs (dict[str, dict[str, Any]], optional): A dictionary containing the\n                configuration of the jobs. Defaults to None.\n\n        Returns:\n            dict: The updated job tree with configurations.\n        \"\"\"\n        # Variables to store the jobs and their configuration\n        self.dic_config_jobs = dic_config_jobs if dic_config_jobs is not None else {}\n        self.skip_configured_jobs = None\n        self._log_and_find(\"Finding and configuring jobs in the tree\", False)\n        return self.dic_tree\n\n    def find_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Finds all jobs in the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs and their details.\n        \"\"\"\n        if not self.all_jobs_found:\n            self._log_and_find(\"Finding all jobs in the tree\", True)\n        else:\n            logging.info(\"All jobs have already been found. Returning the existing dictionary.\")\n\n        return self.dic_all_jobs\n\n    def _log_and_find(self, log_str, find_only):\n        logging.info(log_str)\n        self._find_and_configure_jobs_recursion(self.dic_tree, depth=-1, find_only=find_only)\n        self.all_jobs_found = True\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.__init__","title":"<code>__init__(dic_tree)</code>","text":"<p>Initializes the ConfigJobs class.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> required Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def __init__(self, dic_tree: dict):\n    \"\"\"\n    Initializes the ConfigJobs class.\n\n    Args:\n        dic_tree (dict): The dictionary representing the job tree.\n\n    \"\"\"\n    self.dic_tree: dict = dic_tree\n\n    # Flag set to True if self.find_all_jobs() has been called at least once\n    self.all_jobs_found: bool = False\n\n    # Variables to store the jobs and their configuration\n    self.dic_all_jobs: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.check_config_jobs","title":"<code>check_config_jobs(job_name)</code>","text":"<p>Check the configuration of a job and ensure that it is properly set. Useful when the dic_config_jobs is provided externally.</p> <p>Parameters:</p> Name Type Description Default <code>job_name</code> <code>str</code> <p>The name of the job to be configured.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>None</code> <p>The updated job configuration.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def check_config_jobs(self, job_name: str) -&gt; None:\n    \"\"\"\n    Check the configuration of a job and ensure that it is properly set.\n    Useful when the dic_config_jobs is provided externally.\n\n    Args:\n        job_name (str): The name of the job to be configured.\n\n    Returns:\n        dict: The updated job configuration.\n    \"\"\"\n\n    # Ensure flavour is set for htc jobs\n    if (\n        self.dic_config_jobs[job_name][\"submission_type\"] in [\"htc\", \"htc_docker\"]\n        and \"htc_flavor\" not in self.dic_config_jobs[job_name]\n    ):\n        raise ValueError(\n            f\"Job {job_name} is not properly configured. Please set the htc_flavor.\"\n        )\n\n    # Set status to to_submit if not already set\n    if \"status\" not in self.dic_config_jobs[job_name]:\n        self.dic_config_jobs[job_name][\"status\"] = \"to_submit\"\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.find_all_jobs","title":"<code>find_all_jobs()</code>","text":"<p>Finds all jobs in the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs and their details.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def find_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Finds all jobs in the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs and their details.\n    \"\"\"\n    if not self.all_jobs_found:\n        self._log_and_find(\"Finding all jobs in the tree\", True)\n    else:\n        logging.info(\"All jobs have already been found. Returning the existing dictionary.\")\n\n    return self.dic_all_jobs\n</code></pre>"},{"location":"reference/study_da/submit/config_jobs.html#study_da.submit.config_jobs.ConfigJobs.find_and_configure_jobs","title":"<code>find_and_configure_jobs(dic_config_jobs=None)</code>","text":"<p>Finds and configures all jobs in the tree.</p> <p>Parameters:</p> Name Type Description Default <code>dic_config_jobs</code> <code>dict[str, dict[str, Any]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated job tree with configurations.</p> Source code in <code>study_da/submit/config_jobs.py</code> <pre><code>def find_and_configure_jobs(\n    self, dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None\n) -&gt; dict:\n    \"\"\"\n    Finds and configures all jobs in the tree.\n\n    Args:\n        dic_config_jobs (dict[str, dict[str, Any]], optional): A dictionary containing the\n            configuration of the jobs. Defaults to None.\n\n    Returns:\n        dict: The updated job tree with configurations.\n    \"\"\"\n    # Variables to store the jobs and their configuration\n    self.dic_config_jobs = dic_config_jobs if dic_config_jobs is not None else {}\n    self.skip_configured_jobs = None\n    self._log_and_find(\"Finding and configuring jobs in the tree\", False)\n    return self.dic_tree\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html","title":"dependency_graph","text":"<p>This module contains the DependencyGraph class to manage the dependencies between jobs</p>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph","title":"<code>DependencyGraph</code>","text":"<p>A class to manage the dependencies between jobs.</p> <p>Attributes:</p> Name Type Description <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> <code>dic_all_jobs</code> <code>dict</code> <p>The dictionary containing all jobs and their details.</p> <code>dependency_graph</code> <code>dict</code> <p>The dictionary representing the dependency graph.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the DependencyGraph class.</p> <code>build_full_dependency_graph</code> <p>Builds the full dependency graph.</p> <code>get_unfinished_dependency</code> <p>Gets the list of unfinished dependencies for a given job.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>class DependencyGraph:\n    \"\"\"\n    A class to manage the dependencies between jobs.\n\n    Attributes:\n        dic_tree (dict): The dictionary representing the job tree.\n        dic_all_jobs (dict): The dictionary containing all jobs and their details.\n        dependency_graph (dict): The dictionary representing the dependency graph.\n\n    Methods:\n        __init__(dic_tree, dic_all_jobs): Initializes the DependencyGraph class.\n        build_full_dependency_graph(): Builds the full dependency graph.\n        get_unfinished_dependency(job): Gets the list of unfinished dependencies for a given job.\n    \"\"\"\n\n    def __init__(self, dic_tree: dict, dic_all_jobs: dict):\n        \"\"\"\n        Initializes the DependencyGraph class.\n\n        Args:\n            dic_tree (dict): The dictionary representing the job tree.\n            dic_all_jobs (dict): The dictionary containing all jobs and their details.\n        \"\"\"\n        self.dic_tree = dic_tree\n        self.dic_all_jobs = dic_all_jobs\n        self.dependency_graph = {}\n\n    def build_full_dependency_graph(self) -&gt; dict:\n        \"\"\"\n        Builds the full dependency graph.\n\n        Returns:\n            dict: The full dependency graph.\n        \"\"\"\n        self.set_l_keys = {\n            tuple(self.dic_all_jobs[job][\"l_keys\"][:-1]) for job in self.dic_all_jobs\n        }\n        for job in self.dic_all_jobs:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            self.dependency_graph[job] = set()\n            # Add all parents to the dependency graph\n            for i in range(len(l_keys) - 1):\n                l_keys_parent = l_keys[:i]\n                if tuple(l_keys_parent) in self.set_l_keys:\n                    parent = nested_get(self.dic_tree, l_keys_parent)\n                    # Look for all the jobs in the parent (but not the generations below)\n                    for name_parent, sub_dict in parent.items():\n                        if \"file\" in sub_dict:\n                            self.dependency_graph[job].add(sub_dict[\"file\"])\n        return self.dependency_graph\n\n    def get_unfinished_dependency(self, job: str) -&gt; list:\n        \"\"\"\n        Gets the list of unfinished dependencies for a given job.\n\n        Args:\n            job (str): The name of the job.\n\n        Returns:\n            list: The list of unfinished dependencies.\n        \"\"\"\n        # Ensure the dependency graph is built\n        if self.dependency_graph == {}:\n            self.build_full_dependency_graph()\n\n        # Get the list of dependencies\n        l_dependencies = self.dependency_graph[job]\n\n        # Get the corresponding list of l_keys\n        ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n        return [\n            dep\n            for dep, l_keys in zip(l_dependencies, ll_keys)\n            if nested_get(self.dic_tree, l_keys + [\"status\"]) not in [\"finished\", \"failed\"]\n        ]\n\n    def get_failed_dependency(self, job: str) -&gt; list:\n        \"\"\"\n        Gets the list of failed dependencies for a given job.\n\n        Args:\n            job (str): The name of the job.\n\n        Returns:\n            list: The list of failed dependencies.\n        \"\"\"\n        # Ensure the dependency graph is built\n        if self.dependency_graph == {}:\n            self.build_full_dependency_graph()\n\n        # Get the list of dependencies\n        l_dependencies = self.dependency_graph[job]\n\n        # Get the corresponding list of l_keys\n        ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n        return [\n            dep\n            for dep, l_keys in zip(l_dependencies, ll_keys)\n            if nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        ]\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.__init__","title":"<code>__init__(dic_tree, dic_all_jobs)</code>","text":"<p>Initializes the DependencyGraph class.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary representing the job tree.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>The dictionary containing all jobs and their details.</p> required Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def __init__(self, dic_tree: dict, dic_all_jobs: dict):\n    \"\"\"\n    Initializes the DependencyGraph class.\n\n    Args:\n        dic_tree (dict): The dictionary representing the job tree.\n        dic_all_jobs (dict): The dictionary containing all jobs and their details.\n    \"\"\"\n    self.dic_tree = dic_tree\n    self.dic_all_jobs = dic_all_jobs\n    self.dependency_graph = {}\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.build_full_dependency_graph","title":"<code>build_full_dependency_graph()</code>","text":"<p>Builds the full dependency graph.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The full dependency graph.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def build_full_dependency_graph(self) -&gt; dict:\n    \"\"\"\n    Builds the full dependency graph.\n\n    Returns:\n        dict: The full dependency graph.\n    \"\"\"\n    self.set_l_keys = {\n        tuple(self.dic_all_jobs[job][\"l_keys\"][:-1]) for job in self.dic_all_jobs\n    }\n    for job in self.dic_all_jobs:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        self.dependency_graph[job] = set()\n        # Add all parents to the dependency graph\n        for i in range(len(l_keys) - 1):\n            l_keys_parent = l_keys[:i]\n            if tuple(l_keys_parent) in self.set_l_keys:\n                parent = nested_get(self.dic_tree, l_keys_parent)\n                # Look for all the jobs in the parent (but not the generations below)\n                for name_parent, sub_dict in parent.items():\n                    if \"file\" in sub_dict:\n                        self.dependency_graph[job].add(sub_dict[\"file\"])\n    return self.dependency_graph\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.get_failed_dependency","title":"<code>get_failed_dependency(job)</code>","text":"<p>Gets the list of failed dependencies for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>str</code> <p>The name of the job.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of failed dependencies.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def get_failed_dependency(self, job: str) -&gt; list:\n    \"\"\"\n    Gets the list of failed dependencies for a given job.\n\n    Args:\n        job (str): The name of the job.\n\n    Returns:\n        list: The list of failed dependencies.\n    \"\"\"\n    # Ensure the dependency graph is built\n    if self.dependency_graph == {}:\n        self.build_full_dependency_graph()\n\n    # Get the list of dependencies\n    l_dependencies = self.dependency_graph[job]\n\n    # Get the corresponding list of l_keys\n    ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n    return [\n        dep\n        for dep, l_keys in zip(l_dependencies, ll_keys)\n        if nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n    ]\n</code></pre>"},{"location":"reference/study_da/submit/dependency_graph.html#study_da.submit.dependency_graph.DependencyGraph.get_unfinished_dependency","title":"<code>get_unfinished_dependency(job)</code>","text":"<p>Gets the list of unfinished dependencies for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>str</code> <p>The name of the job.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of unfinished dependencies.</p> Source code in <code>study_da/submit/dependency_graph.py</code> <pre><code>def get_unfinished_dependency(self, job: str) -&gt; list:\n    \"\"\"\n    Gets the list of unfinished dependencies for a given job.\n\n    Args:\n        job (str): The name of the job.\n\n    Returns:\n        list: The list of unfinished dependencies.\n    \"\"\"\n    # Ensure the dependency graph is built\n    if self.dependency_graph == {}:\n        self.build_full_dependency_graph()\n\n    # Get the list of dependencies\n    l_dependencies = self.dependency_graph[job]\n\n    # Get the corresponding list of l_keys\n    ll_keys = [self.dic_all_jobs[dep][\"l_keys\"] for dep in l_dependencies]\n\n    return [\n        dep\n        for dep, l_keys in zip(l_dependencies, ll_keys)\n        if nested_get(self.dic_tree, l_keys + [\"status\"]) not in [\"finished\", \"failed\"]\n    ]\n</code></pre>"},{"location":"reference/study_da/submit/generate_run.html","title":"generate_run","text":"<p>This module provides functions to generate run files for jobs in different environments (local/Slurm and HTCondor). It includes functions to create shell script snippets for tagging jobs as finished and generating run files with appropriate configurations.</p> <p>Functions:</p> Name Description <code>tag_str</code> <p>str) -&gt; str:</p> <code>generate_run_file</code> <p>str, job_name: str, setup_env_script: str, generation_number: int, tree_path: str, l_keys: list[str], htc: bool = False, additionnal_command: str = \"\", l_dependencies: list[str] | None = None, name_config: str = \"config.yaml\") -&gt; str:</p> <code>_generate_run_file</code> <p>str, job_name: str, setup_env_script: str, tree_path: str, l_keys: list[str], additionnal_command: str = \"\") -&gt; str:</p> <code>_generate_run_file_htc</code> <p>str, job_name: str, setup_env_script: str, generation_number: int, tree_path: str, l_keys: list[str], additionnal_command: str = \"\", l_dependencies: list[str] | None = None, name_config: str = \"config.yaml\") -&gt; str:</p>"},{"location":"reference/study_da/submit/generate_run.html#study_da.submit.generate_run.generate_run_file","title":"<code>generate_run_file(abs_job_folder, job_name, setup_env_script, htc=False, additionnal_command='', **kwargs_htc_run_files)</code>","text":"<p>Generates a run file for a job, either for local/Slurm or HTC environments.</p> <p>Parameters:</p> Name Type Description Default <code>abs_job_folder</code> <code>str</code> <p>The (absolute) folder where the job is located.</p> required <code>job_name</code> <code>str</code> <p>The name of the job script.</p> required <code>setup_env_script</code> <code>str</code> <p>The script to set up the environment.</p> required <code>htc</code> <code>bool</code> <p>Whether the job shoud be run on HTCondor. Defaults to False.</p> <code>False</code> <code>additionnal_command</code> <code>str</code> <p>Additional command to run. Defaults to \"\".</p> <code>''</code> <code>**kwargs_htc_run_files</code> <code>Any</code> <p>Additional keyword arguments for the generate_run_files method     when submitting HTC jobs.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated run file content.</p> Source code in <code>study_da/submit/generate_run.py</code> <pre><code>def generate_run_file(\n    abs_job_folder: str,\n    job_name: str,\n    setup_env_script: str,\n    htc: bool = False,\n    additionnal_command: str = \"\",\n    **kwargs_htc_run_files: Any,\n) -&gt; str:\n    \"\"\"\n    Generates a run file for a job, either for local/Slurm or HTC environments.\n\n    Args:\n        abs_job_folder (str): The (absolute) folder where the job is located.\n        job_name (str): The name of the job script.\n        setup_env_script (str): The script to set up the environment.\n        htc (bool, optional): Whether the job shoud be run on HTCondor. Defaults to False.\n        additionnal_command (str, optional): Additional command to run. Defaults to \"\".\n        **kwargs_htc_run_files (Any): Additional keyword arguments for the generate_run_files method\n                when submitting HTC jobs.\n\n    Returns:\n        str: The generated run file content.\n    \"\"\"\n    if not htc:\n        return _generate_run_file(\n            abs_job_folder,\n            job_name,\n            setup_env_script,\n            additionnal_command,\n        )\n    if kwargs_htc_run_files[\"l_dependencies\"] is None:\n        kwargs_htc_run_files[\"l_dependencies\"] = []\n    return _generate_run_file_htc(\n        abs_job_folder,\n        job_name,\n        setup_env_script,\n        additionnal_command,\n        **kwargs_htc_run_files,\n    )\n</code></pre>"},{"location":"reference/study_da/submit/generate_run.html#study_da.submit.generate_run.tag_str","title":"<code>tag_str(abs_job_folder)</code>","text":"<p>\" Generates a shell script snippet to tag a job as finished if it was successful. Args:     abs_job_folder (str): The (absolute) folder where the job is located. Returns:     str: A formatted string containing the shell script snippet.</p> Source code in <code>study_da/submit/generate_run.py</code> <pre><code>def tag_str(abs_job_folder: str) -&gt; str:\n    \"\"\" \"\n    Generates a shell script snippet to tag a job as finished if it was successful.\n    Args:\n        abs_job_folder (str): The (absolute) folder where the job is located.\n    Returns:\n        str: A formatted string containing the shell script snippet.\n    \"\"\"\n    return f\"\"\"\n# Ensure job run was successful and tag as finished, or as failed otherwise\nif [ $? -eq 0 ]; then\n    touch {abs_job_folder}/.finished\nelse\n    touch {abs_job_folder}/.failed\nfi\\n\n\"\"\"\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html","title":"submit_scan","text":"<p>This module contains the SubmitScan class, which is used to submit jobs either locally or on a cluster.</p>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan","title":"<code>SubmitScan</code>","text":"Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>class SubmitScan:\n    def __init__(\n        self,\n        path_tree: str,\n        path_python_environment: str,\n        path_python_environment_container: str = \"\",\n        path_container_image: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the SubmitScan class.\n\n        Args:\n            path_tree (str): The path to the tree structure.\n            path_python_environment (str): The path to the Python environment.\n            path_python_environment_container (str, optional): The path to the Python environment\n                in the container. Defaults to \"\".\n            path_container_image (Optional[str], optional): The path to the container image.\n                Defaults to None.\n        \"\"\"\n        # Path to study files\n        self.path_tree = path_tree\n\n        # Absolute path to the tree\n        self.abs_path_tree = os.path.abspath(path_tree)\n\n        # Name of the study folder\n        self.study_name = os.path.dirname(path_tree)\n\n        # Absolute path to the study folder (get from the path_tree)\n        self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n        # Path to the python environment, activate with `source path_python_environment`\n        # Turn to absolute path if it is not already\n        if not os.path.isabs(path_python_environment):\n            self.path_python_environment = os.path.abspath(path_python_environment)\n        else:\n            self.path_python_environment = path_python_environment\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment.endswith(\"/bin/activate\"):\n            self.path_python_environment += \"/bin/activate\"\n\n        # Container image (Docker or Singularity, if any)\n        # Turn to absolute path if it is not already\n        if path_container_image is None:\n            self.path_container_image = None\n        elif not os.path.isabs(path_container_image):\n            self.path_container_image = os.path.abspath(path_container_image)\n        else:\n            self.path_container_image = path_container_image\n\n        # Python environment for the container\n        self.path_python_environment_container = path_python_environment_container\n\n        # Ensure that the container image is set if the python environment is set\n        if self.path_container_image and not self.path_python_environment_container:\n            raise ValueError(\n                \"The path to the python environment in the container must be set if the container\"\n                \"image is set.\"\n            )\n\n        # Add /bin/activate to the path_python_environment if needed\n        if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n            self.path_python_environment_container += \"/bin/activate\"\n\n        # Lock file to avoid concurrent access (softlock as several platforms are used)\n        self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n\n    # dic_tree as a property so that it is reloaded every time it is accessed\n    @property\n    def dic_tree(self) -&gt; dict:\n        \"\"\"\n        Loads the dictionary tree from the path.\n\n        Returns:\n            dict: The loaded dictionary tree.\n        \"\"\"\n        logging.info(f\"Loading tree from {self.path_tree}\")\n        return load_dic_from_path(self.path_tree)[0]\n\n    # Setter for the dic_tree property\n    @dic_tree.setter\n    def dic_tree(self, value: dict) -&gt; None:\n        \"\"\"\n        Writes the dictionary tree to the path.\n\n        Args:\n            value (dict): The dictionary tree to write.\n        \"\"\"\n        logging.info(f\"Writing tree to {self.path_tree}\")\n        write_dic_to_path(value, self.path_tree)\n\n    def configure_jobs(\n        self,\n        force_configure: bool = False,\n        dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n        Args:\n            force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n            dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n                the configuration of the jobs. Defaults to None.\n        \"\"\"\n        # Lock since we are modifying the tree\n        logging.info(\"Acquiring lock to configure jobs\")\n        with self.lock:\n            # Get the tree\n            dic_tree = self.dic_tree\n\n            # Ensure jobs have not been configured already\n            if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n                logging.warning(\"Jobs have already been configured. Skipping.\")\n                return\n\n            # Configure the jobs (add generation and job keys, set status to \"To finish\")\n            dic_tree = ConfigJobs(dic_tree).find_and_configure_jobs(dic_config_jobs)\n\n            # Add the python environment, container image and absolute path of the study to the tree\n            dic_tree[\"python_environment\"] = self.path_python_environment\n            dic_tree[\"container_image\"] = self.path_container_image\n            dic_tree[\"absolute_path\"] = self.abs_path\n            dic_tree[\"status\"] = \"to_finish\"\n            dic_tree[\"configured\"] = True\n\n            # Explicitly set the dic_tree property to force rewrite\n            self.dic_tree = dic_tree\n\n        logging.info(\"Jobs have been configured. Lock released.\")\n\n    def get_all_jobs(self) -&gt; dict:\n        \"\"\"\n        Retrieves all jobs from the configuration, without modifying the tree.\n\n        Returns:\n            dict: A dictionary containing all jobs.\n        \"\"\"\n        # Get a copy of the tree as it's safer\n        with self.lock:\n            dic_tree = self.dic_tree\n        return ConfigJobs(dic_tree).find_all_jobs()\n\n    def generate_run_files(\n        self,\n        dic_tree: dict[str, Any],\n        l_jobs: list[str],\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; dict:\n        \"\"\"\n        Generates run files for the specified jobs.\n\n        Args:\n            dic_tree (dict): The dictionary tree structure.\n            l_jobs (list[str]): List of jobs to submit.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to {}.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n            name_config (str, optional): The name of the configuration file for the study.\n\n        Returns:\n            dict: The updated dictionary tree structure.\n        \"\"\"\n\n        logging.info(\"Generating run files for the jobs to submit\")\n        # Generate the run files for the jobs to submit\n        dic_all_jobs = self.get_all_jobs()\n        for job in l_jobs:\n            l_keys = dic_all_jobs[job][\"l_keys\"]\n            job_name = os.path.basename(job)\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            generation_number = dic_all_jobs[job][\"gen\"]\n            submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n            singularity = \"docker\" in submission_type\n            path_python_environment = (\n                self.path_python_environment_container\n                if singularity\n                else self.path_python_environment\n            )\n\n            # Ensure that the run file does not already exist\n            if \"path_run\" in nested_get(dic_tree, l_keys):\n                path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n                if path_run_curr is not None and os.path.exists(path_run_curr):\n                    logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                    continue\n\n            # Build l_dependencies and add to the kwargs\n            l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n            # Get arguments of current generation\n            dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n            # Mutate the keys\n            dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n            # Build kwargs for the run file\n            kwargs_htc = {\n                \"l_dependencies\": l_dependencies,\n                \"name_config\": name_config,\n            } | dic_args\n\n            run_str = generate_run_file(\n                absolute_job_folder,\n                job_name,\n                path_python_environment,\n                htc=\"htc\" in submission_type,\n                additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n                **kwargs_htc,\n            )\n            # Write the run file\n            path_run_job = f\"{absolute_job_folder}/run.sh\"\n            with open(path_run_job, \"w\") as f:\n                f.write(run_str)\n\n            # Record the path to the run file in the tree\n            nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n        return dic_tree\n\n    def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"\n        Checks the status of all jobs and updates their status in the job dictionary.\n\n        This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n        the job's folder, and updates the job's status accordingly. If at least one job is not\n        finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n        failed, the overall status is set to \"finished\".\n\n        Returns:\n            tuple[dict[str, Any], str]: A tuple containing:\n            - A dictionary with all jobs and their updated statuses.\n            - A string representing the final status (\"to_finish\" or \"finished\").\n        \"\"\"\n        dic_all_jobs = self.get_all_jobs()\n        at_least_one_job_to_finish = False\n        final_status = \"to_finish\"\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n            for job in dic_all_jobs:\n                relative_job_folder = os.path.dirname(job)\n                absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n                # Check if the file .finished exists\n                if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n                # Check if the job failed otherwise (not to resubmit it again)\n                elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                    nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n                else:\n                    at_least_one_job_to_finish = True\n\n            if not at_least_one_job_to_finish:\n                dic_tree[\"status\"] = final_status = \"finished\"\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n\n        return dic_all_jobs, final_status\n\n    def submit(\n        self,\n        one_generation_at_a_time: bool = False,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n    ) -&gt; str:\n        \"\"\"\n        Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n        can trigger a throttling mechanism in AFS.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n\n        Returns:\n            str: The final status of the jobs.\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n        if dic_copy_back_per_gen is None:\n            dic_copy_back_per_gen = {}\n\n        # Update the status of all jobs before submitting\n        dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n        if final_status == \"finished\":\n            logging.info(\"All jobs are finished. No need to submit.\")\n            return final_status\n\n        logging.info(\"Acquiring lock to submit jobs\")\n        with self.lock:\n            # Get dic tree once to avoid reloading it for every job\n            dic_tree = self.dic_tree\n\n            # Submit the jobs\n            self._submit(\n                dic_tree,\n                dic_all_jobs,\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n\n            # Update dic_tree from cluster_submission\n            self.dic_tree = dic_tree\n        logging.info(\"Jobs have been submitted. Lock released.\")\n        return final_status\n\n    def _submit(\n        self,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        one_generation_at_a_time: bool,\n        dic_additional_commands_per_gen: dict[int, str],\n        dic_dependencies_per_gen: dict[int, list[str]],\n        dic_copy_back_per_gen: dict[int, dict[str, bool]],\n        name_config: str,\n    ) -&gt; None:\n        \"\"\"\n        Submits the jobs to the cluster.\n\n        Args:\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            one_generation_at_a_time (bool): Whether to submit one full generation at a time.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation.\n\n            The following arguments are only used for HTC jobs submission:\n\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation.\n            name_config (str, optional): The name of the configuration file for the study.\n        \"\"\"\n        # Collect dict of list of unfinished jobs for every tree branch and every gen\n        dic_to_submit_by_gen = {}\n        dic_summary_by_gen = {}\n        dependency_graph = DependencyGraph(dic_tree, dic_all_jobs)\n        for job in dic_all_jobs:\n            dic_to_submit_by_gen, dic_summary_by_gen = self._check_job_submit_status(\n                job,\n                dic_tree,\n                dic_all_jobs,\n                dic_to_submit_by_gen,\n                dic_summary_by_gen,\n                dependency_graph,\n            )\n\n        # Only keep the topmost generation if one_generation_at_a_time is True\n        if one_generation_at_a_time:\n            logging.info(\n                \"Cropping list of jobs to submit to ensure only one generation is submitted at \"\n                \"a time.\"\n            )\n            max_gen = max(dic_to_submit_by_gen.keys())\n            dic_to_submit_by_gen = {max_gen: dic_to_submit_by_gen[max_gen]}\n\n        # Convert dic_to_submit_by_gen to contain all requested information\n        l_jobs_to_submit = [job for dic_gen in dic_to_submit_by_gen.values() for job in dic_gen]\n\n        # Generate run files for the jobs to submit\n        # ! Run files are generated at submit and not at configuration as the configuration\n        # ! files are created at the end of each generation\n        dic_tree = self.generate_run_files(\n            dic_tree,\n            l_jobs_to_submit,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen=dic_dependencies_per_gen,\n            dic_copy_back_per_gen=dic_copy_back_per_gen,\n            name_config=name_config,\n        )\n\n        # Create the ClusterSubmission object\n        path_submission_file = f\"{self.abs_path}/{self.study_name}/submission/submission_file.sub\"\n        cluster_submission = ClusterSubmission(\n            self.study_name,\n            l_jobs_to_submit,\n            dic_all_jobs,\n            dic_tree,\n            path_submission_file,\n            self.abs_path,\n        )\n\n        # Write and submit the submission files\n        logging.info(\"Writing and submitting submission files\")\n        dic_submission_files = cluster_submission.write_sub_files(dic_summary_by_gen)\n\n        # Log the state of the jobs\n        self.log_jobs_state(dic_summary_by_gen)\n        for submission_type, (\n            list_of_jobs,\n            l_submission_filenames,\n        ) in dic_submission_files.items():\n            cluster_submission.submit(list_of_jobs, l_submission_filenames, submission_type)\n\n    @staticmethod\n    def log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n        \"\"\"\n        Logs the state of jobs for each generation.\n\n        Args:\n            dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n                and the values are dictionaries summarizing job states.\n                Each summary dictionary should contain the following keys:\n                - 'to_submit_later': int, number of jobs left to submit later\n                - 'running_or_queuing': int, number of jobs running or queuing\n                - 'submitted_now': int, number of jobs submitted now\n                - 'finished': int, number of jobs finished\n                - 'failed': int, number of jobs failed\n                - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n        Returns:\n            None\n        \"\"\"\n        logging.info(\"State of the jobs:\")\n        for gen, dic_summary in dic_summary_by_gen.items():\n            logging.info(\"********************************\")\n            logging.info(f\"Generation {gen}\")\n            logging.info(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n            logging.info(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n            logging.info(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n            logging.info(f\"Jobs finished: {dic_summary['finished']}\")\n            logging.info(f\"Jobs failed: {dic_summary['failed']}\")\n            logging.info(\n                f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\"\n            )\n            logging.info(\"********************************\")\n\n    @staticmethod\n    def _check_job_submit_status(\n        job: str,\n        dic_tree: dict[str, Any],\n        dic_all_jobs: dict[str, dict[str, Any]],\n        dic_to_submit_by_gen: dict[int, list[str]],\n        dic_summary_by_gen: dict[int, dict[str, int]],\n        dependency_graph: DependencyGraph,\n    ) -&gt; tuple[dict[int, list[str]], dict[int, dict[str, int]]]:\n        \"\"\"\n        Checks the status and dependencies of a job and updates the submission and summary\n        dictionaries.\n\n        Args:\n            job (str): The job identifier.\n            dic_tree (dict[str, Any]): The dictionary tree structure.\n            dic_all_jobs (dict[str, dict[str,Any]]): A dictionary containing all jobs.\n            dic_to_submit_by_gen (dict[int, list[str]]): A dictionary where keys are generation\n                numbers and values are lists of jobs to submit for each generation.\n            dic_summary_by_gen (dict[int, dict[str, int]]): A dictionary where keys are generation\n                numbers and values are dictionaries summarizing job states.\n            dependency_graph (DependencyGraph): An object to check job dependencies.\n\n        Returns:\n            tuple[dict[int, list[str]], dict[int, dict[str, int]]]: Updated dictionaries for jobs to\n                submit and job summaries.\n        \"\"\"\n        gen = dic_all_jobs[job][\"gen\"]\n        if gen not in dic_to_submit_by_gen:\n            dic_to_submit_by_gen[gen] = []\n            dic_summary_by_gen[gen] = {\n                \"finished\": 0,\n                \"failed\": 0,\n                \"dependency_failed\": 0,\n                \"running_or_queuing\": 0,\n                \"submitted_now\": 0,\n                \"to_submit_later\": 0,\n            }\n        logging.info(f\"Checking job {job} dependencies and status in tree\")\n        l_dep = dependency_graph.get_unfinished_dependency(job)\n        l_dep_failed = dependency_graph.get_failed_dependency(job)\n\n        # Job will be on hold as it has failed dependencies\n        if len(l_dep_failed) &gt; 0:\n            logging.warning(\n                f\"Job {job} has failed dependencies: {l_dep_failed}, it won't be submitted.\"\n            )\n            dic_summary_by_gen[gen][\"dependency_failed\"] += 1\n\n        # Jobs is waiting for dependencies to finish\n        elif len(l_dep) &gt; 0:\n            dic_summary_by_gen[gen][\"to_submit_later\"] += 1\n\n        # Job dependencies are ok\n        elif len(l_dep) == 0:\n            # But job has failed already\n            if nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"failed\":\n                dic_summary_by_gen[gen][\"failed\"] += 1\n\n            # Or job has finished already\n            elif nested_get(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"]) == \"finished\":\n                dic_summary_by_gen[gen][\"finished\"] += 1\n\n            # Else everything is ok, added to the submit dict\n            else:\n                logging.info(f\"Job {job} is added for submission.\")\n                dic_to_submit_by_gen[gen].append(job)\n                # We'll determine which jobs actually have to be submitted and which jobs\n                # are running at the end of the function, after querying the cluster or the local pc\n\n        return dic_to_submit_by_gen, dic_summary_by_gen\n\n    def keep_submit_until_done(\n        self,\n        one_generation_at_a_time: bool = False,\n        wait_time: float = 30,\n        dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n        dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n        dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n        name_config: str = \"config.yaml\",\n    ) -&gt; None:\n        \"\"\"\n        Keeps submitting jobs until all jobs are finished or failed.\n\n        The following arguments are only used for HTC jobs submission:\n        - dic_additional_commands_per_gen\n        - dic_dependencies_per_gen\n        - dic_copy_back_per_gen\n        - name_config\n\n        Args:\n            one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n                time. Defaults to False.\n            wait_time (float, optional): The wait time between submissions in minutes.\n                Defaults to 30.\n            dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n                generation. Defaults to None.\n            dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n                Only used when doing a HTC submission. Defaults to None.\n            dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n                containing the files to copy back per generation. Accepted keys are \"parquet\",\n                \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n                back only \"light\" files, i.e. parquet, yaml and txt.\n            name_config (str, optional): The name of the configuration file for the study.\n                Defaults to \"config.yaml\".\n\n\n        Returns:\n            None\n        \"\"\"\n        # Handle mutable default arguments\n        if dic_additional_commands_per_gen is None:\n            dic_additional_commands_per_gen = {}\n        if dic_dependencies_per_gen is None:\n            dic_dependencies_per_gen = {}\n\n        if wait_time &lt; 1 / 20:\n            logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n            logging.warning(\"Setting wait time to 10 seconds.\")\n            wait_time = 10 / 60\n\n        # I don't need to lock the tree here since the status cheking is read only and\n        # the lock is acquired in the submit method for the submission\n        while (\n            self.submit(\n                one_generation_at_a_time,\n                dic_additional_commands_per_gen,\n                dic_dependencies_per_gen,\n                dic_copy_back_per_gen,\n                name_config,\n            )\n            != \"finished\"\n        ):\n            # Wait for a certain amount of time before checking again\n            logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n            time.sleep(wait_time * 60)\n\n        logging.info(\"All jobs are finished.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.dic_tree","title":"<code>dic_tree: dict</code>  <code>property</code> <code>writable</code>","text":"<p>Loads the dictionary tree from the path.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded dictionary tree.</p>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.__init__","title":"<code>__init__(path_tree, path_python_environment, path_python_environment_container='', path_container_image=None)</code>","text":"<p>Initializes the SubmitScan class.</p> <p>Parameters:</p> Name Type Description Default <code>path_tree</code> <code>str</code> <p>The path to the tree structure.</p> required <code>path_python_environment</code> <code>str</code> <p>The path to the Python environment.</p> required <code>path_python_environment_container</code> <code>str</code> <p>The path to the Python environment in the container. Defaults to \"\".</p> <code>''</code> <code>path_container_image</code> <code>Optional[str]</code> <p>The path to the container image. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def __init__(\n    self,\n    path_tree: str,\n    path_python_environment: str,\n    path_python_environment_container: str = \"\",\n    path_container_image: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the SubmitScan class.\n\n    Args:\n        path_tree (str): The path to the tree structure.\n        path_python_environment (str): The path to the Python environment.\n        path_python_environment_container (str, optional): The path to the Python environment\n            in the container. Defaults to \"\".\n        path_container_image (Optional[str], optional): The path to the container image.\n            Defaults to None.\n    \"\"\"\n    # Path to study files\n    self.path_tree = path_tree\n\n    # Absolute path to the tree\n    self.abs_path_tree = os.path.abspath(path_tree)\n\n    # Name of the study folder\n    self.study_name = os.path.dirname(path_tree)\n\n    # Absolute path to the study folder (get from the path_tree)\n    self.abs_path = os.path.abspath(self.study_name).split(f\"/{self.study_name}\")[0]\n\n    # Path to the python environment, activate with `source path_python_environment`\n    # Turn to absolute path if it is not already\n    if not os.path.isabs(path_python_environment):\n        self.path_python_environment = os.path.abspath(path_python_environment)\n    else:\n        self.path_python_environment = path_python_environment\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment.endswith(\"/bin/activate\"):\n        self.path_python_environment += \"/bin/activate\"\n\n    # Container image (Docker or Singularity, if any)\n    # Turn to absolute path if it is not already\n    if path_container_image is None:\n        self.path_container_image = None\n    elif not os.path.isabs(path_container_image):\n        self.path_container_image = os.path.abspath(path_container_image)\n    else:\n        self.path_container_image = path_container_image\n\n    # Python environment for the container\n    self.path_python_environment_container = path_python_environment_container\n\n    # Ensure that the container image is set if the python environment is set\n    if self.path_container_image and not self.path_python_environment_container:\n        raise ValueError(\n            \"The path to the python environment in the container must be set if the container\"\n            \"image is set.\"\n        )\n\n    # Add /bin/activate to the path_python_environment if needed\n    if not self.path_python_environment_container.endswith(\"/bin/activate\"):\n        self.path_python_environment_container += \"/bin/activate\"\n\n    # Lock file to avoid concurrent access (softlock as several platforms are used)\n    self.lock = SoftFileLock(f\"{self.path_tree}.lock\", timeout=60)\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.check_and_update_all_jobs_status","title":"<code>check_and_update_all_jobs_status()</code>","text":"<p>Checks the status of all jobs and updates their status in the job dictionary.</p> <p>This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in the job's folder, and updates the job's status accordingly. If at least one job is not finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or failed, the overall status is set to \"finished\".</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>tuple[dict[str, Any], str]: A tuple containing:</p> <code>str</code> <ul> <li>A dictionary with all jobs and their updated statuses.</li> </ul> <code>tuple[dict[str, Any], str]</code> <ul> <li>A string representing the final status (\"to_finish\" or \"finished\").</li> </ul> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def check_and_update_all_jobs_status(self) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"\n    Checks the status of all jobs and updates their status in the job dictionary.\n\n    This method iterates through all jobs, checks if a \".finished\" or a \".failed\" file exists in\n    the job's folder, and updates the job's status accordingly. If at least one job is not\n    finished or failed, the overall status is set to \"to_finish\". If all jobs are finished or\n    failed, the overall status is set to \"finished\".\n\n    Returns:\n        tuple[dict[str, Any], str]: A tuple containing:\n        - A dictionary with all jobs and their updated statuses.\n        - A string representing the final status (\"to_finish\" or \"finished\").\n    \"\"\"\n    dic_all_jobs = self.get_all_jobs()\n    at_least_one_job_to_finish = False\n    final_status = \"to_finish\"\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n        for job in dic_all_jobs:\n            relative_job_folder = os.path.dirname(job)\n            absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n            # Check if the file .finished exists\n            if os.path.exists(f\"{absolute_job_folder}/.finished\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"finished\")\n            # Check if the job failed otherwise (not to resubmit it again)\n            elif os.path.exists(f\"{absolute_job_folder}/.failed\"):\n                nested_set(dic_tree, dic_all_jobs[job][\"l_keys\"] + [\"status\"], \"failed\")\n            else:\n                at_least_one_job_to_finish = True\n\n        if not at_least_one_job_to_finish:\n            dic_tree[\"status\"] = final_status = \"finished\"\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n\n    return dic_all_jobs, final_status\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.configure_jobs","title":"<code>configure_jobs(force_configure=False, dic_config_jobs=None)</code>","text":"<p>Configures the jobs by modifying the tree structure and creating the run files for each job.</p> <p>Parameters:</p> Name Type Description Default <code>force_configure</code> <code>bool</code> <p>Whether to force reconfiguration. Defaults to False.</p> <code>False</code> <code>dic_config_jobs</code> <code>Optional[dict[str, dict[str, Any]]]</code> <p>A dictionary containing the configuration of the jobs. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def configure_jobs(\n    self,\n    force_configure: bool = False,\n    dic_config_jobs: Optional[dict[str, dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Configures the jobs by modifying the tree structure and creating the run files for each job.\n\n    Args:\n        force_configure (bool, optional): Whether to force reconfiguration. Defaults to False.\n        dic_config_jobs (Optional[dict[str, dict[str, Any]]], optional): A dictionary containing\n            the configuration of the jobs. Defaults to None.\n    \"\"\"\n    # Lock since we are modifying the tree\n    logging.info(\"Acquiring lock to configure jobs\")\n    with self.lock:\n        # Get the tree\n        dic_tree = self.dic_tree\n\n        # Ensure jobs have not been configured already\n        if (\"configured\" in dic_tree and dic_tree[\"configured\"]) and not force_configure:\n            logging.warning(\"Jobs have already been configured. Skipping.\")\n            return\n\n        # Configure the jobs (add generation and job keys, set status to \"To finish\")\n        dic_tree = ConfigJobs(dic_tree).find_and_configure_jobs(dic_config_jobs)\n\n        # Add the python environment, container image and absolute path of the study to the tree\n        dic_tree[\"python_environment\"] = self.path_python_environment\n        dic_tree[\"container_image\"] = self.path_container_image\n        dic_tree[\"absolute_path\"] = self.abs_path\n        dic_tree[\"status\"] = \"to_finish\"\n        dic_tree[\"configured\"] = True\n\n        # Explicitly set the dic_tree property to force rewrite\n        self.dic_tree = dic_tree\n\n    logging.info(\"Jobs have been configured. Lock released.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.generate_run_files","title":"<code>generate_run_files(dic_tree, l_jobs, dic_additional_commands_per_gen, dic_dependencies_per_gen, dic_copy_back_per_gen, name_config)</code>","text":"<p>Generates run files for the specified jobs.</p> <p>Parameters:</p> Name Type Description Default <code>dic_tree</code> <code>dict</code> <p>The dictionary tree structure.</p> required <code>l_jobs</code> <code>list[str]</code> <p>List of jobs to submit.</p> required <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to {}.</p> required <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission.</p> required <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".</p> required <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated dictionary tree structure.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def generate_run_files(\n    self,\n    dic_tree: dict[str, Any],\n    l_jobs: list[str],\n    dic_additional_commands_per_gen: dict[int, str],\n    dic_dependencies_per_gen: dict[int, list[str]],\n    dic_copy_back_per_gen: dict[int, dict[str, bool]],\n    name_config: str,\n) -&gt; dict:\n    \"\"\"\n    Generates run files for the specified jobs.\n\n    Args:\n        dic_tree (dict): The dictionary tree structure.\n        l_jobs (list[str]): List of jobs to submit.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to {}.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\".\n        name_config (str, optional): The name of the configuration file for the study.\n\n    Returns:\n        dict: The updated dictionary tree structure.\n    \"\"\"\n\n    logging.info(\"Generating run files for the jobs to submit\")\n    # Generate the run files for the jobs to submit\n    dic_all_jobs = self.get_all_jobs()\n    for job in l_jobs:\n        l_keys = dic_all_jobs[job][\"l_keys\"]\n        job_name = os.path.basename(job)\n        relative_job_folder = os.path.dirname(job)\n        absolute_job_folder = f\"{self.abs_path}/{relative_job_folder}\"\n        generation_number = dic_all_jobs[job][\"gen\"]\n        submission_type = nested_get(dic_tree, l_keys + [\"submission_type\"])\n        singularity = \"docker\" in submission_type\n        path_python_environment = (\n            self.path_python_environment_container\n            if singularity\n            else self.path_python_environment\n        )\n\n        # Ensure that the run file does not already exist\n        if \"path_run\" in nested_get(dic_tree, l_keys):\n            path_run_curr = nested_get(dic_tree, l_keys + [\"path_run\"])\n            if path_run_curr is not None and os.path.exists(path_run_curr):\n                logging.info(f\"Run file already exists for job {job}. Skipping.\")\n                continue\n\n        # Build l_dependencies and add to the kwargs\n        l_dependencies = dic_dependencies_per_gen.get(generation_number, [])\n\n        # Get arguments of current generation\n        dic_args = dic_copy_back_per_gen.get(generation_number, {})\n\n        # Mutate the keys\n        dic_args = {f\"copy_back_{key}\": value for key, value in dic_args.items()}\n\n        # Build kwargs for the run file\n        kwargs_htc = {\n            \"l_dependencies\": l_dependencies,\n            \"name_config\": name_config,\n        } | dic_args\n\n        run_str = generate_run_file(\n            absolute_job_folder,\n            job_name,\n            path_python_environment,\n            htc=\"htc\" in submission_type,\n            additionnal_command=dic_additional_commands_per_gen.get(generation_number, \"\"),\n            **kwargs_htc,\n        )\n        # Write the run file\n        path_run_job = f\"{absolute_job_folder}/run.sh\"\n        with open(path_run_job, \"w\") as f:\n            f.write(run_str)\n\n        # Record the path to the run file in the tree\n        nested_set(dic_tree, l_keys + [\"path_run\"], path_run_job)\n\n    return dic_tree\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Retrieves all jobs from the configuration, without modifying the tree.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing all jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def get_all_jobs(self) -&gt; dict:\n    \"\"\"\n    Retrieves all jobs from the configuration, without modifying the tree.\n\n    Returns:\n        dict: A dictionary containing all jobs.\n    \"\"\"\n    # Get a copy of the tree as it's safer\n    with self.lock:\n        dic_tree = self.dic_tree\n    return ConfigJobs(dic_tree).find_all_jobs()\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.keep_submit_until_done","title":"<code>keep_submit_until_done(one_generation_at_a_time=False, wait_time=30, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Keeps submitting jobs until all jobs are finished or failed.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>wait_time</code> <code>float</code> <p>The wait time between submissions in minutes. Defaults to 30.</p> <code>30</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def keep_submit_until_done(\n    self,\n    one_generation_at_a_time: bool = False,\n    wait_time: float = 30,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; None:\n    \"\"\"\n    Keeps submitting jobs until all jobs are finished or failed.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        wait_time (float, optional): The wait time between submissions in minutes.\n            Defaults to 30.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n\n    Returns:\n        None\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n\n    if wait_time &lt; 1 / 20:\n        logging.warning(\"Wait time should be at least 10 seconds to prevent locking errors.\")\n        logging.warning(\"Setting wait time to 10 seconds.\")\n        wait_time = 10 / 60\n\n    # I don't need to lock the tree here since the status cheking is read only and\n    # the lock is acquired in the submit method for the submission\n    while (\n        self.submit(\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n        != \"finished\"\n    ):\n        # Wait for a certain amount of time before checking again\n        logging.info(f\"Waiting {wait_time} minutes before checking again.\")\n        time.sleep(wait_time * 60)\n\n    logging.info(\"All jobs are finished.\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.log_jobs_state","title":"<code>log_jobs_state(dic_summary_by_gen)</code>  <code>staticmethod</code>","text":"<p>Logs the state of jobs for each generation.</p> <p>Parameters:</p> Name Type Description Default <code>dic_summary_by_gen</code> <code>dict</code> <p>A dictionary where the keys are generation numbers and the values are dictionaries summarizing job states. Each summary dictionary should contain the following keys: - 'to_submit_later': int, number of jobs left to submit later - 'running_or_queuing': int, number of jobs running or queuing - 'submitted_now': int, number of jobs submitted now - 'finished': int, number of jobs finished - 'failed': int, number of jobs failed - 'dependency_failed': int, number of jobs on hold due to failed dependencies</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>@staticmethod\ndef log_jobs_state(dic_summary_by_gen: dict[int, dict[str, int]]) -&gt; None:\n    \"\"\"\n    Logs the state of jobs for each generation.\n\n    Args:\n        dic_summary_by_gen (dict): A dictionary where the keys are generation numbers\n            and the values are dictionaries summarizing job states.\n            Each summary dictionary should contain the following keys:\n            - 'to_submit_later': int, number of jobs left to submit later\n            - 'running_or_queuing': int, number of jobs running or queuing\n            - 'submitted_now': int, number of jobs submitted now\n            - 'finished': int, number of jobs finished\n            - 'failed': int, number of jobs failed\n            - 'dependency_failed': int, number of jobs on hold due to failed dependencies\n\n    Returns:\n        None\n    \"\"\"\n    logging.info(\"State of the jobs:\")\n    for gen, dic_summary in dic_summary_by_gen.items():\n        logging.info(\"********************************\")\n        logging.info(f\"Generation {gen}\")\n        logging.info(f\"Jobs left to submit later: {dic_summary['to_submit_later']}\")\n        logging.info(f\"Jobs running or queuing: {dic_summary['running_or_queuing']}\")\n        logging.info(f\"Jobs submitted now: {dic_summary['submitted_now']}\")\n        logging.info(f\"Jobs finished: {dic_summary['finished']}\")\n        logging.info(f\"Jobs failed: {dic_summary['failed']}\")\n        logging.info(\n            f\"Jobs on hold due to failed dependencies: {dic_summary['dependency_failed']}\"\n        )\n        logging.info(\"********************************\")\n</code></pre>"},{"location":"reference/study_da/submit/submit_scan.html#study_da.submit.submit_scan.SubmitScan.submit","title":"<code>submit(one_generation_at_a_time=False, dic_additional_commands_per_gen=None, dic_dependencies_per_gen=None, dic_copy_back_per_gen=None, name_config='config.yaml')</code>","text":"<p>Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders) can trigger a throttling mechanism in AFS.</p> <p>The following arguments are only used for HTC jobs submission: - dic_additional_commands_per_gen - dic_dependencies_per_gen - dic_copy_back_per_gen - name_config</p> <p>Parameters:</p> Name Type Description Default <code>one_generation_at_a_time</code> <code>bool</code> <p>Whether to submit one full generation at a time. Defaults to False.</p> <code>False</code> <code>dic_additional_commands_per_gen</code> <code>dict[int, str]</code> <p>Additional commands per generation. Defaults to None.</p> <code>None</code> <code>dic_dependencies_per_gen</code> <code>dict[int, list[str]]</code> <p>Dependencies per generation. Only used when doing a HTC submission. Defaults to None.</p> <code>None</code> <code>dic_copy_back_per_gen</code> <code>Optional[dict[int, dict[str, bool]]]</code> <p>A dictionary containing the files to copy back per generation. Accepted keys are \"parquet\", \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying back only \"light\" files, i.e. parquet, yaml and txt.</p> <code>None</code> <code>name_config</code> <code>str</code> <p>The name of the configuration file for the study. Defaults to \"config.yaml\".</p> <code>'config.yaml'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The final status of the jobs.</p> Source code in <code>study_da/submit/submit_scan.py</code> <pre><code>def submit(\n    self,\n    one_generation_at_a_time: bool = False,\n    dic_additional_commands_per_gen: Optional[dict[int, str]] = None,\n    dic_dependencies_per_gen: Optional[dict[int, list[str]]] = None,\n    dic_copy_back_per_gen: Optional[dict[int, dict[str, bool]]] = None,\n    name_config: str = \"config.yaml\",\n) -&gt; str:\n    \"\"\"\n    Submits the jobs to the cluster. Note that copying back large files (e.g. json colliders)\n    can trigger a throttling mechanism in AFS.\n\n    The following arguments are only used for HTC jobs submission:\n    - dic_additional_commands_per_gen\n    - dic_dependencies_per_gen\n    - dic_copy_back_per_gen\n    - name_config\n\n    Args:\n        one_generation_at_a_time (bool, optional): Whether to submit one full generation at a\n            time. Defaults to False.\n        dic_additional_commands_per_gen (dict[int, str], optional): Additional commands per\n            generation. Defaults to None.\n        dic_dependencies_per_gen (dict[int, list[str]], optional): Dependencies per generation.\n            Only used when doing a HTC submission. Defaults to None.\n        dic_copy_back_per_gen (Optional[dict[int, dict[str, bool]]], optional): A dictionary\n            containing the files to copy back per generation. Accepted keys are \"parquet\",\n            \"yaml\", \"txt\", \"json\", \"zip\" and \"all\". Defaults to None, corresponding to copying\n            back only \"light\" files, i.e. parquet, yaml and txt.\n        name_config (str, optional): The name of the configuration file for the study.\n            Defaults to \"config.yaml\".\n\n    Returns:\n        str: The final status of the jobs.\n    \"\"\"\n    # Handle mutable default arguments\n    if dic_additional_commands_per_gen is None:\n        dic_additional_commands_per_gen = {}\n    if dic_dependencies_per_gen is None:\n        dic_dependencies_per_gen = {}\n    if dic_copy_back_per_gen is None:\n        dic_copy_back_per_gen = {}\n\n    # Update the status of all jobs before submitting\n    dic_all_jobs, final_status = self.check_and_update_all_jobs_status()\n    if final_status == \"finished\":\n        logging.info(\"All jobs are finished. No need to submit.\")\n        return final_status\n\n    logging.info(\"Acquiring lock to submit jobs\")\n    with self.lock:\n        # Get dic tree once to avoid reloading it for every job\n        dic_tree = self.dic_tree\n\n        # Submit the jobs\n        self._submit(\n            dic_tree,\n            dic_all_jobs,\n            one_generation_at_a_time,\n            dic_additional_commands_per_gen,\n            dic_dependencies_per_gen,\n            dic_copy_back_per_gen,\n            name_config,\n        )\n\n        # Update dic_tree from cluster_submission\n        self.dic_tree = dic_tree\n    logging.info(\"Jobs have been submitted. Lock released.\")\n    return final_status\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html","title":"cluster_submission","text":""},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission","title":"<code>ClusterSubmission</code>","text":"<p>A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and Slurm.</p> <p>Attributes:</p> Name Type Description <code>study_name</code> <code>str</code> <p>The name of the study.</p> <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of jobs to submit.</p> <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree.</p> <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> <code>dic_submission</code> <code>dict</code> <p>A dictionary mapping submission types to their corresponding classes.</p> <p>Methods:</p> Name Description <code>dic_id_to_path_job</code> <p>Getter for the dictionary mapping job IDs to their paths.</p> <code>dic_id_to_path_job</code> <p>dict[int, str]): Setter for the dictionary mapping job IDs to their paths.</p> <code>_update_dic_id_to_path_job</code> <p>list[str], queuing_jobs: list[str]) -&gt; None: Updates the dictionary mapping job IDs to their paths based on the current running and queuing jobs.</p> <code>_check_submission_type</code> <p>Checks the submission type for the jobs and ensures that HTC and Slurm submissions are not mixed.</p> <code>_get_state_jobs</code> <p>bool = True) -&gt; tuple[list[str], list[str]]: Gets the current state of the jobs (running and queuing).</p> <code>_test_job</code> <p>str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool: Tests if a job is completed, running, or queuing.</p> <code>_return_htc_flavour</code> <p>str) -&gt; str: Returns the HTC flavor for a given job.</p> <code>_return_abs_path_job</code> <p>str) -&gt; tuple[str, str]: Returns the absolute path of a job.</p> <code>_write_sub_files_slurm_docker</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]: Writes submission files for Slurm Docker jobs.</p> <code>_get_Sub</code> <p>str, submission_type: str, sub_filename: str, abs_path_job: str, context: str) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: Returns the appropriate submission object based on the submission type.</p> <code>_write_sub_file</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes a submission file for the given jobs.</p> <code>_write_sub_files</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes submission files for the given jobs based on the submission type.</p> <code>write_sub_files</code> <p>Writes submission files for all jobs to be submitted and returns a dictionary of submission files.</p> <code>_update_job_status_from_hpc_output</code> <p>str, submission_type: str, dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)     -&gt; tuple[dict, int]: Updates the job status from the HPC output.</p> <code>submit</code> <p>list[str], l_submission_filenames: list[str], submission_type: str) -&gt; None: Submits the jobs to the appropriate cluster system.</p> <code>_get_local_jobs</code> <p>Gets the list of local jobs.</p> <code>_get_condor_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Condor jobs based on the status.</p> <code>_get_slurm_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Slurm jobs based on the status.</p> <code>querying_jobs</code> <p>bool, check_htc: bool, check_slurm: bool, status: str = \"running\") -&gt; list[str]: Queries the jobs based on the submission type and status.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>class ClusterSubmission:\n    \"\"\"\n    A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and\n    Slurm.\n\n    Attributes:\n        study_name (str): The name of the study.\n        l_jobs_to_submit (list[str]): A list of jobs to submit.\n        dic_all_jobs (dict): A dictionary containing all jobs.\n        dic_tree (dict): A dictionary representing the job tree.\n        path_submission_file (str): The path to the submission file.\n        abs_path_study (str): The absolute path to the study.\n        dic_submission (dict): A dictionary mapping submission types to their corresponding classes.\n\n    Methods:\n        dic_id_to_path_job() -&gt; dict | None:\n            Getter for the dictionary mapping job IDs to their paths.\n        dic_id_to_path_job(dic_id_to_path_job: dict[int, str]):\n            Setter for the dictionary mapping job IDs to their paths.\n        _update_dic_id_to_path_job(running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n            Updates the dictionary mapping job IDs to their paths based on the current running and\n            queuing jobs.\n        _check_submission_type() -&gt; tuple[bool, bool, bool]:\n            Checks the submission type for the jobs and ensures that HTC and Slurm submissions are\n            not mixed.\n        _get_state_jobs(verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n            Gets the current state of the jobs (running and queuing).\n        _test_job(job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool:\n            Tests if a job is completed, running, or queuing.\n        _return_htc_flavour(job: str) -&gt; str:\n            Returns the HTC flavor for a given job.\n        _return_abs_path_job(job: str) -&gt; tuple[str, str]:\n            Returns the absolute path of a job.\n        _write_sub_files_slurm_docker(sub_filename: str, running_jobs: list[str],\n            queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for Slurm Docker jobs.\n        _get_Sub(job: str, submission_type: str, sub_filename: str, abs_path_job: str,\n            context: str) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n            Returns the appropriate submission object based on the submission type.\n        _write_sub_file(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes a submission file for the given jobs.\n        _write_sub_files(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for the given jobs based on the submission type.\n        write_sub_files() -&gt; dict:\n            Writes submission files for all jobs to be submitted and returns a dictionary of\n            submission files.\n        _update_job_status_from_hpc_output(submit_command: str, submission_type: str,\n            dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)\n                -&gt; tuple[dict, int]:\n            Updates the job status from the HPC output.\n        submit(list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str)\n            -&gt; None:\n            Submits the jobs to the appropriate cluster system.\n        _get_local_jobs() -&gt; list[str]:\n            Gets the list of local jobs.\n        _get_condor_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Condor jobs based on the status.\n        _get_slurm_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Slurm jobs based on the status.\n        querying_jobs(check_local: bool, check_htc: bool, check_slurm: bool,\n            status: str = \"running\") -&gt; list[str]:\n            Queries the jobs based on the submission type and status.\n    \"\"\"\n\n    def __init__(\n        self,\n        study_name: str,\n        l_jobs_to_submit: list[str],\n        dic_all_jobs: dict,\n        dic_tree: dict,\n        path_submission_file: str,\n        abs_path_study: str,\n    ):\n        self.study_name: str = study_name\n        self.l_jobs_to_submit: list[str] = l_jobs_to_submit\n        self.dic_all_jobs: dict = dic_all_jobs\n        self.dic_tree: dict = dic_tree\n        self.path_submission_file: str = path_submission_file\n        self.abs_path_study: str = abs_path_study\n        self.dic_submission: dict = {\n            \"local\": LocalPC,\n            \"htc\": HTC,\n            \"htc_docker\": HTCDocker,\n            \"slurm\": Slurm,\n            \"slurm_docker\": SlurmDocker,\n        }\n        \"\"\"\n        Initialize the ClusterSubmission class.\n\n        Args:\n            study_name (str): The name of the study.\n            l_jobs_to_submit (list[str]): A list of job names to submit.\n            dic_all_jobs (dict): A dictionary containing all jobs.\n            dic_tree (dict): A dictionary representing the job tree structure.\n            path_submission_file (str): The path to the submission file.\n            abs_path_study (str): The absolute path to the study.\n        \"\"\"\n\n    # Getter for dic_id_to_path_job\n    @property\n    def dic_id_to_path_job(self) -&gt; dict | None:\n        \"\"\"\n        Generates a dictionary mapping job IDs to their respective job paths.\n\n        This method iterates over the list of jobs to submit and constructs a dictionary\n        where the keys are job IDs and the values are the absolute paths to the jobs.\n        If no job IDs are found, the method returns None.\n\n        Returns:\n            dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.\n        \"\"\"\n        dic_id_to_path_job = {}\n        found_at_least_one = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job:\n                dic_id_to_path_job[subdic_job[\"id_sub\"]] = self._return_abs_path_job(job)[0]\n                found_at_least_one = True\n\n        return dic_id_to_path_job if found_at_least_one else None\n\n    # Setter for dic_id_to_path_job\n    @dic_id_to_path_job.setter\n    def dic_id_to_path_job(self, dic_id_to_path_job: dict[int, str]):\n        \"\"\"\n        Updates the internal job submission tree with job IDs and their corresponding paths.\n\n        Args:\n            dic_id_to_path_job (dict[int, str]): A dictionary mapping job IDs (integers) to their\n            respective paths (strings).\n\n        Raises:\n            AssertionError: If dic_id_to_path_job is not a dictionary.\n\n        Notes:\n            - Ensures all job IDs are integers.\n            - Updates the internal job submission tree by adding or removing job IDs based on the\n                provided dictionary.\n            - If a job's path is found in the dictionary, its ID is updated in the tree.\n            - If a job's ID is not found in the dictionary, it is removed from the tree.\n        \"\"\"\n        assert isinstance(dic_id_to_path_job, dict)\n        # Ensure all ids are integers\n        dic_id_to_path_job = {\n            int(id_job): path_job for id_job, path_job in dic_id_to_path_job.items()\n        }\n        dic_job_to_id = {path_job: int(id_job) for id_job, path_job in dic_id_to_path_job.items()}\n\n        # Update the tree\n        for job in self.l_jobs_to_submit:\n            path_job = self._return_abs_path_job(job)[0]\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job and int(subdic_job[\"id_sub\"]) not in dic_id_to_path_job:\n                del subdic_job[\"id_sub\"]\n            elif \"id_sub\" not in subdic_job and path_job in dic_job_to_id:\n                subdic_job[\"id_sub\"] = dic_job_to_id[path_job]\n            # Else all is consistent\n\n    def _update_dic_id_to_path_job(self, running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n        \"\"\"\n        Updates the dictionary `dic_id_to_path_job` by removing jobs that are no longer running or\n        queuing.\n\n        Args:\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n\n        Returns:\n            None\n        \"\"\"\n        # Look for jobs in the dictionnary that are not running or queuing anymore\n        set_current_jobs = set(running_jobs + queuing_jobs)\n        if self.dic_id_to_path_job is not None:\n            dic_id_to_path_job = self.dic_id_to_path_job\n            for id_job, job in self.dic_id_to_path_job.items():\n                if job not in set_current_jobs:\n                    del dic_id_to_path_job[id_job]\n\n            # Update dic_id_to_path_job\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n    def _check_submission_type(self) -&gt; tuple[bool, bool, bool]:\n        \"\"\"\n        Checks the types of job submissions in the current batch and ensures that\n        there is no mixing of HTC and Slurm submission types.\n\n        Args:\n            None\n\n        Returns:\n            tuple[bool, bool, bool]: A tuple containing three boolean values indicating\n            whether there are local, HTC, and Slurm submissions respectively.\n\n        Raises:\n            ValueError: If both HTC and Slurm submission types are found in the jobs to submit.\n        \"\"\"\n        check_local = False\n        check_htc = False\n        check_slurm = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            if submission_type == \"local\":\n                check_local = True\n            elif submission_type in [\"htc\", \"htc_docker\"]:\n                check_htc = True\n            elif submission_type in [\"slurm\", \"slurm_docker\"]:\n                check_slurm = True\n\n        if check_htc and check_slurm:\n            raise ValueError(\"Error: Mixing htc and slurm submission is not allowed\")\n\n        return check_local, check_htc, check_slurm\n\n    def _get_state_jobs(self, verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Retrieves the state of jobs (running and queuing) based on the submission type.\n\n        This method first determines the submission type (local, HTC, or Slurm) and then queries\n        the jobs accordingly. It updates the internal dictionary mapping job IDs to their paths\n        and optionally prints the running and queuing jobs.\n\n        Args:\n            verbose (bool): If True, prints the running and queuing jobs. Default is True.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - The first list contains the IDs of running jobs.\n                - The second list contains the IDs of queuing jobs.\n        \"\"\"\n        # First check whether the jobs are submitted on local, htc or slurm\n        check_local, check_htc, check_slurm = self._check_submission_type()\n\n        # Then query accordingly\n        running_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"running\")\n        queuing_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"queuing\")\n        self._update_dic_id_to_path_job(running_jobs, queuing_jobs)\n        if verbose:\n            logging.info(\"Running: \\n\" + \"\\n\".join(running_jobs))\n            logging.info(\"queuing: \\n\" + \"\\n\".join(queuing_jobs))\n        return running_jobs, queuing_jobs\n\n    def _test_job(\n        self, job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]\n    ) -&gt; bool:\n        \"\"\"\n        Tests the status of a job and determines if it needs to be (re)submitted.\n\n        Args:\n            job (str): The job identifier.\n            path_job (str): The path to the job.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of currently queuing jobs.\n\n        Returns:\n            bool: True if the job must be (re)submitted, False otherwise.\n        \"\"\"\n        # Test if job is completed\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        completed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"finished\"\n        failed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        if completed:\n            logging.info(f\"{path_job} is already completed.\")\n\n        # Test if job has failed\n        if failed:\n            logging.info(f\"{path_job} has failed.\")\n\n        # Test if job is running\n        elif path_job in running_jobs:\n            logging.info(f\"{path_job} is already running.\")\n\n        # Test if job is queuing\n        elif path_job in queuing_jobs:\n            logging.info(f\"{path_job} is already queuing.\")\n\n        # True if job must be (re)submitted\n        else:\n            return True\n        return False\n\n    def _return_htc_flavour(self, job: str) -&gt; str:\n        \"\"\"\n        Retrieve the HTC flavor for a given job.\n\n        Args:\n            job (str): The job identifier.\n\n        Returns:\n            str: The HTC flavor associated with the job.\n        \"\"\"\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        return nested_get(self.dic_tree, l_keys + [\"htc_flavor\"])\n\n    def _return_abs_path_job(self, job: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Generate the relative and absolute paths for a given job.\n\n        Args:\n            job (str): The job string containing the path to the job file.\n\n        Returns:\n            tuple[str, str]: A tuple containing:\n            - path_job (str): The relative path to the job directory.\n            - abs_path_job (str): The absolute path to the job directory.\n        \"\"\"\n        # Get corresponding path job (remove the python file name)\n        path_job = \"/\".join(job.split(\"/\")[:-1]) + \"/\"\n        abs_path_job = f\"{self.abs_path_study}/{path_job}\"\n        return path_job, abs_path_job\n\n    def _write_sub_files_slurm_docker(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Generates SLURM submission files for Docker jobs and writes them to disk.\n\n        This method iterates over a list of jobs, checks their status (running, queuing, or\n        completed), and writes the corresponding SLURM submission files for Docker jobs. The\n        submission files are written to disk with a specific naming convention.\n\n        Args:\n            sub_filename (str): The base name for the submission files.\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n            list_of_jobs (list[str]): A list of job identifiers to process.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n            - A list of filenames for the generated submission files.\n            - A list of job identifiers that were updated.\n        \"\"\"\n        l_filenames = []\n        list_of_jobs_updated = []\n        for idx_job, job in enumerate(list_of_jobs):\n            path_job, abs_path_job = self._return_abs_path_job(job)\n\n            # Test if job is running, queuing or completed\n            if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                filename_sub = f\"{sub_filename.split('.sub')[0]}_{idx_job}.sub\"\n\n                # Get job context\n                l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                context = nested_get(self.dic_tree, l_keys + [\"context\"])\n\n                # Write the submission files\n                # ! Careful, I implemented a fix for path due to the temporary home recovery folder\n                logging.info(f'Writing submission file for node \"{abs_path_job}\"')\n                fix = True\n                Sub = self.dic_submission[\"slurm_docker\"](\n                    filename_sub, abs_path_job, context, self.dic_tree[\"container_image\"], fix=fix\n                )\n                with open(filename_sub, \"w\") as fid:\n                    fid.write(Sub.head + \"\\n\")\n                    if fix:\n                        fid.write(Sub.str_fixed_run + \"\\n\")\n                    fid.write(Sub.body + \"\\n\")\n                    fid.write(Sub.tail + \"\\n\")\n\n                l_filenames.append(filename_sub)\n                list_of_jobs_updated.append(job)\n        return l_filenames, list_of_jobs_updated\n\n    def _get_Sub(\n        self, job: str, submission_type: str, sub_filename: str, abs_path_job: str, context: str\n    ) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n        \"\"\"\n        Generate a submission object based on the specified submission type.\n\n        Args:\n            job (str): The job identifier.\n            submission_type (str): The type of submission (e.g., \"slurm\", \"htc\", \"htc_docker\",\n                \"slurm_docker\", \"local\").\n            sub_filename (str): The submission filename.\n            abs_path_job (str): The absolute path to the job.\n            context (str): The context for the submission.\n\n        Returns:\n            LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: An instance of the appropriate\n                submission class.\n\n        Raises:\n            ValueError: If the submission type is not valid or if the container_image is not defined\n                in the tree for docker submissions.\n        \"\"\"\n        match submission_type:\n            case \"slurm\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job, context)\n            case \"htc\":\n                return self.dic_submission[submission_type](\n                    sub_filename, abs_path_job, context, self._return_htc_flavour(job)\n                )\n            case w if w in [\"htc_docker\", \"slurm_docker\"]:\n                # Path to singularity image\n                if (\n                    \"container_image\" in self.dic_tree\n                    and self.dic_tree[\"container_image\"] is not None\n                ):\n                    self.path_image = self.dic_tree[\"container_image\"]\n                else:\n                    raise ValueError(\n                        \"Error: container_image is not defined in the tree. Please define it in the\"\n                        \" config.yaml file.\"\n                    )\n\n                if submission_type == \"htc_docker\":\n                    return self.dic_submission[submission_type](\n                        sub_filename,\n                        abs_path_job,\n                        context,\n                        self.path_image,\n                        self._return_htc_flavour(job),\n                    )\n                else:\n                    return self.dic_submission[submission_type](\n                        sub_filename, abs_path_job, context, self.path_image\n                    )\n            case \"local\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job)\n            case _:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    def _write_sub_file(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes a submission file for a list of jobs and returns the updated list of jobs.\n\n        Args:\n            sub_filename (str): The filename for the submission file.\n            running_jobs (list[str]): List of currently running jobs.\n            queuing_jobs (list[str]): List of currently queuing jobs.\n            list_of_jobs (list[str]): List of jobs to be submitted.\n            submission_type (str): The type of submission.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing:\n            - A list with the submission filename if the file was created, otherwise an empty list.\n            - An updated list of jobs that were included in the submission file.\n        \"\"\"\n        # Flag to know if the file can be submitted (at least one job in it)\n        ok_to_submit = False\n\n        # Flat to know if the header has been written\n        header_written = False\n\n        # Create folder to the submission file if it does not exist\n        os.makedirs(\"/\".join(sub_filename.split(\"/\")[:-1]), exist_ok=True)\n\n        # Updated list of jobs (without unsubmitted jobs)\n        list_of_jobs_updated = []\n\n        # Write the submission file\n        Sub = None\n        with open(sub_filename, \"w\") as fid:\n            for job in list_of_jobs:\n                # Get corresponding path job (remove the python file name)\n                path_job, abs_path_job = self._return_abs_path_job(job)\n\n                # Test if job is running, queuing or completed\n                if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                    logging.info(f'Writing submission command for node \"{abs_path_job}\"')\n\n                    # Get context\n                    l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                    context = nested_get(self.dic_tree, l_keys + [\"context\"])\n\n                    # Get Submission object\n                    Sub = self._get_Sub(job, submission_type, sub_filename, abs_path_job, context)\n\n                    # Take the first job as reference for head\n                    if not header_written:\n                        fid.write(Sub.head + \"\\n\")\n                        header_written = True\n\n                    # Write instruction for submission\n                    fid.write(Sub.body + \"\\n\")\n\n                    # Append job to list_of_jobs_updated\n                    list_of_jobs_updated.append(job)\n\n            # Tail instruction\n            if Sub is not None:\n                fid.write(Sub.tail + \"\\n\")\n                ok_to_submit = True\n\n        if not ok_to_submit:\n            os.remove(sub_filename)\n\n        return ([sub_filename], list_of_jobs_updated) if ok_to_submit else ([], [])\n\n    def _write_sub_files(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes submission files based on the specified submission type.\n\n        Args:\n            sub_filename (str): The name of the submission file to be created.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of jobs that are queued.\n            list_of_jobs (list[str]): A list of all jobs to be submitted.\n            submission_type (str): The type of submission system being used (e.g., \"slurm_docker\").\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - Updated list of running jobs.\n                - Updated list of queuing jobs.\n        \"\"\"\n        # Slurm docker is a peculiar case as one submission file must be created per job\n        if submission_type == \"slurm_docker\":\n            return self._write_sub_files_slurm_docker(\n                sub_filename, running_jobs, queuing_jobs, list_of_jobs\n            )\n\n        else:\n            return self._write_sub_file(\n                sub_filename,\n                running_jobs,\n                queuing_jobs,\n                list_of_jobs,\n                submission_type,\n            )\n\n    def write_sub_files(\n        self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Generates and writes submission files for jobs based on their submission type.\n\n        This method categorizes jobs to be submitted by their submission type, writes the\n        corresponding submission files, and returns a dictionary containing the submission\n        files and their associated job contexts.\n\n        Returns:\n            dict: A dictionary where keys are submission types and values are tuples containing\n                a list of updated jobs and a list of submission filenames.\n        \"\"\"\n        running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n        # Make a dict of all jobs to submit depending on the submission type\n        dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n        # Write submission files for each submission type\n        dic_submission_files = {}\n        for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n            if len(list_of_jobs) &gt; 0:\n                # Write submission files\n                l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                    self.path_submission_file,\n                    running_jobs,\n                    queuing_jobs,\n                    copy.copy(list_of_jobs),\n                    submission_type,\n                )\n\n                # Record submission files and context\n                dic_submission_files[submission_type] = (\n                    list_of_jobs_updated,\n                    l_submission_filenames,\n                )\n\n                # Update dic_summary_by_gen inplace\n                if dic_summary_by_gen is not None:\n                    for job in list_of_jobs:\n                        gen = self.dic_all_jobs[job][\"gen\"]\n                        if job in list_of_jobs_updated:\n                            dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                        else:\n                            dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n        return dic_submission_files\n\n    def _update_job_status_from_hpc_output(\n        self,\n        submit_command: str,\n        submission_type: str,\n        dic_id_to_path_job_temp: dict,\n        list_of_jobs: list[str],\n        idx_submission: int = 0,\n    ) -&gt; tuple[dict, int]:\n        \"\"\"\n        Updates the job status from the HPC output.\n\n        This method parses the output of a job submission command to update the job status\n        in a dictionary mapping job IDs to their respective paths. It supports both HTC and\n        SLURM submission types.\n\n        Args:\n            submit_command (str): The command used to submit the job.\n            submission_type (str): The type of submission system ('htc' or 'slurm').\n            dic_id_to_path_job_temp (dict): A dictionary mapping job IDs to their paths.\n            list_of_jobs (list[str]): A list of job paths.\n            idx_submission (int, optional): The index of the current submission. Defaults to 0.\n\n        Returns:\n            tuple[dict, int]: A tuple containing the updated dictionary and the updated index of\n                submission.\n\n        Raises:\n            RuntimeError: If there is an error in the submission process.\n        \"\"\"\n        process = subprocess.run(\n            submit_command.split(\" \"),\n            capture_output=True,\n        )\n\n        output = process.stdout.decode(\"utf-8\")\n        output_error = process.stderr.decode(\"utf-8\")\n        if \"ERROR\" in output_error:\n            raise RuntimeError(f\"Error in submission: {output_error}\")\n        for line in output.split(\"\\n\"):\n            if \"htc\" in submission_type:\n                if \"cluster\" in line:\n                    cluster_id = int(line.split(\"cluster \")[1][:-1])\n                    dic_id_to_path_job_temp[cluster_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n            elif \"slurm\" in submission_type:\n                if \"Submitted\" in line:\n                    job_id = int(line.split(\" \")[3])\n                    dic_id_to_path_job_temp[job_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n\n        return dic_id_to_path_job_temp, idx_submission\n\n    def submit(\n        self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n    ) -&gt; None:\n        \"\"\"\n        Submits a list of jobs to the specified submission system.\n\n        Args:\n            list_of_jobs (list[str]): List of job identifiers to be submitted.\n            l_submission_filenames (list[str]): List of filenames containing submission scripts.\n            submission_type (str): Type of submission system to use. Valid options are \"local\",\n                \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n        Raises:\n            ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n                submission type.\n            ValueError: If the submission type is not valid.\n\n        Returns:\n            None\n        \"\"\"\n        # Check that the submission file(s) is/are appropriate for the submission mode\n        if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n            raise ValueError(\n                \"Error: Multiple submission files should not be implemented for this submission\"\n                \" mode\"\n            )\n\n        # Check that at least one job is being submitted\n        if not l_submission_filenames:\n            logging.info(\"No job being submitted.\")\n\n        # Submit\n        dic_id_to_path_job_temp = {}\n        idx_submission = 0\n        for sub_filename in l_submission_filenames:\n            if submission_type == \"local\":\n                os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n            elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n                submit_command = self.dic_submission[submission_type].get_submit_command(\n                    sub_filename\n                )\n                dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                    submit_command,\n                    submission_type,\n                    dic_id_to_path_job_temp,\n                    list_of_jobs,\n                    idx_submission,\n                )\n            else:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n        # Update and write the id-job file\n        if dic_id_to_path_job_temp:\n            assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n        # Merge with the previous id-job file\n        dic_id_to_path_job = self.dic_id_to_path_job\n\n        # Update and write on disk\n        if dic_id_to_path_job is not None:\n            dic_id_to_path_job.update(dic_id_to_path_job_temp)\n            self.dic_id_to_path_job = dic_id_to_path_job\n        elif dic_id_to_path_job_temp:\n            dic_id_to_path_job = dic_id_to_path_job_temp\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n        logging.info(\"Jobs status after submission:\")\n        _, _ = self._get_state_jobs(verbose=True)\n\n    def _get_local_jobs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieves a list of local job paths.\n        This method scans the current processes to identify jobs that are running\n        a script named 'run.sh'. It extracts the job paths and filters them to\n        include only the paths that are relevant to the current study.\n\n        Args:\n            None\n\n        Returns:\n            list[str]: A list of job paths that are currently running and relevant\n            to the study.\n        \"\"\"\n\n        l_path_jobs = []\n        # Warning, does not work at the moment in lxplus...\n        for ps in psutil.pids():\n            try:\n                aux = psutil.Process(ps).cmdline()\n            except Exception:\n                aux = []\n            if len(aux) &gt; 1 and \"run.sh\" in aux[-1]:\n                job = str(Path(aux[-1]).parent)\n\n                # Only get path after name of the study\n                try:\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}/\")\n                except IndexError:\n                    logging.warning(\n                        \"Some jobs from another study are running. Acquiring the full path as the \"\n                        \"study name is unknown.\"\n                    )\n                    l_path_jobs.append(job)\n        return l_path_jobs\n\n    def _get_condor_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve the paths of Condor jobs based on their status.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Can be \"running\" or \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually if the\n                id-job file is missing. Defaults to False.\n\n        Returns:\n            list[str]: A list of paths to the jobs that match the specified status.\n\n        Raises:\n            ValueError: If the status provided is not \"running\" or \"queuing\".\n\n        Notes:\n            - The method relies on the `condor_q` command to retrieve job information.\n            - If the id-job file is missing and `force_query_individually` is False, jobs not in\n                `dic_id_to_path_job` will be ignored.\n            - If `force_query_individually` is True, the method will query each job individually to\n                retrieve its path.\n            - Warnings are printed if jobs are found that are not in the id-job file or if the\n                id-job file is missing.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": 1, \"queuing\": 2}\n        condor_output = subprocess.run([\"condor_q\"], capture_output=True).stdout.decode(\"utf-8\")\n\n        # Check which jobs are running\n        first_line = True\n        first_missing_job = True\n        for line in condor_output.split(\"\\n\")[4:]:\n            if line == \"\":\n                break\n            jobid = int(line.split(\"ID:\")[1][1:8])\n            condor_status = line.split(\"      \")[1:5]  # Done, Run, Idle, and potentially Hold\n\n            # If job is running/queuing, get the path to the job\n            if condor_status[dic_status[status]] == \"1\":\n                # Get path from dic_id_to_path_job if available\n                if self.dic_id_to_path_job is not None:\n                    if jobid in self.dic_id_to_path_job:\n                        l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                    elif first_missing_job:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and are not in the id-job\"\n                            \" file. They may come from another study. Ignoring them.\"\n                        )\n                        first_missing_job = False\n\n                elif force_query_individually:\n                    if first_line:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and the id-job file is\"\n                            \" missing... Querying them individually.\"\n                        )\n                        first_line = False\n                    job_details = subprocess.run(\n                        [\"condor_q\", \"-l\", f\"{jobid}\"], capture_output=True\n                    ).stdout.decode(\"utf-8\")\n                    job = job_details.split('Cmd = \"')[1].split(\"run.sh\")[0]\n\n                    # Only get path after master_study\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}\")\n\n                elif first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Ignoring them.\"\n                    )\n                    first_line = False\n\n        return l_path_jobs\n\n    def _get_slurm_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve a list of SLURM job paths based on their status.\n\n        This method queries SLURM to get the job IDs and their statuses for the current user.\n        It then attempts to map these job IDs to their corresponding paths using an internal\n        dictionary. If the dictionary is not available or does not contain the job ID, it can\n        optionally query each job individually for its details.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Expected values are \"running\" or\n                \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually for its\n                details when the job ID is not found in the internal dictionary. Defaults to False.\n\n        Returns:\n            list[str]: A list of job paths corresponding to the specified status.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": \"RUNNING\", \"queuing\": \"PENDING\"}\n        username = (\n            subprocess.run([\"id\", \"-u\", \"-n\"], capture_output=True).stdout.decode(\"utf-8\").strip()\n        )\n        slurm_output = subprocess.run(\n            [\"squeue\", \"-u\", f\"{username}\", \"-t\", dic_status[status]], capture_output=True\n        ).stdout.decode(\"utf-8\")\n\n        # Get job id and details\n        first_line = True\n        first_missing_job = True\n        for line in slurm_output.split(\"\\n\")[1:]:\n            l_split = line.split()\n            if len(l_split) == 0:\n                break\n            jobid = int(l_split[0])\n            slurm_status = l_split[4]  # R or PD  # noqa: F841\n\n            # Get path from dic_id_to_path_job if available\n            if self.dic_id_to_path_job is not None:\n                if jobid in self.dic_id_to_path_job:\n                    l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                elif first_missing_job:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and are not in the id-job\"\n                        \" file. They may come from another study. Ignoring them.\"\n                    )\n                    first_missing_job = False\n\n            elif force_query_individually:\n                if first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Querying them individually.\"\n                    )\n                    first_line = False\n                job_details = subprocess.run(\n                    [\"scontrol\", \"show\", \"jobid\", \"-dd\", f\"{jobid}\"], capture_output=True\n                ).stdout.decode(\"utf-8\")\n                job = (\n                    job_details.split(\"Command=\")[1].split(\"run.sh\")[0]\n                    if \"run.sh\" in job_details\n                    else job_details.split(\"StdOut=\")[1].split(\"output.txt\")[0]\n                )\n                # Only get path after study_name\n                job = job.split(self.study_name)[1]\n                l_path_jobs.append(f\"{self.study_name}{job}\")\n\n            elif first_line:\n                logging.warning(\n                    \"Warning, some jobs are queuing/running and the id-job file is\"\n                    \" missing... Ignoring them.\"\n                )\n                first_line = False\n\n        return l_path_jobs\n\n    def querying_jobs(\n        self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n    ) -&gt; list[str]:\n        \"\"\"\n        Queries jobs from different job management systems based on the provided flags and status.\n\n        Args:\n            check_local (bool): If True, check for local jobs.\n            check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n            check_slurm (bool): If True, check for SLURM jobs.\n            status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n        Returns:\n            list[str]: A list of job paths that match the query criteria.\n        \"\"\"\n        # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n        l_path_jobs = []\n        if check_local:\n            if status == \"running\":\n                l_path_jobs.extend(self._get_local_jobs())\n            else:\n                # Always empty return as there is no queuing in local pc\n                pass\n\n        if check_htc:\n            l_path_jobs.extend(self._get_condor_jobs(status))\n\n        if check_slurm:\n            l_path_jobs.extend(self._get_slurm_jobs(status))\n\n        return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.dic_id_to_path_job","title":"<code>dic_id_to_path_job: dict | None</code>  <code>property</code> <code>writable</code>","text":"<p>Generates a dictionary mapping job IDs to their respective job paths.</p> <p>This method iterates over the list of jobs to submit and constructs a dictionary where the keys are job IDs and the values are the absolute paths to the jobs. If no job IDs are found, the method returns None.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.</p>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.dic_submission","title":"<code>dic_submission: dict = {'local': LocalPC, 'htc': HTC, 'htc_docker': HTCDocker, 'slurm': Slurm, 'slurm_docker': SlurmDocker}</code>  <code>instance-attribute</code>","text":"<p>Initialize the ClusterSubmission class.</p> <p>Parameters:</p> Name Type Description Default <code>study_name</code> <code>str</code> <p>The name of the study.</p> required <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of job names to submit.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> required <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree structure.</p> required <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> required <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> required"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.querying_jobs","title":"<code>querying_jobs(check_local, check_htc, check_slurm, status='running')</code>","text":"<p>Queries jobs from different job management systems based on the provided flags and status.</p> <p>Parameters:</p> Name Type Description Default <code>check_local</code> <code>bool</code> <p>If True, check for local jobs.</p> required <code>check_htc</code> <code>bool</code> <p>If True, check for HTC (High Throughput Computing) jobs.</p> required <code>check_slurm</code> <code>bool</code> <p>If True, check for SLURM jobs.</p> required <code>status</code> <code>str</code> <p>The status of the jobs to query. Defaults to \"running\".</p> <code>'running'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of job paths that match the query criteria.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def querying_jobs(\n    self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n) -&gt; list[str]:\n    \"\"\"\n    Queries jobs from different job management systems based on the provided flags and status.\n\n    Args:\n        check_local (bool): If True, check for local jobs.\n        check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n        check_slurm (bool): If True, check for SLURM jobs.\n        status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n    Returns:\n        list[str]: A list of job paths that match the query criteria.\n    \"\"\"\n    # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n    l_path_jobs = []\n    if check_local:\n        if status == \"running\":\n            l_path_jobs.extend(self._get_local_jobs())\n        else:\n            # Always empty return as there is no queuing in local pc\n            pass\n\n    if check_htc:\n        l_path_jobs.extend(self._get_condor_jobs(status))\n\n    if check_slurm:\n        l_path_jobs.extend(self._get_slurm_jobs(status))\n\n    return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.submit","title":"<code>submit(list_of_jobs, l_submission_filenames, submission_type)</code>","text":"<p>Submits a list of jobs to the specified submission system.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_jobs</code> <code>list[str]</code> <p>List of job identifiers to be submitted.</p> required <code>l_submission_filenames</code> <code>list[str]</code> <p>List of filenames containing submission scripts.</p> required <code>submission_type</code> <code>str</code> <p>Type of submission system to use. Valid options are \"local\", \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple submission files are provided for a non-\"slurm_docker\" submission type.</p> <code>ValueError</code> <p>If the submission type is not valid.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def submit(\n    self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n) -&gt; None:\n    \"\"\"\n    Submits a list of jobs to the specified submission system.\n\n    Args:\n        list_of_jobs (list[str]): List of job identifiers to be submitted.\n        l_submission_filenames (list[str]): List of filenames containing submission scripts.\n        submission_type (str): Type of submission system to use. Valid options are \"local\",\n            \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n    Raises:\n        ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n            submission type.\n        ValueError: If the submission type is not valid.\n\n    Returns:\n        None\n    \"\"\"\n    # Check that the submission file(s) is/are appropriate for the submission mode\n    if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n        raise ValueError(\n            \"Error: Multiple submission files should not be implemented for this submission\"\n            \" mode\"\n        )\n\n    # Check that at least one job is being submitted\n    if not l_submission_filenames:\n        logging.info(\"No job being submitted.\")\n\n    # Submit\n    dic_id_to_path_job_temp = {}\n    idx_submission = 0\n    for sub_filename in l_submission_filenames:\n        if submission_type == \"local\":\n            os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n        elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n            submit_command = self.dic_submission[submission_type].get_submit_command(\n                sub_filename\n            )\n            dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                submit_command,\n                submission_type,\n                dic_id_to_path_job_temp,\n                list_of_jobs,\n                idx_submission,\n            )\n        else:\n            raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    # Update and write the id-job file\n    if dic_id_to_path_job_temp:\n        assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n    # Merge with the previous id-job file\n    dic_id_to_path_job = self.dic_id_to_path_job\n\n    # Update and write on disk\n    if dic_id_to_path_job is not None:\n        dic_id_to_path_job.update(dic_id_to_path_job_temp)\n        self.dic_id_to_path_job = dic_id_to_path_job\n    elif dic_id_to_path_job_temp:\n        dic_id_to_path_job = dic_id_to_path_job_temp\n        self.dic_id_to_path_job = dic_id_to_path_job\n\n    logging.info(\"Jobs status after submission:\")\n    _, _ = self._get_state_jobs(verbose=True)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/index.html#study_da.submit.cluster_submission.ClusterSubmission.write_sub_files","title":"<code>write_sub_files(dic_summary_by_gen=None)</code>","text":"<p>Generates and writes submission files for jobs based on their submission type.</p> <p>This method categorizes jobs to be submitted by their submission type, writes the corresponding submission files, and returns a dictionary containing the submission files and their associated job contexts.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are submission types and values are tuples containing a list of updated jobs and a list of submission filenames.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def write_sub_files(\n    self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n) -&gt; dict:\n    \"\"\"\n    Generates and writes submission files for jobs based on their submission type.\n\n    This method categorizes jobs to be submitted by their submission type, writes the\n    corresponding submission files, and returns a dictionary containing the submission\n    files and their associated job contexts.\n\n    Returns:\n        dict: A dictionary where keys are submission types and values are tuples containing\n            a list of updated jobs and a list of submission filenames.\n    \"\"\"\n    running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n    # Make a dict of all jobs to submit depending on the submission type\n    dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n    for job in self.l_jobs_to_submit:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n        dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n    # Write submission files for each submission type\n    dic_submission_files = {}\n    for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n        if len(list_of_jobs) &gt; 0:\n            # Write submission files\n            l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                self.path_submission_file,\n                running_jobs,\n                queuing_jobs,\n                copy.copy(list_of_jobs),\n                submission_type,\n            )\n\n            # Record submission files and context\n            dic_submission_files[submission_type] = (\n                list_of_jobs_updated,\n                l_submission_filenames,\n            )\n\n            # Update dic_summary_by_gen inplace\n            if dic_summary_by_gen is not None:\n                for job in list_of_jobs:\n                    gen = self.dic_all_jobs[job][\"gen\"]\n                    if job in list_of_jobs_updated:\n                        dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                    else:\n                        dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n    return dic_submission_files\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html","title":"cluster_submission","text":"<p>This module contains a class for submitting jobs on a cluster (or locally).</p>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission","title":"<code>ClusterSubmission</code>","text":"<p>A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and Slurm.</p> <p>Attributes:</p> Name Type Description <code>study_name</code> <code>str</code> <p>The name of the study.</p> <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of jobs to submit.</p> <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree.</p> <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> <code>dic_submission</code> <code>dict</code> <p>A dictionary mapping submission types to their corresponding classes.</p> <p>Methods:</p> Name Description <code>dic_id_to_path_job</code> <p>Getter for the dictionary mapping job IDs to their paths.</p> <code>dic_id_to_path_job</code> <p>dict[int, str]): Setter for the dictionary mapping job IDs to their paths.</p> <code>_update_dic_id_to_path_job</code> <p>list[str], queuing_jobs: list[str]) -&gt; None: Updates the dictionary mapping job IDs to their paths based on the current running and queuing jobs.</p> <code>_check_submission_type</code> <p>Checks the submission type for the jobs and ensures that HTC and Slurm submissions are not mixed.</p> <code>_get_state_jobs</code> <p>bool = True) -&gt; tuple[list[str], list[str]]: Gets the current state of the jobs (running and queuing).</p> <code>_test_job</code> <p>str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool: Tests if a job is completed, running, or queuing.</p> <code>_return_htc_flavour</code> <p>str) -&gt; str: Returns the HTC flavor for a given job.</p> <code>_return_abs_path_job</code> <p>str) -&gt; tuple[str, str]: Returns the absolute path of a job.</p> <code>_write_sub_files_slurm_docker</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]: Writes submission files for Slurm Docker jobs.</p> <code>_get_Sub</code> <p>str, submission_type: str, sub_filename: str, abs_path_job: str, context: str) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: Returns the appropriate submission object based on the submission type.</p> <code>_write_sub_file</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes a submission file for the given jobs.</p> <code>_write_sub_files</code> <p>str, running_jobs: list[str], queuing_jobs: list[str], list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]: Writes submission files for the given jobs based on the submission type.</p> <code>write_sub_files</code> <p>Writes submission files for all jobs to be submitted and returns a dictionary of submission files.</p> <code>_update_job_status_from_hpc_output</code> <p>str, submission_type: str, dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)     -&gt; tuple[dict, int]: Updates the job status from the HPC output.</p> <code>submit</code> <p>list[str], l_submission_filenames: list[str], submission_type: str) -&gt; None: Submits the jobs to the appropriate cluster system.</p> <code>_get_local_jobs</code> <p>Gets the list of local jobs.</p> <code>_get_condor_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Condor jobs based on the status.</p> <code>_get_slurm_jobs</code> <p>str, force_query_individually: bool = False) -&gt; list[str]: Gets the list of Slurm jobs based on the status.</p> <code>querying_jobs</code> <p>bool, check_htc: bool, check_slurm: bool, status: str = \"running\") -&gt; list[str]: Queries the jobs based on the submission type and status.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>class ClusterSubmission:\n    \"\"\"\n    A class to handle the submission of jobs to various cluster systems such as local PC, HTC, and\n    Slurm.\n\n    Attributes:\n        study_name (str): The name of the study.\n        l_jobs_to_submit (list[str]): A list of jobs to submit.\n        dic_all_jobs (dict): A dictionary containing all jobs.\n        dic_tree (dict): A dictionary representing the job tree.\n        path_submission_file (str): The path to the submission file.\n        abs_path_study (str): The absolute path to the study.\n        dic_submission (dict): A dictionary mapping submission types to their corresponding classes.\n\n    Methods:\n        dic_id_to_path_job() -&gt; dict | None:\n            Getter for the dictionary mapping job IDs to their paths.\n        dic_id_to_path_job(dic_id_to_path_job: dict[int, str]):\n            Setter for the dictionary mapping job IDs to their paths.\n        _update_dic_id_to_path_job(running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n            Updates the dictionary mapping job IDs to their paths based on the current running and\n            queuing jobs.\n        _check_submission_type() -&gt; tuple[bool, bool, bool]:\n            Checks the submission type for the jobs and ensures that HTC and Slurm submissions are\n            not mixed.\n        _get_state_jobs(verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n            Gets the current state of the jobs (running and queuing).\n        _test_job(job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]) -&gt; bool:\n            Tests if a job is completed, running, or queuing.\n        _return_htc_flavour(job: str) -&gt; str:\n            Returns the HTC flavor for a given job.\n        _return_abs_path_job(job: str) -&gt; tuple[str, str]:\n            Returns the absolute path of a job.\n        _write_sub_files_slurm_docker(sub_filename: str, running_jobs: list[str],\n            queuing_jobs: list[str], list_of_jobs: list[str]) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for Slurm Docker jobs.\n        _get_Sub(job: str, submission_type: str, sub_filename: str, abs_path_job: str,\n            context: str) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n            Returns the appropriate submission object based on the submission type.\n        _write_sub_file(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes a submission file for the given jobs.\n        _write_sub_files(sub_filename: str, running_jobs: list[str], queuing_jobs: list[str],\n            list_of_jobs: list[str], submission_type: str) -&gt; tuple[list[str], list[str]]:\n            Writes submission files for the given jobs based on the submission type.\n        write_sub_files() -&gt; dict:\n            Writes submission files for all jobs to be submitted and returns a dictionary of\n            submission files.\n        _update_job_status_from_hpc_output(submit_command: str, submission_type: str,\n            dic_id_to_path_job_temp: dict, list_of_jobs: list[str], idx_submission: int = 0)\n                -&gt; tuple[dict, int]:\n            Updates the job status from the HPC output.\n        submit(list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str)\n            -&gt; None:\n            Submits the jobs to the appropriate cluster system.\n        _get_local_jobs() -&gt; list[str]:\n            Gets the list of local jobs.\n        _get_condor_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Condor jobs based on the status.\n        _get_slurm_jobs(status: str, force_query_individually: bool = False) -&gt; list[str]:\n            Gets the list of Slurm jobs based on the status.\n        querying_jobs(check_local: bool, check_htc: bool, check_slurm: bool,\n            status: str = \"running\") -&gt; list[str]:\n            Queries the jobs based on the submission type and status.\n    \"\"\"\n\n    def __init__(\n        self,\n        study_name: str,\n        l_jobs_to_submit: list[str],\n        dic_all_jobs: dict,\n        dic_tree: dict,\n        path_submission_file: str,\n        abs_path_study: str,\n    ):\n        self.study_name: str = study_name\n        self.l_jobs_to_submit: list[str] = l_jobs_to_submit\n        self.dic_all_jobs: dict = dic_all_jobs\n        self.dic_tree: dict = dic_tree\n        self.path_submission_file: str = path_submission_file\n        self.abs_path_study: str = abs_path_study\n        self.dic_submission: dict = {\n            \"local\": LocalPC,\n            \"htc\": HTC,\n            \"htc_docker\": HTCDocker,\n            \"slurm\": Slurm,\n            \"slurm_docker\": SlurmDocker,\n        }\n        \"\"\"\n        Initialize the ClusterSubmission class.\n\n        Args:\n            study_name (str): The name of the study.\n            l_jobs_to_submit (list[str]): A list of job names to submit.\n            dic_all_jobs (dict): A dictionary containing all jobs.\n            dic_tree (dict): A dictionary representing the job tree structure.\n            path_submission_file (str): The path to the submission file.\n            abs_path_study (str): The absolute path to the study.\n        \"\"\"\n\n    # Getter for dic_id_to_path_job\n    @property\n    def dic_id_to_path_job(self) -&gt; dict | None:\n        \"\"\"\n        Generates a dictionary mapping job IDs to their respective job paths.\n\n        This method iterates over the list of jobs to submit and constructs a dictionary\n        where the keys are job IDs and the values are the absolute paths to the jobs.\n        If no job IDs are found, the method returns None.\n\n        Returns:\n            dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.\n        \"\"\"\n        dic_id_to_path_job = {}\n        found_at_least_one = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job:\n                dic_id_to_path_job[subdic_job[\"id_sub\"]] = self._return_abs_path_job(job)[0]\n                found_at_least_one = True\n\n        return dic_id_to_path_job if found_at_least_one else None\n\n    # Setter for dic_id_to_path_job\n    @dic_id_to_path_job.setter\n    def dic_id_to_path_job(self, dic_id_to_path_job: dict[int, str]):\n        \"\"\"\n        Updates the internal job submission tree with job IDs and their corresponding paths.\n\n        Args:\n            dic_id_to_path_job (dict[int, str]): A dictionary mapping job IDs (integers) to their\n            respective paths (strings).\n\n        Raises:\n            AssertionError: If dic_id_to_path_job is not a dictionary.\n\n        Notes:\n            - Ensures all job IDs are integers.\n            - Updates the internal job submission tree by adding or removing job IDs based on the\n                provided dictionary.\n            - If a job's path is found in the dictionary, its ID is updated in the tree.\n            - If a job's ID is not found in the dictionary, it is removed from the tree.\n        \"\"\"\n        assert isinstance(dic_id_to_path_job, dict)\n        # Ensure all ids are integers\n        dic_id_to_path_job = {\n            int(id_job): path_job for id_job, path_job in dic_id_to_path_job.items()\n        }\n        dic_job_to_id = {path_job: int(id_job) for id_job, path_job in dic_id_to_path_job.items()}\n\n        # Update the tree\n        for job in self.l_jobs_to_submit:\n            path_job = self._return_abs_path_job(job)[0]\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            subdic_job = nested_get(self.dic_tree, l_keys)\n            if \"id_sub\" in subdic_job and int(subdic_job[\"id_sub\"]) not in dic_id_to_path_job:\n                del subdic_job[\"id_sub\"]\n            elif \"id_sub\" not in subdic_job and path_job in dic_job_to_id:\n                subdic_job[\"id_sub\"] = dic_job_to_id[path_job]\n            # Else all is consistent\n\n    def _update_dic_id_to_path_job(self, running_jobs: list[str], queuing_jobs: list[str]) -&gt; None:\n        \"\"\"\n        Updates the dictionary `dic_id_to_path_job` by removing jobs that are no longer running or\n        queuing.\n\n        Args:\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n\n        Returns:\n            None\n        \"\"\"\n        # Look for jobs in the dictionnary that are not running or queuing anymore\n        set_current_jobs = set(running_jobs + queuing_jobs)\n        if self.dic_id_to_path_job is not None:\n            dic_id_to_path_job = self.dic_id_to_path_job\n            for id_job, job in self.dic_id_to_path_job.items():\n                if job not in set_current_jobs:\n                    del dic_id_to_path_job[id_job]\n\n            # Update dic_id_to_path_job\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n    def _check_submission_type(self) -&gt; tuple[bool, bool, bool]:\n        \"\"\"\n        Checks the types of job submissions in the current batch and ensures that\n        there is no mixing of HTC and Slurm submission types.\n\n        Args:\n            None\n\n        Returns:\n            tuple[bool, bool, bool]: A tuple containing three boolean values indicating\n            whether there are local, HTC, and Slurm submissions respectively.\n\n        Raises:\n            ValueError: If both HTC and Slurm submission types are found in the jobs to submit.\n        \"\"\"\n        check_local = False\n        check_htc = False\n        check_slurm = False\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            if submission_type == \"local\":\n                check_local = True\n            elif submission_type in [\"htc\", \"htc_docker\"]:\n                check_htc = True\n            elif submission_type in [\"slurm\", \"slurm_docker\"]:\n                check_slurm = True\n\n        if check_htc and check_slurm:\n            raise ValueError(\"Error: Mixing htc and slurm submission is not allowed\")\n\n        return check_local, check_htc, check_slurm\n\n    def _get_state_jobs(self, verbose: bool = True) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Retrieves the state of jobs (running and queuing) based on the submission type.\n\n        This method first determines the submission type (local, HTC, or Slurm) and then queries\n        the jobs accordingly. It updates the internal dictionary mapping job IDs to their paths\n        and optionally prints the running and queuing jobs.\n\n        Args:\n            verbose (bool): If True, prints the running and queuing jobs. Default is True.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - The first list contains the IDs of running jobs.\n                - The second list contains the IDs of queuing jobs.\n        \"\"\"\n        # First check whether the jobs are submitted on local, htc or slurm\n        check_local, check_htc, check_slurm = self._check_submission_type()\n\n        # Then query accordingly\n        running_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"running\")\n        queuing_jobs = self.querying_jobs(check_local, check_htc, check_slurm, status=\"queuing\")\n        self._update_dic_id_to_path_job(running_jobs, queuing_jobs)\n        if verbose:\n            logging.info(\"Running: \\n\" + \"\\n\".join(running_jobs))\n            logging.info(\"queuing: \\n\" + \"\\n\".join(queuing_jobs))\n        return running_jobs, queuing_jobs\n\n    def _test_job(\n        self, job: str, path_job: str, running_jobs: list[str], queuing_jobs: list[str]\n    ) -&gt; bool:\n        \"\"\"\n        Tests the status of a job and determines if it needs to be (re)submitted.\n\n        Args:\n            job (str): The job identifier.\n            path_job (str): The path to the job.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of currently queuing jobs.\n\n        Returns:\n            bool: True if the job must be (re)submitted, False otherwise.\n        \"\"\"\n        # Test if job is completed\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        completed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"finished\"\n        failed = nested_get(self.dic_tree, l_keys + [\"status\"]) == \"failed\"\n        if completed:\n            logging.info(f\"{path_job} is already completed.\")\n\n        # Test if job has failed\n        if failed:\n            logging.info(f\"{path_job} has failed.\")\n\n        # Test if job is running\n        elif path_job in running_jobs:\n            logging.info(f\"{path_job} is already running.\")\n\n        # Test if job is queuing\n        elif path_job in queuing_jobs:\n            logging.info(f\"{path_job} is already queuing.\")\n\n        # True if job must be (re)submitted\n        else:\n            return True\n        return False\n\n    def _return_htc_flavour(self, job: str) -&gt; str:\n        \"\"\"\n        Retrieve the HTC flavor for a given job.\n\n        Args:\n            job (str): The job identifier.\n\n        Returns:\n            str: The HTC flavor associated with the job.\n        \"\"\"\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        return nested_get(self.dic_tree, l_keys + [\"htc_flavor\"])\n\n    def _return_abs_path_job(self, job: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Generate the relative and absolute paths for a given job.\n\n        Args:\n            job (str): The job string containing the path to the job file.\n\n        Returns:\n            tuple[str, str]: A tuple containing:\n            - path_job (str): The relative path to the job directory.\n            - abs_path_job (str): The absolute path to the job directory.\n        \"\"\"\n        # Get corresponding path job (remove the python file name)\n        path_job = \"/\".join(job.split(\"/\")[:-1]) + \"/\"\n        abs_path_job = f\"{self.abs_path_study}/{path_job}\"\n        return path_job, abs_path_job\n\n    def _write_sub_files_slurm_docker(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Generates SLURM submission files for Docker jobs and writes them to disk.\n\n        This method iterates over a list of jobs, checks their status (running, queuing, or\n        completed), and writes the corresponding SLURM submission files for Docker jobs. The\n        submission files are written to disk with a specific naming convention.\n\n        Args:\n            sub_filename (str): The base name for the submission files.\n            running_jobs (list[str]): A list of job identifiers that are currently running.\n            queuing_jobs (list[str]): A list of job identifiers that are currently queuing.\n            list_of_jobs (list[str]): A list of job identifiers to process.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n            - A list of filenames for the generated submission files.\n            - A list of job identifiers that were updated.\n        \"\"\"\n        l_filenames = []\n        list_of_jobs_updated = []\n        for idx_job, job in enumerate(list_of_jobs):\n            path_job, abs_path_job = self._return_abs_path_job(job)\n\n            # Test if job is running, queuing or completed\n            if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                filename_sub = f\"{sub_filename.split('.sub')[0]}_{idx_job}.sub\"\n\n                # Get job context\n                l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                context = nested_get(self.dic_tree, l_keys + [\"context\"])\n\n                # Write the submission files\n                # ! Careful, I implemented a fix for path due to the temporary home recovery folder\n                logging.info(f'Writing submission file for node \"{abs_path_job}\"')\n                fix = True\n                Sub = self.dic_submission[\"slurm_docker\"](\n                    filename_sub, abs_path_job, context, self.dic_tree[\"container_image\"], fix=fix\n                )\n                with open(filename_sub, \"w\") as fid:\n                    fid.write(Sub.head + \"\\n\")\n                    if fix:\n                        fid.write(Sub.str_fixed_run + \"\\n\")\n                    fid.write(Sub.body + \"\\n\")\n                    fid.write(Sub.tail + \"\\n\")\n\n                l_filenames.append(filename_sub)\n                list_of_jobs_updated.append(job)\n        return l_filenames, list_of_jobs_updated\n\n    def _get_Sub(\n        self, job: str, submission_type: str, sub_filename: str, abs_path_job: str, context: str\n    ) -&gt; LocalPC | HTC | HTCDocker | Slurm | SlurmDocker:\n        \"\"\"\n        Generate a submission object based on the specified submission type.\n\n        Args:\n            job (str): The job identifier.\n            submission_type (str): The type of submission (e.g., \"slurm\", \"htc\", \"htc_docker\",\n                \"slurm_docker\", \"local\").\n            sub_filename (str): The submission filename.\n            abs_path_job (str): The absolute path to the job.\n            context (str): The context for the submission.\n\n        Returns:\n            LocalPC | HTC | HTCDocker | Slurm | SlurmDocker: An instance of the appropriate\n                submission class.\n\n        Raises:\n            ValueError: If the submission type is not valid or if the container_image is not defined\n                in the tree for docker submissions.\n        \"\"\"\n        match submission_type:\n            case \"slurm\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job, context)\n            case \"htc\":\n                return self.dic_submission[submission_type](\n                    sub_filename, abs_path_job, context, self._return_htc_flavour(job)\n                )\n            case w if w in [\"htc_docker\", \"slurm_docker\"]:\n                # Path to singularity image\n                if (\n                    \"container_image\" in self.dic_tree\n                    and self.dic_tree[\"container_image\"] is not None\n                ):\n                    self.path_image = self.dic_tree[\"container_image\"]\n                else:\n                    raise ValueError(\n                        \"Error: container_image is not defined in the tree. Please define it in the\"\n                        \" config.yaml file.\"\n                    )\n\n                if submission_type == \"htc_docker\":\n                    return self.dic_submission[submission_type](\n                        sub_filename,\n                        abs_path_job,\n                        context,\n                        self.path_image,\n                        self._return_htc_flavour(job),\n                    )\n                else:\n                    return self.dic_submission[submission_type](\n                        sub_filename, abs_path_job, context, self.path_image\n                    )\n            case \"local\":\n                return self.dic_submission[submission_type](sub_filename, abs_path_job)\n            case _:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    def _write_sub_file(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes a submission file for a list of jobs and returns the updated list of jobs.\n\n        Args:\n            sub_filename (str): The filename for the submission file.\n            running_jobs (list[str]): List of currently running jobs.\n            queuing_jobs (list[str]): List of currently queuing jobs.\n            list_of_jobs (list[str]): List of jobs to be submitted.\n            submission_type (str): The type of submission.\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing:\n            - A list with the submission filename if the file was created, otherwise an empty list.\n            - An updated list of jobs that were included in the submission file.\n        \"\"\"\n        # Flag to know if the file can be submitted (at least one job in it)\n        ok_to_submit = False\n\n        # Flat to know if the header has been written\n        header_written = False\n\n        # Create folder to the submission file if it does not exist\n        os.makedirs(\"/\".join(sub_filename.split(\"/\")[:-1]), exist_ok=True)\n\n        # Updated list of jobs (without unsubmitted jobs)\n        list_of_jobs_updated = []\n\n        # Write the submission file\n        Sub = None\n        with open(sub_filename, \"w\") as fid:\n            for job in list_of_jobs:\n                # Get corresponding path job (remove the python file name)\n                path_job, abs_path_job = self._return_abs_path_job(job)\n\n                # Test if job is running, queuing or completed\n                if self._test_job(job, path_job, running_jobs, queuing_jobs):\n                    logging.info(f'Writing submission command for node \"{abs_path_job}\"')\n\n                    # Get context\n                    l_keys = self.dic_all_jobs[job][\"l_keys\"]\n                    context = nested_get(self.dic_tree, l_keys + [\"context\"])\n\n                    # Get Submission object\n                    Sub = self._get_Sub(job, submission_type, sub_filename, abs_path_job, context)\n\n                    # Take the first job as reference for head\n                    if not header_written:\n                        fid.write(Sub.head + \"\\n\")\n                        header_written = True\n\n                    # Write instruction for submission\n                    fid.write(Sub.body + \"\\n\")\n\n                    # Append job to list_of_jobs_updated\n                    list_of_jobs_updated.append(job)\n\n            # Tail instruction\n            if Sub is not None:\n                fid.write(Sub.tail + \"\\n\")\n                ok_to_submit = True\n\n        if not ok_to_submit:\n            os.remove(sub_filename)\n\n        return ([sub_filename], list_of_jobs_updated) if ok_to_submit else ([], [])\n\n    def _write_sub_files(\n        self,\n        sub_filename: str,\n        running_jobs: list[str],\n        queuing_jobs: list[str],\n        list_of_jobs: list[str],\n        submission_type: str,\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"\n        Writes submission files based on the specified submission type.\n\n        Args:\n            sub_filename (str): The name of the submission file to be created.\n            running_jobs (list[str]): A list of currently running jobs.\n            queuing_jobs (list[str]): A list of jobs that are queued.\n            list_of_jobs (list[str]): A list of all jobs to be submitted.\n            submission_type (str): The type of submission system being used (e.g., \"slurm_docker\").\n\n        Returns:\n            tuple[list[str], list[str]]: A tuple containing two lists:\n                - Updated list of running jobs.\n                - Updated list of queuing jobs.\n        \"\"\"\n        # Slurm docker is a peculiar case as one submission file must be created per job\n        if submission_type == \"slurm_docker\":\n            return self._write_sub_files_slurm_docker(\n                sub_filename, running_jobs, queuing_jobs, list_of_jobs\n            )\n\n        else:\n            return self._write_sub_file(\n                sub_filename,\n                running_jobs,\n                queuing_jobs,\n                list_of_jobs,\n                submission_type,\n            )\n\n    def write_sub_files(\n        self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n    ) -&gt; dict:\n        \"\"\"\n        Generates and writes submission files for jobs based on their submission type.\n\n        This method categorizes jobs to be submitted by their submission type, writes the\n        corresponding submission files, and returns a dictionary containing the submission\n        files and their associated job contexts.\n\n        Returns:\n            dict: A dictionary where keys are submission types and values are tuples containing\n                a list of updated jobs and a list of submission filenames.\n        \"\"\"\n        running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n        # Make a dict of all jobs to submit depending on the submission type\n        dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n        for job in self.l_jobs_to_submit:\n            l_keys = self.dic_all_jobs[job][\"l_keys\"]\n            submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n            dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n        # Write submission files for each submission type\n        dic_submission_files = {}\n        for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n            if len(list_of_jobs) &gt; 0:\n                # Write submission files\n                l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                    self.path_submission_file,\n                    running_jobs,\n                    queuing_jobs,\n                    copy.copy(list_of_jobs),\n                    submission_type,\n                )\n\n                # Record submission files and context\n                dic_submission_files[submission_type] = (\n                    list_of_jobs_updated,\n                    l_submission_filenames,\n                )\n\n                # Update dic_summary_by_gen inplace\n                if dic_summary_by_gen is not None:\n                    for job in list_of_jobs:\n                        gen = self.dic_all_jobs[job][\"gen\"]\n                        if job in list_of_jobs_updated:\n                            dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                        else:\n                            dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n        return dic_submission_files\n\n    def _update_job_status_from_hpc_output(\n        self,\n        submit_command: str,\n        submission_type: str,\n        dic_id_to_path_job_temp: dict,\n        list_of_jobs: list[str],\n        idx_submission: int = 0,\n    ) -&gt; tuple[dict, int]:\n        \"\"\"\n        Updates the job status from the HPC output.\n\n        This method parses the output of a job submission command to update the job status\n        in a dictionary mapping job IDs to their respective paths. It supports both HTC and\n        SLURM submission types.\n\n        Args:\n            submit_command (str): The command used to submit the job.\n            submission_type (str): The type of submission system ('htc' or 'slurm').\n            dic_id_to_path_job_temp (dict): A dictionary mapping job IDs to their paths.\n            list_of_jobs (list[str]): A list of job paths.\n            idx_submission (int, optional): The index of the current submission. Defaults to 0.\n\n        Returns:\n            tuple[dict, int]: A tuple containing the updated dictionary and the updated index of\n                submission.\n\n        Raises:\n            RuntimeError: If there is an error in the submission process.\n        \"\"\"\n        process = subprocess.run(\n            submit_command.split(\" \"),\n            capture_output=True,\n        )\n\n        output = process.stdout.decode(\"utf-8\")\n        output_error = process.stderr.decode(\"utf-8\")\n        if \"ERROR\" in output_error:\n            raise RuntimeError(f\"Error in submission: {output_error}\")\n        for line in output.split(\"\\n\"):\n            if \"htc\" in submission_type:\n                if \"cluster\" in line:\n                    cluster_id = int(line.split(\"cluster \")[1][:-1])\n                    dic_id_to_path_job_temp[cluster_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n            elif \"slurm\" in submission_type:\n                if \"Submitted\" in line:\n                    job_id = int(line.split(\" \")[3])\n                    dic_id_to_path_job_temp[job_id] = self._return_abs_path_job(\n                        list_of_jobs[idx_submission]\n                    )[0]\n                    idx_submission += 1\n\n        return dic_id_to_path_job_temp, idx_submission\n\n    def submit(\n        self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n    ) -&gt; None:\n        \"\"\"\n        Submits a list of jobs to the specified submission system.\n\n        Args:\n            list_of_jobs (list[str]): List of job identifiers to be submitted.\n            l_submission_filenames (list[str]): List of filenames containing submission scripts.\n            submission_type (str): Type of submission system to use. Valid options are \"local\",\n                \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n        Raises:\n            ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n                submission type.\n            ValueError: If the submission type is not valid.\n\n        Returns:\n            None\n        \"\"\"\n        # Check that the submission file(s) is/are appropriate for the submission mode\n        if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n            raise ValueError(\n                \"Error: Multiple submission files should not be implemented for this submission\"\n                \" mode\"\n            )\n\n        # Check that at least one job is being submitted\n        if not l_submission_filenames:\n            logging.info(\"No job being submitted.\")\n\n        # Submit\n        dic_id_to_path_job_temp = {}\n        idx_submission = 0\n        for sub_filename in l_submission_filenames:\n            if submission_type == \"local\":\n                os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n            elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n                submit_command = self.dic_submission[submission_type].get_submit_command(\n                    sub_filename\n                )\n                dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                    submit_command,\n                    submission_type,\n                    dic_id_to_path_job_temp,\n                    list_of_jobs,\n                    idx_submission,\n                )\n            else:\n                raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n        # Update and write the id-job file\n        if dic_id_to_path_job_temp:\n            assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n        # Merge with the previous id-job file\n        dic_id_to_path_job = self.dic_id_to_path_job\n\n        # Update and write on disk\n        if dic_id_to_path_job is not None:\n            dic_id_to_path_job.update(dic_id_to_path_job_temp)\n            self.dic_id_to_path_job = dic_id_to_path_job\n        elif dic_id_to_path_job_temp:\n            dic_id_to_path_job = dic_id_to_path_job_temp\n            self.dic_id_to_path_job = dic_id_to_path_job\n\n        logging.info(\"Jobs status after submission:\")\n        _, _ = self._get_state_jobs(verbose=True)\n\n    def _get_local_jobs(self) -&gt; list[str]:\n        \"\"\"\n        Retrieves a list of local job paths.\n        This method scans the current processes to identify jobs that are running\n        a script named 'run.sh'. It extracts the job paths and filters them to\n        include only the paths that are relevant to the current study.\n\n        Args:\n            None\n\n        Returns:\n            list[str]: A list of job paths that are currently running and relevant\n            to the study.\n        \"\"\"\n\n        l_path_jobs = []\n        # Warning, does not work at the moment in lxplus...\n        for ps in psutil.pids():\n            try:\n                aux = psutil.Process(ps).cmdline()\n            except Exception:\n                aux = []\n            if len(aux) &gt; 1 and \"run.sh\" in aux[-1]:\n                job = str(Path(aux[-1]).parent)\n\n                # Only get path after name of the study\n                try:\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}/\")\n                except IndexError:\n                    logging.warning(\n                        \"Some jobs from another study are running. Acquiring the full path as the \"\n                        \"study name is unknown.\"\n                    )\n                    l_path_jobs.append(job)\n        return l_path_jobs\n\n    def _get_condor_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve the paths of Condor jobs based on their status.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Can be \"running\" or \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually if the\n                id-job file is missing. Defaults to False.\n\n        Returns:\n            list[str]: A list of paths to the jobs that match the specified status.\n\n        Raises:\n            ValueError: If the status provided is not \"running\" or \"queuing\".\n\n        Notes:\n            - The method relies on the `condor_q` command to retrieve job information.\n            - If the id-job file is missing and `force_query_individually` is False, jobs not in\n                `dic_id_to_path_job` will be ignored.\n            - If `force_query_individually` is True, the method will query each job individually to\n                retrieve its path.\n            - Warnings are printed if jobs are found that are not in the id-job file or if the\n                id-job file is missing.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": 1, \"queuing\": 2}\n        condor_output = subprocess.run([\"condor_q\"], capture_output=True).stdout.decode(\"utf-8\")\n\n        # Check which jobs are running\n        first_line = True\n        first_missing_job = True\n        for line in condor_output.split(\"\\n\")[4:]:\n            if line == \"\":\n                break\n            jobid = int(line.split(\"ID:\")[1][1:8])\n            condor_status = line.split(\"      \")[1:5]  # Done, Run, Idle, and potentially Hold\n\n            # If job is running/queuing, get the path to the job\n            if condor_status[dic_status[status]] == \"1\":\n                # Get path from dic_id_to_path_job if available\n                if self.dic_id_to_path_job is not None:\n                    if jobid in self.dic_id_to_path_job:\n                        l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                    elif first_missing_job:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and are not in the id-job\"\n                            \" file. They may come from another study. Ignoring them.\"\n                        )\n                        first_missing_job = False\n\n                elif force_query_individually:\n                    if first_line:\n                        logging.warning(\n                            \"Warning, some jobs are queuing/running and the id-job file is\"\n                            \" missing... Querying them individually.\"\n                        )\n                        first_line = False\n                    job_details = subprocess.run(\n                        [\"condor_q\", \"-l\", f\"{jobid}\"], capture_output=True\n                    ).stdout.decode(\"utf-8\")\n                    job = job_details.split('Cmd = \"')[1].split(\"run.sh\")[0]\n\n                    # Only get path after master_study\n                    job = job.split(self.study_name)[1]\n                    l_path_jobs.append(f\"{self.study_name}{job}\")\n\n                elif first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Ignoring them.\"\n                    )\n                    first_line = False\n\n        return l_path_jobs\n\n    def _get_slurm_jobs(self, status: str, force_query_individually: bool = False) -&gt; list[str]:\n        \"\"\"\n        Retrieve a list of SLURM job paths based on their status.\n\n        This method queries SLURM to get the job IDs and their statuses for the current user.\n        It then attempts to map these job IDs to their corresponding paths using an internal\n        dictionary. If the dictionary is not available or does not contain the job ID, it can\n        optionally query each job individually for its details.\n\n        Args:\n            status (str): The status of the jobs to retrieve. Expected values are \"running\" or\n                \"queuing\".\n            force_query_individually (bool, optional): If True, query each job individually for its\n                details when the job ID is not found in the internal dictionary. Defaults to False.\n\n        Returns:\n            list[str]: A list of job paths corresponding to the specified status.\n        \"\"\"\n        l_path_jobs = []\n        dic_status = {\"running\": \"RUNNING\", \"queuing\": \"PENDING\"}\n        username = (\n            subprocess.run([\"id\", \"-u\", \"-n\"], capture_output=True).stdout.decode(\"utf-8\").strip()\n        )\n        slurm_output = subprocess.run(\n            [\"squeue\", \"-u\", f\"{username}\", \"-t\", dic_status[status]], capture_output=True\n        ).stdout.decode(\"utf-8\")\n\n        # Get job id and details\n        first_line = True\n        first_missing_job = True\n        for line in slurm_output.split(\"\\n\")[1:]:\n            l_split = line.split()\n            if len(l_split) == 0:\n                break\n            jobid = int(l_split[0])\n            slurm_status = l_split[4]  # R or PD  # noqa: F841\n\n            # Get path from dic_id_to_path_job if available\n            if self.dic_id_to_path_job is not None:\n                if jobid in self.dic_id_to_path_job:\n                    l_path_jobs.append(self.dic_id_to_path_job[jobid])\n                elif first_missing_job:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and are not in the id-job\"\n                        \" file. They may come from another study. Ignoring them.\"\n                    )\n                    first_missing_job = False\n\n            elif force_query_individually:\n                if first_line:\n                    logging.warning(\n                        \"Warning, some jobs are queuing/running and the id-job file is\"\n                        \" missing... Querying them individually.\"\n                    )\n                    first_line = False\n                job_details = subprocess.run(\n                    [\"scontrol\", \"show\", \"jobid\", \"-dd\", f\"{jobid}\"], capture_output=True\n                ).stdout.decode(\"utf-8\")\n                job = (\n                    job_details.split(\"Command=\")[1].split(\"run.sh\")[0]\n                    if \"run.sh\" in job_details\n                    else job_details.split(\"StdOut=\")[1].split(\"output.txt\")[0]\n                )\n                # Only get path after study_name\n                job = job.split(self.study_name)[1]\n                l_path_jobs.append(f\"{self.study_name}{job}\")\n\n            elif first_line:\n                logging.warning(\n                    \"Warning, some jobs are queuing/running and the id-job file is\"\n                    \" missing... Ignoring them.\"\n                )\n                first_line = False\n\n        return l_path_jobs\n\n    def querying_jobs(\n        self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n    ) -&gt; list[str]:\n        \"\"\"\n        Queries jobs from different job management systems based on the provided flags and status.\n\n        Args:\n            check_local (bool): If True, check for local jobs.\n            check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n            check_slurm (bool): If True, check for SLURM jobs.\n            status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n        Returns:\n            list[str]: A list of job paths that match the query criteria.\n        \"\"\"\n        # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n        l_path_jobs = []\n        if check_local:\n            if status == \"running\":\n                l_path_jobs.extend(self._get_local_jobs())\n            else:\n                # Always empty return as there is no queuing in local pc\n                pass\n\n        if check_htc:\n            l_path_jobs.extend(self._get_condor_jobs(status))\n\n        if check_slurm:\n            l_path_jobs.extend(self._get_slurm_jobs(status))\n\n        return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.dic_id_to_path_job","title":"<code>dic_id_to_path_job: dict | None</code>  <code>property</code> <code>writable</code>","text":"<p>Generates a dictionary mapping job IDs to their respective job paths.</p> <p>This method iterates over the list of jobs to submit and constructs a dictionary where the keys are job IDs and the values are the absolute paths to the jobs. If no job IDs are found, the method returns None.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>dict | None: A dictionary mapping job IDs to job paths, or None if no job IDs are found.</p>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.dic_submission","title":"<code>dic_submission: dict = {'local': LocalPC, 'htc': HTC, 'htc_docker': HTCDocker, 'slurm': Slurm, 'slurm_docker': SlurmDocker}</code>  <code>instance-attribute</code>","text":"<p>Initialize the ClusterSubmission class.</p> <p>Parameters:</p> Name Type Description Default <code>study_name</code> <code>str</code> <p>The name of the study.</p> required <code>l_jobs_to_submit</code> <code>list[str]</code> <p>A list of job names to submit.</p> required <code>dic_all_jobs</code> <code>dict</code> <p>A dictionary containing all jobs.</p> required <code>dic_tree</code> <code>dict</code> <p>A dictionary representing the job tree structure.</p> required <code>path_submission_file</code> <code>str</code> <p>The path to the submission file.</p> required <code>abs_path_study</code> <code>str</code> <p>The absolute path to the study.</p> required"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.querying_jobs","title":"<code>querying_jobs(check_local, check_htc, check_slurm, status='running')</code>","text":"<p>Queries jobs from different job management systems based on the provided flags and status.</p> <p>Parameters:</p> Name Type Description Default <code>check_local</code> <code>bool</code> <p>If True, check for local jobs.</p> required <code>check_htc</code> <code>bool</code> <p>If True, check for HTC (High Throughput Computing) jobs.</p> required <code>check_slurm</code> <code>bool</code> <p>If True, check for SLURM jobs.</p> required <code>status</code> <code>str</code> <p>The status of the jobs to query. Defaults to \"running\".</p> <code>'running'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of job paths that match the query criteria.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def querying_jobs(\n    self, check_local: bool, check_htc: bool, check_slurm: bool, status: str = \"running\"\n) -&gt; list[str]:\n    \"\"\"\n    Queries jobs from different job management systems based on the provided flags and status.\n\n    Args:\n        check_local (bool): If True, check for local jobs.\n        check_htc (bool): If True, check for HTC (High Throughput Computing) jobs.\n        check_slurm (bool): If True, check for SLURM jobs.\n        status (str, optional): The status of the jobs to query. Defaults to \"running\".\n\n    Returns:\n        list[str]: A list of job paths that match the query criteria.\n    \"\"\"\n    # sourcery skip: remove-redundant-if, remove-redundant-pass, swap-nested-ifs\n    l_path_jobs = []\n    if check_local:\n        if status == \"running\":\n            l_path_jobs.extend(self._get_local_jobs())\n        else:\n            # Always empty return as there is no queuing in local pc\n            pass\n\n    if check_htc:\n        l_path_jobs.extend(self._get_condor_jobs(status))\n\n    if check_slurm:\n        l_path_jobs.extend(self._get_slurm_jobs(status))\n\n    return l_path_jobs\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.submit","title":"<code>submit(list_of_jobs, l_submission_filenames, submission_type)</code>","text":"<p>Submits a list of jobs to the specified submission system.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_jobs</code> <code>list[str]</code> <p>List of job identifiers to be submitted.</p> required <code>l_submission_filenames</code> <code>list[str]</code> <p>List of filenames containing submission scripts.</p> required <code>submission_type</code> <code>str</code> <p>Type of submission system to use. Valid options are \"local\", \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiple submission files are provided for a non-\"slurm_docker\" submission type.</p> <code>ValueError</code> <p>If the submission type is not valid.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def submit(\n    self, list_of_jobs: list[str], l_submission_filenames: list[str], submission_type: str\n) -&gt; None:\n    \"\"\"\n    Submits a list of jobs to the specified submission system.\n\n    Args:\n        list_of_jobs (list[str]): List of job identifiers to be submitted.\n        l_submission_filenames (list[str]): List of filenames containing submission scripts.\n        submission_type (str): Type of submission system to use. Valid options are \"local\",\n            \"htc\", \"slurm\", \"htc_docker\", and \"slurm_docker\".\n\n    Raises:\n        ValueError: If multiple submission files are provided for a non-\"slurm_docker\"\n            submission type.\n        ValueError: If the submission type is not valid.\n\n    Returns:\n        None\n    \"\"\"\n    # Check that the submission file(s) is/are appropriate for the submission mode\n    if len(l_submission_filenames) &gt; 1 and submission_type != \"slurm_docker\":\n        raise ValueError(\n            \"Error: Multiple submission files should not be implemented for this submission\"\n            \" mode\"\n        )\n\n    # Check that at least one job is being submitted\n    if not l_submission_filenames:\n        logging.info(\"No job being submitted.\")\n\n    # Submit\n    dic_id_to_path_job_temp = {}\n    idx_submission = 0\n    for sub_filename in l_submission_filenames:\n        if submission_type == \"local\":\n            os.system(self.dic_submission[submission_type].get_submit_command(sub_filename))\n        elif submission_type in {\"htc\", \"slurm\", \"htc_docker\", \"slurm_docker\"}:\n            submit_command = self.dic_submission[submission_type].get_submit_command(\n                sub_filename\n            )\n            dic_id_to_path_job_temp, idx_submission = self._update_job_status_from_hpc_output(\n                submit_command,\n                submission_type,\n                dic_id_to_path_job_temp,\n                list_of_jobs,\n                idx_submission,\n            )\n        else:\n            raise ValueError(f\"Error: {submission_type} is not a valid submission mode\")\n\n    # Update and write the id-job file\n    if dic_id_to_path_job_temp:\n        assert len(dic_id_to_path_job_temp) == len(list_of_jobs)\n\n    # Merge with the previous id-job file\n    dic_id_to_path_job = self.dic_id_to_path_job\n\n    # Update and write on disk\n    if dic_id_to_path_job is not None:\n        dic_id_to_path_job.update(dic_id_to_path_job_temp)\n        self.dic_id_to_path_job = dic_id_to_path_job\n    elif dic_id_to_path_job_temp:\n        dic_id_to_path_job = dic_id_to_path_job_temp\n        self.dic_id_to_path_job = dic_id_to_path_job\n\n    logging.info(\"Jobs status after submission:\")\n    _, _ = self._get_state_jobs(verbose=True)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/cluster_submission.html#study_da.submit.cluster_submission.cluster_submission.ClusterSubmission.write_sub_files","title":"<code>write_sub_files(dic_summary_by_gen=None)</code>","text":"<p>Generates and writes submission files for jobs based on their submission type.</p> <p>This method categorizes jobs to be submitted by their submission type, writes the corresponding submission files, and returns a dictionary containing the submission files and their associated job contexts.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are submission types and values are tuples containing a list of updated jobs and a list of submission filenames.</p> Source code in <code>study_da/submit/cluster_submission/cluster_submission.py</code> <pre><code>def write_sub_files(\n    self, dic_summary_by_gen: Optional[dict[int, dict[str, int]]] = None\n) -&gt; dict:\n    \"\"\"\n    Generates and writes submission files for jobs based on their submission type.\n\n    This method categorizes jobs to be submitted by their submission type, writes the\n    corresponding submission files, and returns a dictionary containing the submission\n    files and their associated job contexts.\n\n    Returns:\n        dict: A dictionary where keys are submission types and values are tuples containing\n            a list of updated jobs and a list of submission filenames.\n    \"\"\"\n    running_jobs, queuing_jobs = self._get_state_jobs(verbose=False)\n\n    # Make a dict of all jobs to submit depending on the submission type\n    dic_jobs_to_submit = {key: [] for key in self.dic_submission.keys()}\n    for job in self.l_jobs_to_submit:\n        l_keys = self.dic_all_jobs[job][\"l_keys\"]\n        submission_type = nested_get(self.dic_tree, l_keys + [\"submission_type\"])\n        dic_jobs_to_submit[submission_type].append(job)  # type: ignore\n\n    # Write submission files for each submission type\n    dic_submission_files = {}\n    for submission_type, list_of_jobs in dic_jobs_to_submit.items():\n        if len(list_of_jobs) &gt; 0:\n            # Write submission files\n            l_submission_filenames, list_of_jobs_updated = self._write_sub_files(\n                self.path_submission_file,\n                running_jobs,\n                queuing_jobs,\n                copy.copy(list_of_jobs),\n                submission_type,\n            )\n\n            # Record submission files and context\n            dic_submission_files[submission_type] = (\n                list_of_jobs_updated,\n                l_submission_filenames,\n            )\n\n            # Update dic_summary_by_gen inplace\n            if dic_summary_by_gen is not None:\n                for job in list_of_jobs:\n                    gen = self.dic_all_jobs[job][\"gen\"]\n                    if job in list_of_jobs_updated:\n                        dic_summary_by_gen[gen][\"submitted_now\"] += 1\n                    else:\n                        dic_summary_by_gen[gen][\"running_or_queuing\"] += 1\n\n    return dic_submission_files\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html","title":"submission_statements","text":"<p>This module contains the classes for the submission statements for the different cluster systems.</p>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC","title":"<code>HTC</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent an HTCondor submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the HTCondor submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class HTC(SubmissionStatement):\n    \"\"\"\n    A class to represent an HTCondor submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, context, htc_flavor='espresso'):\n            Initializes the HTCondor submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self, sub_filename: str, path_job_folder: str, context: str, htc_flavor: str = \"espresso\"\n    ):\n        \"\"\"\n        Initializes the HTCondor submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            context (str): The context for the submission.\n            htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, context)\n\n        self.head: str = (\n            \"# This is a HTCondor submission file\\n\"\n            + \"error  = error.txt\\n\"\n            + \"output = output.txt\\n\"\n            + \"log  = log.txt\"\n        )\n        self.body: str = (\n            f\"initialdir = {self.path_job_folder}\\n\"\n            + f\"executable = {self.path_job_folder}/run.sh\\n\"\n            + f\"request_GPUs = {self.request_GPUs}\\n\"\n            + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n            + \"queue\"\n        )\n        self.tail: str = \"# HTC\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC.__init__","title":"<code>__init__(sub_filename, path_job_folder, context, htc_flavor='espresso')</code>","text":"<p>Initializes the HTCondor submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>context</code> <code>str</code> <p>The context for the submission.</p> required <code>htc_flavor</code> <code>str</code> <p>The flavor of the HTCondor job. Defaults to \"espresso\".</p> <code>'espresso'</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self, sub_filename: str, path_job_folder: str, context: str, htc_flavor: str = \"espresso\"\n):\n    \"\"\"\n    Initializes the HTCondor submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        context (str): The context for the submission.\n        htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, context)\n\n    self.head: str = (\n        \"# This is a HTCondor submission file\\n\"\n        + \"error  = error.txt\\n\"\n        + \"output = output.txt\\n\"\n        + \"log  = log.txt\"\n    )\n    self.body: str = (\n        f\"initialdir = {self.path_job_folder}\\n\"\n        + f\"executable = {self.path_job_folder}/run.sh\\n\"\n        + f\"request_GPUs = {self.request_GPUs}\\n\"\n        + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n        + \"queue\"\n    )\n    self.tail: str = \"# HTC\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTC.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker","title":"<code>HTCDocker</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent an HTCondor submission statement using Docker.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the HTCondor Docker submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class HTCDocker(SubmissionStatement):\n    \"\"\"\n    A class to represent an HTCondor submission statement using Docker.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, context, path_image, htc_flavor='espresso'):\n            Initializes the HTCondor Docker submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self,\n        sub_filename: str,\n        path_job_folder: str,\n        context: str,\n        path_image: str,\n        htc_flavor: str = \"espresso\",\n    ):\n        \"\"\"\n        Initializes the HTCondor Docker submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            context (str): The context for the submission.\n            path_image (str): The path to the Docker image.\n            htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, context)\n\n        self.head: str = (\n            \"# This is a HTCondor submission file using Docker\\n\"\n            + \"error  = error.txt\\n\"\n            + \"output = output.txt\\n\"\n            + \"log  = log.txt\\n\"\n            + \"universe = vanilla\\n\"\n            + \"+SingularityImage =\"\n            + f' \"{path_image}\"'\n        )\n        self.body: str = (\n            f\"initialdir = {self.path_job_folder}\\n\"\n            + f\"executable = {self.path_job_folder}/run.sh\\n\"\n            + f\"request_GPUs = {self.request_GPUs}\\n\"\n            + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n            + \"queue\"\n        )\n        self.tail: str = \"# HTC Docker\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker.__init__","title":"<code>__init__(sub_filename, path_job_folder, context, path_image, htc_flavor='espresso')</code>","text":"<p>Initializes the HTCondor Docker submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>context</code> <code>str</code> <p>The context for the submission.</p> required <code>path_image</code> <code>str</code> <p>The path to the Docker image.</p> required <code>htc_flavor</code> <code>str</code> <p>The flavor of the HTCondor job. Defaults to \"espresso\".</p> <code>'espresso'</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self,\n    sub_filename: str,\n    path_job_folder: str,\n    context: str,\n    path_image: str,\n    htc_flavor: str = \"espresso\",\n):\n    \"\"\"\n    Initializes the HTCondor Docker submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        context (str): The context for the submission.\n        path_image (str): The path to the Docker image.\n        htc_flavor (str, optional): The flavor of the HTCondor job. Defaults to \"espresso\".\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, context)\n\n    self.head: str = (\n        \"# This is a HTCondor submission file using Docker\\n\"\n        + \"error  = error.txt\\n\"\n        + \"output = output.txt\\n\"\n        + \"log  = log.txt\\n\"\n        + \"universe = vanilla\\n\"\n        + \"+SingularityImage =\"\n        + f' \"{path_image}\"'\n    )\n    self.body: str = (\n        f\"initialdir = {self.path_job_folder}\\n\"\n        + f\"executable = {self.path_job_folder}/run.sh\\n\"\n        + f\"request_GPUs = {self.request_GPUs}\\n\"\n        + f'+JobFlavour  = \"{htc_flavor}\"\\n'\n        + \"queue\"\n    )\n    self.tail: str = \"# HTC Docker\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.HTCDocker.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"condor_submit {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC","title":"<code>LocalPC</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a local PC submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the LocalPC submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class LocalPC(SubmissionStatement):\n    \"\"\"\n    A class to represent a local PC submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, context=None): Initializes the LocalPC submission\n            statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, context: str | None = None):\n        \"\"\"\n        Initializes the LocalPC submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            context (str, optional): The context for the submission. Defaults to None.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, context)\n\n        self.head: str = \"# Running on local pc\"\n        self.body: str = f\"bash {self.path_job_folder}/run.sh &amp;\"\n        self.tail: str = \"# Local pc\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC.__init__","title":"<code>__init__(sub_filename, path_job_folder, context=None)</code>","text":"<p>Initializes the LocalPC submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>context</code> <code>str</code> <p>The context for the submission. Defaults to None.</p> <code>None</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, context: str | None = None):\n    \"\"\"\n    Initializes the LocalPC submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        context (str, optional): The context for the submission. Defaults to None.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, context)\n\n    self.head: str = \"# Running on local pc\"\n    self.body: str = f\"bash {self.path_job_folder}/run.sh &amp;\"\n    self.tail: str = \"# Local pc\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.LocalPC.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm","title":"<code>Slurm</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a SLURM submission statement.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the SLURM submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class Slurm(SubmissionStatement):\n    \"\"\"\n    A class to represent a SLURM submission statement.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, context): Initializes the SLURM submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, context: str | None):\n        \"\"\"\n        Initializes the SLURM submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            context (str): The context for the submission.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, context)\n\n        self.head: str = \"# Running on SLURM \"\n        if self.slurm_queue_statement != \"\":\n            queue_statement = self.slurm_queue_statement.split(\" \")[1]\n        else:\n            queue_statement = self.slurm_queue_statement\n        self.body: str = (\n            f\"sbatch --ntasks=2 {queue_statement} \"\n            f\"--output=output.txt --error=error.txt \"\n            f\"--gres=gpu:{self.request_GPUs} {self.path_job_folder}/run.sh\"\n        )\n        self.tail: str = \"# SLURM\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm.__init__","title":"<code>__init__(sub_filename, path_job_folder, context)</code>","text":"<p>Initializes the SLURM submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>context</code> <code>str</code> <p>The context for the submission.</p> required Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, context: str | None):\n    \"\"\"\n    Initializes the SLURM submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        context (str): The context for the submission.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, context)\n\n    self.head: str = \"# Running on SLURM \"\n    if self.slurm_queue_statement != \"\":\n        queue_statement = self.slurm_queue_statement.split(\" \")[1]\n    else:\n        queue_statement = self.slurm_queue_statement\n    self.body: str = (\n        f\"sbatch --ntasks=2 {queue_statement} \"\n        f\"--output=output.txt --error=error.txt \"\n        f\"--gres=gpu:{self.request_GPUs} {self.path_job_folder}/run.sh\"\n    )\n    self.tail: str = \"# SLURM\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.Slurm.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"bash {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker","title":"<code>SlurmDocker</code>","text":"<p>               Bases: <code>SubmissionStatement</code></p> <p>A class to represent a SLURM submission statement using Docker.</p> <p>Attributes:</p> Name Type Description <code>head</code> <code>str</code> <p>The header of the submission script.</p> <code>body</code> <code>str</code> <p>The body of the submission script.</p> <code>tail</code> <code>str</code> <p>The tail of the submission script.</p> <code>submit_command</code> <code>str</code> <p>The command to submit the job.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the SLURM Docker submission statement.</p> <code>get_submit_command</code> <p>Returns the command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class SlurmDocker(SubmissionStatement):\n    \"\"\"\n    A class to represent a SLURM submission statement using Docker.\n\n    Attributes:\n        head (str): The header of the submission script.\n        body (str): The body of the submission script.\n        tail (str): The tail of the submission script.\n        submit_command (str): The command to submit the job.\n\n    Methods:\n        __init__(sub_filename, path_job_folder, context, path_image, fix=False): Initializes the\n            SLURM Docker submission statement.\n        get_submit_command(sub_filename): Returns the command to submit the job.\n    \"\"\"\n\n    def __init__(\n        self,\n        sub_filename: str,\n        path_job_folder: str,\n        context: str,\n        path_image: str,  # type: ignore\n        fix: bool = False,\n    ):\n        \"\"\"\n        Initializes the SLURM Docker submission statement.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder.\n            context (str): The context for the submission.\n            path_image (str): The path to the Docker image.\n            fix (bool, optional): A flag to apply a fix for INFN. Defaults to False.\n        \"\"\"\n        super().__init__(sub_filename, path_job_folder, context)\n\n        # ! Ugly fix, will need to be removed when INFN is fixed\n        if fix:\n            to_replace = \"/storage-hpc/gpfs_data/HPC/home_recovery\"\n            replacement = \"/home/HPC\"\n            self.path_job_folder: str = self.path_job_folder.replace(to_replace, replacement)\n            path_image: str = path_image.replace(to_replace, replacement)\n            self.sub_filename: str = self.sub_filename.replace(to_replace, replacement)\n            self.str_fixed_run: str = (\n                f\"sed -i 's/{to_replace}/{replacement}/' {self.path_job_folder}/run.sh\\n\"\n            )\n\n        self.head: str = (\n            \"#!/bin/bash\\n\"\n            + \"# This is a SLURM submission file using Docker\\n\"\n            + self.slurm_queue_statement\n            + \"\\n\"\n            + f\"#SBATCH --output={self.path_job_folder}/output.txt\\n\"\n            + f\"#SBATCH --error={self.path_job_folder}/error.txt\\n\"\n            + \"#SBATCH --ntasks=2\\n\"\n            + f\"#SBATCH --gres=gpu:{self.request_GPUs}\"\n        )\n        self.body: str = f\"singularity exec {path_image} {self.path_job_folder}/run.sh\"\n        self.tail: str = \"# SLURM Docker\"\n        self.submit_command: str = self.get_submit_command(sub_filename)\n\n    @staticmethod\n    def get_submit_command(sub_filename: str) -&gt; str:\n        \"\"\"\n        Returns the command to submit the job.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n\n        Returns:\n            str: The command to submit the job.\n        \"\"\"\n        return f\"sbatch {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker.__init__","title":"<code>__init__(sub_filename, path_job_folder, context, path_image, fix=False)</code>","text":"<p>Initializes the SLURM Docker submission statement.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder.</p> required <code>context</code> <code>str</code> <p>The context for the submission.</p> required <code>path_image</code> <code>str</code> <p>The path to the Docker image.</p> required <code>fix</code> <code>bool</code> <p>A flag to apply a fix for INFN. Defaults to False.</p> <code>False</code> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(\n    self,\n    sub_filename: str,\n    path_job_folder: str,\n    context: str,\n    path_image: str,  # type: ignore\n    fix: bool = False,\n):\n    \"\"\"\n    Initializes the SLURM Docker submission statement.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder.\n        context (str): The context for the submission.\n        path_image (str): The path to the Docker image.\n        fix (bool, optional): A flag to apply a fix for INFN. Defaults to False.\n    \"\"\"\n    super().__init__(sub_filename, path_job_folder, context)\n\n    # ! Ugly fix, will need to be removed when INFN is fixed\n    if fix:\n        to_replace = \"/storage-hpc/gpfs_data/HPC/home_recovery\"\n        replacement = \"/home/HPC\"\n        self.path_job_folder: str = self.path_job_folder.replace(to_replace, replacement)\n        path_image: str = path_image.replace(to_replace, replacement)\n        self.sub_filename: str = self.sub_filename.replace(to_replace, replacement)\n        self.str_fixed_run: str = (\n            f\"sed -i 's/{to_replace}/{replacement}/' {self.path_job_folder}/run.sh\\n\"\n        )\n\n    self.head: str = (\n        \"#!/bin/bash\\n\"\n        + \"# This is a SLURM submission file using Docker\\n\"\n        + self.slurm_queue_statement\n        + \"\\n\"\n        + f\"#SBATCH --output={self.path_job_folder}/output.txt\\n\"\n        + f\"#SBATCH --error={self.path_job_folder}/error.txt\\n\"\n        + \"#SBATCH --ntasks=2\\n\"\n        + f\"#SBATCH --gres=gpu:{self.request_GPUs}\"\n    )\n    self.body: str = f\"singularity exec {path_image} {self.path_job_folder}/run.sh\"\n    self.tail: str = \"# SLURM Docker\"\n    self.submit_command: str = self.get_submit_command(sub_filename)\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SlurmDocker.get_submit_command","title":"<code>get_submit_command(sub_filename)</code>  <code>staticmethod</code>","text":"<p>Returns the command to submit the job.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The command to submit the job.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>@staticmethod\ndef get_submit_command(sub_filename: str) -&gt; str:\n    \"\"\"\n    Returns the command to submit the job.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n\n    Returns:\n        str: The command to submit the job.\n    \"\"\"\n    return f\"sbatch {sub_filename}\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SubmissionStatement","title":"<code>SubmissionStatement</code>","text":"<p>A master class to represent a submission statement for job scheduling.</p> <p>Attributes:</p> Name Type Description <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> <code>path_job_folder</code> <code>str</code> <p>The path to the job folder, ensuring no trailing slash.</p> <code>request_GPUs</code> <code>int</code> <p>The number of GPUs requested based on the context.</p> <code>slurm_queue_statement</code> <code>str</code> <p>The SLURM queue statement based on the context.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>str, path_job_folder: str, context: str | None): Initializes the SubmissionStatement with the given filename, job folder path, and context.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>class SubmissionStatement:\n    \"\"\"\n    A master class to represent a submission statement for job scheduling.\n\n    Attributes:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder, ensuring no trailing slash.\n        request_GPUs (int): The number of GPUs requested based on the context.\n        slurm_queue_statement (str): The SLURM queue statement based on the context.\n\n    Methods:\n        __init__(sub_filename: str, path_job_folder: str, context: str | None):\n            Initializes the SubmissionStatement with the given filename, job folder path, and\n            context.\n    \"\"\"\n\n    def __init__(self, sub_filename: str, path_job_folder: str, context: str | None):\n        \"\"\"\n        Initialize the submission statement configuration.\n\n        Args:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder. Trailing slash will be removed if\n                present.\n            context (str | None): The context for GPU configuration. If 'cupy' or 'opencl', GPU\n                will be requested.\n\n        Attributes:\n            sub_filename (str): The name of the submission file.\n            path_job_folder (str): The path to the job folder without trailing slash.\n            request_GPUs (int): Number of GPUs requested. 1 if context is 'cupy' or 'opencl',\n                otherwise 0.\n            slurm_queue_statement (str): SLURM queue statement. Empty if context is 'cupy' or\n                'opencl', otherwise set to '#SBATCH --partition=slurm_hpc_acc'.\n        \"\"\"\n        self.sub_filename: str = sub_filename\n        self.path_job_folder: str = (\n            path_job_folder[:-1] if path_job_folder[-1] == \"/\" else path_job_folder\n        )\n\n        # GPU configuration\n        if context in {\"cupy\", \"opencl\"}:\n            self.request_GPUs: int = 1\n            self.slurm_queue_statement: str = \"\"\n        else:\n            self.request_GPUs: int = 0\n            self.slurm_queue_statement: str = \"#SBATCH --partition=slurm_hpc_acc\"\n</code></pre>"},{"location":"reference/study_da/submit/cluster_submission/submission_statements.html#study_da.submit.cluster_submission.submission_statements.SubmissionStatement.__init__","title":"<code>__init__(sub_filename, path_job_folder, context)</code>","text":"<p>Initialize the submission statement configuration.</p> <p>Parameters:</p> Name Type Description Default <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> required <code>path_job_folder</code> <code>str</code> <p>The path to the job folder. Trailing slash will be removed if present.</p> required <code>context</code> <code>str | None</code> <p>The context for GPU configuration. If 'cupy' or 'opencl', GPU will be requested.</p> required <p>Attributes:</p> Name Type Description <code>sub_filename</code> <code>str</code> <p>The name of the submission file.</p> <code>path_job_folder</code> <code>str</code> <p>The path to the job folder without trailing slash.</p> <code>request_GPUs</code> <code>int</code> <p>Number of GPUs requested. 1 if context is 'cupy' or 'opencl', otherwise 0.</p> <code>slurm_queue_statement</code> <code>str</code> <p>SLURM queue statement. Empty if context is 'cupy' or 'opencl', otherwise set to '#SBATCH --partition=slurm_hpc_acc'.</p> Source code in <code>study_da/submit/cluster_submission/submission_statements.py</code> <pre><code>def __init__(self, sub_filename: str, path_job_folder: str, context: str | None):\n    \"\"\"\n    Initialize the submission statement configuration.\n\n    Args:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder. Trailing slash will be removed if\n            present.\n        context (str | None): The context for GPU configuration. If 'cupy' or 'opencl', GPU\n            will be requested.\n\n    Attributes:\n        sub_filename (str): The name of the submission file.\n        path_job_folder (str): The path to the job folder without trailing slash.\n        request_GPUs (int): Number of GPUs requested. 1 if context is 'cupy' or 'opencl',\n            otherwise 0.\n        slurm_queue_statement (str): SLURM queue statement. Empty if context is 'cupy' or\n            'opencl', otherwise set to '#SBATCH --partition=slurm_hpc_acc'.\n    \"\"\"\n    self.sub_filename: str = sub_filename\n    self.path_job_folder: str = (\n        path_job_folder[:-1] if path_job_folder[-1] == \"/\" else path_job_folder\n    )\n\n    # GPU configuration\n    if context in {\"cupy\", \"opencl\"}:\n        self.request_GPUs: int = 1\n        self.slurm_queue_statement: str = \"\"\n    else:\n        self.request_GPUs: int = 0\n        self.slurm_queue_statement: str = \"#SBATCH --partition=slurm_hpc_acc\"\n</code></pre>"},{"location":"reference/study_da/utils/index.html","title":"utils","text":""},{"location":"reference/study_da/utils/index.html#study_da.utils.clean_dic","title":"<code>clean_dic(o)</code>","text":"<p>Convert numpy types to standard types in a nested dictionary containing number and lists.</p> <p>Parameters:</p> Name Type Description Default <code>o</code> <code>Any</code> <p>The object to convert.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def clean_dic(o: Any) -&gt; None:\n    \"\"\"Convert numpy types to standard types in a nested dictionary containing number and lists.\n\n    Args:\n        o (Any): The object to convert.\n\n    Returns:\n        None\n    \"\"\"\n    if not isinstance(o, dict):\n        return\n    for k, v in o.items():\n        if isinstance(v, np.generic):\n            o[k] = v.item()\n        elif isinstance(v, list):\n            for i, x in enumerate(v):\n                if isinstance(x, np.generic):\n                    v[i] = x.item()\n                if isinstance(x, dict):\n                    clean_dic(x)\n        else:\n            clean_dic(v)\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/utils/index.html#study_da.utils.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html","title":"dic_utils","text":"<p>This module provides utility functions for handling nested dictionaries and YAML files.</p> <p>Functions:</p> Name Description <code>load_dic_from_path</code> <p>str, ryaml: ruamel.yaml.YAML | None = None) -&gt; tuple[dict, ruamel.yaml.YAML]: Load a dictionary from a YAML file.</p> <code>write_dic_to_path</code> <p>dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None: Write a dictionary to a YAML file.</p> <code>nested_get</code> <p>dict, keys: list) -&gt; Any: Get the value from a nested dictionary using a list of keys.</p> <code>nested_set</code> <p>dict, keys: list, value: Any) -&gt; None: Set a value in a nested dictionary using a list of keys.</p> <code>find_item_in_dic</code> <p>dict, key: str) -&gt; Any: Find an item in a nested dictionary.</p> <code>set_item_in_dic</code> <p>dict, key: str, value: Any, found: bool = False) -&gt; None: Set an item in a nested dictionary.</p> <code>clean_dic</code> <p>Any) -&gt; None: Convert numpy types to standard types in a nested dictionary containing numbers and lists.</p>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.clean_dic","title":"<code>clean_dic(o)</code>","text":"<p>Convert numpy types to standard types in a nested dictionary containing number and lists.</p> <p>Parameters:</p> Name Type Description Default <code>o</code> <code>Any</code> <p>The object to convert.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def clean_dic(o: Any) -&gt; None:\n    \"\"\"Convert numpy types to standard types in a nested dictionary containing number and lists.\n\n    Args:\n        o (Any): The object to convert.\n\n    Returns:\n        None\n    \"\"\"\n    if not isinstance(o, dict):\n        return\n    for k, v in o.items():\n        if isinstance(v, np.generic):\n            o[k] = v.item()\n        elif isinstance(v, list):\n            for i, x in enumerate(v):\n                if isinstance(x, np.generic):\n                    v[i] = x.item()\n                if isinstance(x, dict):\n                    clean_dic(x)\n        else:\n            clean_dic(v)\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.find_item_in_dic","title":"<code>find_item_in_dic(obj, key)</code>","text":"<p>Find an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to find in the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the key in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def find_item_in_dic(obj: dict, key: str) -&gt; Any:\n    \"\"\"Find an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to find in the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the key in the nested dictionary.\n\n    \"\"\"\n    if key in obj:\n        return obj[key]\n    for v in obj.values():\n        if isinstance(v, dict):\n            item = find_item_in_dic(v, key)\n            if item is not None:\n                return item\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.load_dic_from_path","title":"<code>load_dic_from_path(path, ryaml=None)</code>","text":"<p>Load a dictionary from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, YAML]</code> <p>tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def load_dic_from_path(\n    path: str, ryaml: ruamel.yaml.YAML | None = None\n) -&gt; tuple[dict, ruamel.yaml.YAML]:\n    \"\"\"Load a dictionary from a yaml file.\n\n    Args:\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        tuple[dict, ruamel.yaml.YAML]: The dictionary and the yaml reader.\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Load dic\n    with open(path, \"r\") as fid:\n        dic = ryaml.load(fid)\n\n    return dic, ryaml\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.nested_get","title":"<code>nested_get(dic, keys)</code>","text":"<p>Get the value from a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value corresponding to the keys in the nested dictionary.</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_get(dic: dict, keys: list) -&gt; Any:\n    # Adapted from https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n    \"\"\"Get the value from a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n\n    Returns:\n        Any: The value corresponding to the keys in the nested dictionary.\n\n    \"\"\"\n    for key in keys:\n        dic = dic[key]\n    return dic\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Set a value in a nested dictionary using a list of keys.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The nested dictionary.</p> required <code>keys</code> <code>list</code> <p>The list of keys to traverse the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def nested_set(dic: dict, keys: list, value: Any) -&gt; None:\n    \"\"\"Set a value in a nested dictionary using a list of keys.\n\n    Args:\n        dic (dict): The nested dictionary.\n        keys (list): The list of keys to traverse the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.set_item_in_dic","title":"<code>set_item_in_dic(obj, key, value, found=False)</code>","text":"<p>Set an item in a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>The nested dictionary.</p> required <code>key</code> <code>str</code> <p>The key to set in the nested dictionary.</p> required <code>value</code> <code>Any</code> <p>The value to set in the nested dictionary.</p> required <code>found</code> <code>bool</code> <p>Whether the key has been found in the nested dictionary.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def set_item_in_dic(obj: dict, key: str, value: Any, found: bool = False) -&gt; None:\n    \"\"\"Set an item in a nested dictionary.\n\n    Args:\n        obj (dict): The nested dictionary.\n        key (str): The key to set in the nested dictionary.\n        value (Any): The value to set in the nested dictionary.\n        found (bool): Whether the key has been found in the nested dictionary.\n\n    Returns:\n        None\n\n    \"\"\"\n    if key in obj:\n        if found:\n            raise ValueError(f\"Key {key} found more than once in the nested dictionary.\")\n\n        obj[key] = value\n        found = True\n    for v in obj.values():\n        if isinstance(v, dict):\n            set_item_in_dic(v, key, value, found)\n</code></pre>"},{"location":"reference/study_da/utils/dic_utils.html#study_da.utils.dic_utils.write_dic_to_path","title":"<code>write_dic_to_path(dic, path, ryaml=None)</code>","text":"<p>Write a dictionary to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>dic</code> <code>dict</code> <p>The dictionary to write.</p> required <code>path</code> <code>str</code> <p>The path to the yaml file.</p> required <code>ryaml</code> <code>YAML</code> <p>The yaml reader.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>study_da/utils/dic_utils.py</code> <pre><code>def write_dic_to_path(dic: dict, path: str, ryaml: ruamel.yaml.YAML | None = None) -&gt; None:\n    \"\"\"Write a dictionary to a yaml file.\n\n    Args:\n        dic (dict): The dictionary to write.\n        path (str): The path to the yaml file.\n        ryaml (ruamel.yaml.YAML): The yaml reader.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    if ryaml is None:\n        # Initialize yaml reader\n        ryaml = ruamel.yaml.YAML()\n\n    # Write dic\n    with open(path, \"w\") as fid:\n        ryaml.dump(dic, fid)\n        # Force os to write to disk now, to avoid race conditions\n        fid.flush()\n        os.fsync(fid.fileno())\n</code></pre>"}]}